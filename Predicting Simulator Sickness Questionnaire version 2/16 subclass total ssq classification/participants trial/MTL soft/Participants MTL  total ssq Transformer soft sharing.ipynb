{"cells":[{"cell_type":"code","execution_count":15,"metadata":{"id":"fT_A9oAGAepC","executionInfo":{"status":"ok","timestamp":1726042383239,"user_tz":300,"elapsed":218,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"outputs":[],"source":["from google.colab import drive\n","import pandas as pd\n","from sklearn.preprocessing import StandardScaler\n","from scipy import stats\n","import numpy as np\n","import logging\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense\n","from datetime import datetime\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import datetime\n","import matplotlib.dates as mdates\n","import os"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1080,"status":"ok","timestamp":1726042384500,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"},"user_tz":300},"id":"TishlaEBAmlN","outputId":"b90cf85c-74cd-47b4-cbc2-fd8fc6768bd6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Set the base path to the desired directory on Google Drive\n","base_path = '/content/drive/MyDrive/Study_1_Data/'"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"XeSwJ8oKAt2r","executionInfo":{"status":"ok","timestamp":1726042384500,"user_tz":300,"elapsed":8,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"outputs":[],"source":["def read_csv(file_path):\n","    data = pd.read_csv(file_path)\n","    return data"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"BFHyHoFvA5bX","executionInfo":{"status":"ok","timestamp":1726042384500,"user_tz":300,"elapsed":7,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"outputs":[],"source":["def process_data(data, columns_to_remove):\n","    processed_data = data.drop(columns=columns_to_remove).values\n","    return processed_data"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"QeoIWcudA94b","executionInfo":{"status":"ok","timestamp":1726042384500,"user_tz":300,"elapsed":6,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"outputs":[],"source":["def construct_3d_array(base_dir, participants, simulations, columns_to_remove_hr, columns_to_remove_gsr, columns_to_remove_head, columns_to_remove_eye):\n","    \"\"\"\n","    Construct 3D array from CSV files.\n","    \"\"\"\n","    num_rows = 180  # Define number of rows to keep (last 180 rows)\n","    arrays_3d = []\n","\n","    for participant in participants:\n","        participant_id = f\"{int(participant):02d}\"  # Format participant number to two digits\n","\n","        valid_simulations = []\n","\n","        for simulation in simulations:\n","            hr_file_path = os.path.join(base_dir, participant_id, simulation, f'HR{simulation.capitalize()}.csv')\n","            gsr_file_path = os.path.join(base_dir, participant_id, simulation, f'EDA{simulation.capitalize()}_downsampled.csv')\n","            head_file_path = os.path.join(base_dir, participant_id, simulation, 'head_tracking_downsampled.csv')\n","            eye_file_path = os.path.join(base_dir, participant_id, simulation, 'eye_tracking_downsampled.csv')\n","\n","            # Check if all files exist\n","            if all(os.path.exists(file) for file in [hr_file_path, gsr_file_path, head_file_path, eye_file_path]):\n","                valid_simulations.append(simulation)\n","\n","        num_valid_simulations = len(valid_simulations)\n","        if num_valid_simulations == 0:\n","            continue  # Skip this participant if no valid simulations are found\n","\n","        array_3d = np.zeros((num_valid_simulations, num_rows, 47)) # hr=1, gsr=1, head=15-3, eye=41-8 total columns after removing columns= 48\n","\n","        for s_idx, simulation in enumerate(valid_simulations):\n","            # Process hr data\n","            hr_file_path = os.path.join(base_dir, participant_id, simulation, f'HR{simulation.capitalize()}.csv')\n","            hr_data = read_csv(hr_file_path)\n","            processed_hr_data = process_data(hr_data, columns_to_remove_hr)\n","            processed_hr_data = processed_hr_data[-num_rows:]  # Keep only the last 180 rows\n","\n","            # Process gsr data\n","            gsr_file_path = os.path.join(base_dir, participant_id, simulation, f'EDA{simulation.capitalize()}_downsampled.csv')\n","            gsr_data = read_csv(gsr_file_path)\n","            processed_gsr_data = process_data(gsr_data, columns_to_remove_gsr)\n","            processed_gsr_data = processed_gsr_data[-num_rows:]  # Keep only the last 180 rows\n","\n","            # Process head data\n","            head_file_path = os.path.join(base_dir, participant_id, simulation, 'head_tracking_downsampled.csv')\n","            head_data = read_csv(head_file_path)\n","            processed_head_data = process_data(head_data, columns_to_remove_head)\n","            processed_head_data = processed_head_data[-num_rows:]  # Keep only the last 180 rows\n","\n","            # Process eye data\n","            eye_file_path = os.path.join(base_dir, participant_id, simulation, 'eye_tracking_downsampled.csv')\n","            eye_data = read_csv(eye_file_path)\n","            processed_eye_data = process_data(eye_data, columns_to_remove_eye)\n","            processed_eye_data = processed_eye_data[-num_rows:]  # Keep only the last 180 rows\n","\n","            # Combine processed data\n","            combined_data = np.concatenate((processed_hr_data, processed_gsr_data, processed_head_data, processed_eye_data), axis=1)\n","\n","            array_3d[s_idx, :, :] = combined_data\n","\n","        arrays_3d.append(array_3d)\n","\n","    return arrays_3d\n"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"fqaeUGUDBCtT","executionInfo":{"status":"ok","timestamp":1726042384501,"user_tz":300,"elapsed":7,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"outputs":[],"source":["sample_size=30\n","simulations = ['flat','noise','bumps']\n","participants = [str(i) for i in range(1, 27)]  # Participants 101 to 127\n","columns_to_remove_hr = []\n","columns_to_remove_gsr = []\n","columns_to_remove_eye = ['#Frame','Time', 'Unnamed: 40','ConvergenceValid','Left_Eye_Closed','Right_Eye_Closed','LocalGazeValid','WorldGazeValid']\n","columns_to_remove_head = ['#Frame','Time', 'Unnamed: 14']"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"vPQyeAYKBYaO","executionInfo":{"status":"ok","timestamp":1726042384501,"user_tz":300,"elapsed":6,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"outputs":[],"source":["def calculate_total_ssq(csv_file):\n","    # Read the CSV file into a DataFrame\n","    df = pd.read_csv(csv_file)\n","    n_columns = [0, 5, 6, 7, 8, 14, 15]\n","    o_columns = [0, 1, 2, 3, 4, 8, 10]\n","    d_columns = [4, 7, 9, 10, 11, 12, 13]\n","\n","    # Calculate sum for each specified set of columns\n","    n_val = df.iloc[0, n_columns].sum()\n","    o_val = df.iloc[0, o_columns].sum()\n","    d_val = df.iloc[0, d_columns].sum()\n","\n","    return n_val, o_val, d_val"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"Xpn0lDt0BfvE","executionInfo":{"status":"ok","timestamp":1726042384708,"user_tz":300,"elapsed":212,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"outputs":[],"source":["def merge_ssq_column(conditions,participants):\n","  directories = []\n","  total_ssq_values = []\n","  for participant in participants:\n","      participant = f\"{int(participant):02d}\"\n","      for condition in conditions:\n","          directory = os.path.join(base_path, participant, condition)\n","          directories.append(directory)\n","\n","  # Loop through each directory\n","  for directory in directories:\n","      # Check if the directory exists\n","      if not os.path.exists(directory):\n","          continue\n","\n","      # Get all CSV files in the directory that are named 'ssq.csv'\n","      csv_files = [file for file in os.listdir(directory) if file == 'ssq.csv']\n","\n","      # Loop through each CSV file\n","      for csv_file in csv_files:\n","          csv_path = os.path.join(directory, csv_file)\n","          df = pd.read_csv(csv_path)\n","          # n_val,o_val,d_val = calculate_total_ssq(csv_path)\n","          # total_ssq_values.append([n_val, o_val, d_val])\n","          ssq_values_participant = df.iloc[:, 0:17].values.flatten()   # Assuming SSQ values are in columns 1 to 16\n","          total_ssq_values.append(ssq_values_participant)\n","  ssq_array = np.array(total_ssq_values)\n","  return ssq_array\n","\n","def merge_total_ssq(conditions,participants):\n","  directories = []\n","  total_ssq_values = []\n","  for participant in participants:\n","      participant = f\"{int(participant):02d}\"\n","      for condition in conditions:\n","          directory = os.path.join(base_path, participant, condition)\n","          directories.append(directory)\n","\n","  # Loop through each directory\n","  for directory in directories:\n","      # Check if the directory exists\n","      if not os.path.exists(directory):\n","          continue\n","\n","      # Get all CSV files in the directory that are named 'ssq.csv'\n","      csv_files = [file for file in os.listdir(directory) if file == 'ssq.csv']\n","\n","      # Loop through each CSV file\n","      for csv_file in csv_files:\n","          csv_path = os.path.join(directory, csv_file)\n","          n_val,o_val,d_val = calculate_total_ssq(csv_path)\n","          total_ssq = (n_val+o_val+d_val) * 3.74\n","          df = pd.read_csv(csv_path)\n","          df[\"total-ssq\"] = total_ssq\n","          #print(\"csv_path: \",csv_path,\"   \",total_ssq)\n","          total_ssq_values.append(total_ssq)\n","  # Create a DataFrame from the list of total SSQ values\n","  df_total_ssq = pd.DataFrame(total_ssq_values, columns=[\"total-ssq\"])\n","  # Convert the list of total SSQ values to a NumPy array\n","  total_ssq_array = np.array(total_ssq_values)\n","  return total_ssq_array\n","\n"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"7k17K0HrCr6-","executionInfo":{"status":"ok","timestamp":1726042387702,"user_tz":300,"elapsed":2997,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"outputs":[],"source":["participants_group_1 = [1,3,4,11,25]\n","participants_group_2 = [2,7,8,9,17]\n","participants_group_3 = [10,12,13,22,23]\n","participants_group_4 = [5,14,18,20,21]\n","participants_group_5 = [6,15,16,19,24,26]\n","\n","arrays_group_1 = construct_3d_array(base_path, participants_group_1, simulations, columns_to_remove_hr, columns_to_remove_gsr, columns_to_remove_head, columns_to_remove_eye)\n","arrays_group_2 = construct_3d_array(base_path, participants_group_2, simulations, columns_to_remove_hr, columns_to_remove_gsr, columns_to_remove_head, columns_to_remove_eye)\n","arrays_group_3 = construct_3d_array(base_path, participants_group_3, simulations, columns_to_remove_hr, columns_to_remove_gsr, columns_to_remove_head, columns_to_remove_eye)\n","arrays_group_4 = construct_3d_array(base_path, participants_group_4, simulations, columns_to_remove_hr, columns_to_remove_gsr, columns_to_remove_head, columns_to_remove_eye)\n","arrays_group_5 = construct_3d_array(base_path, participants_group_5, simulations, columns_to_remove_hr, columns_to_remove_gsr, columns_to_remove_head, columns_to_remove_eye)"]},{"cell_type":"code","source":["# Concatenate arrays along the first axis\n","input_group_1 = np.concatenate(arrays_group_1, axis=0)\n","input_group_2 = np.concatenate(arrays_group_2, axis=0)\n","input_group_3 = np.concatenate(arrays_group_3, axis=0)\n","input_group_4 = np.concatenate(arrays_group_4, axis=0)\n","input_group_5 = np.concatenate(arrays_group_5, axis=0)\n"],"metadata":{"id":"w-JtDyHLsF9Y","executionInfo":{"status":"ok","timestamp":1726042387703,"user_tz":300,"elapsed":5,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["output_group_1=merge_ssq_column(simulations,participants_group_1)\n","output_group_2=merge_ssq_column(simulations,participants_group_2)\n","output_group_3=merge_ssq_column(simulations,participants_group_3)\n","output_group_4=merge_ssq_column(simulations,participants_group_4)\n","output_group_5=merge_ssq_column(simulations,participants_group_5)\n","\n","output_group_1 = np.squeeze(output_group_1)\n","output_group_2 = np.squeeze(output_group_2)\n","output_group_3 = np.squeeze(output_group_3)\n","output_group_4 = np.squeeze(output_group_4)\n","output_group_5 = np.squeeze(output_group_5)\n","\n","\n","output_total_ssq_group_1=merge_total_ssq(simulations,participants_group_1)\n","output_total_ssq_group_2=merge_total_ssq(simulations,participants_group_2)\n","output_total_ssq_group_3=merge_total_ssq(simulations,participants_group_3)\n","output_total_ssq_group_4=merge_total_ssq(simulations,participants_group_4)\n","output_total_ssq_group_5=merge_total_ssq(simulations,participants_group_5)\n","\n","output_total_ssq_group_1=output_total_ssq_group_1.reshape(-1, 1)\n","output_total_ssq_group_2=output_total_ssq_group_2.reshape(-1, 1)\n","output_total_ssq_group_3=output_total_ssq_group_3.reshape(-1, 1)\n","output_total_ssq_group_4=output_total_ssq_group_4.reshape(-1, 1)\n","output_total_ssq_group_5=output_total_ssq_group_5.reshape(-1, 1)\n","\n"],"metadata":{"id":"DbS4TcoLjrFf","executionInfo":{"status":"ok","timestamp":1726042389537,"user_tz":300,"elapsed":1838,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","execution_count":26,"metadata":{"id":"26ADF-kiC1EZ","executionInfo":{"status":"ok","timestamp":1726042389537,"user_tz":300,"elapsed":4,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"outputs":[],"source":["def scale_input_data(input_train, input_test):\n","    # Get the shape of the input data\n","    num_samples_train, time_steps_train, num_features = input_train.shape\n","    num_samples_test, time_steps_test, _ = input_test.shape\n","\n","    # Reshape the input data into 2D arrays\n","    flattened_train_data = input_train.reshape(-1, num_features)\n","    flattened_test_data = input_test.reshape(-1, num_features)\n","\n","    # Initialize a MinMaxScaler object\n","    scaler = MinMaxScaler()\n","\n","    # Fit the scaler on the training data and transform both train and test data\n","    scaled_train_data = scaler.fit_transform(flattened_train_data)\n","    scaled_test_data = scaler.transform(flattened_test_data)\n","\n","    # Reshape the scaled data back to its original shape\n","    scaled_train_data = scaled_train_data.reshape(num_samples_train, time_steps_train, num_features)\n","    scaled_test_data = scaled_test_data.reshape(num_samples_test, time_steps_test, num_features)\n","\n","    return scaled_train_data, scaled_test_data\n","\n","def scale_target_var(target_data):\n","    min_val, max_val = np.min(target_data, axis=0), np.max(target_data, axis=0)\n","    target_data = (target_data-min_val)/(max_val-min_val)\n","\n","    return target_data, min_val, max_val"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":234180,"status":"ok","timestamp":1726042623713,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"},"user_tz":300},"id":"E6ssyYUeDJwI","outputId":"b417cafc-24f6-43fb-b3be-c872a972eea5"},"outputs":[{"output_type":"stream","name":"stdout","text":["input_train : (38, 120, 47)\n","output_train : (38, 16, 4)\n","input_val : (14, 120, 47)\n","output_val : (14, 16, 4)\n","input_test : (16, 120, 47)\n","output_test : (16, 16)\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 83ms/step - dense_124_accuracy: 0.2484 - dense_125_accuracy: 0.7763 - dense_126_accuracy: 0.6469 - dense_127_accuracy: 0.0175 - dense_128_accuracy: 0.0910 - dense_129_accuracy: 0.6190 - dense_130_accuracy: 0.2659 - dense_131_accuracy: 0.3810 - dense_132_accuracy: 0.2171 - dense_133_accuracy: 0.7588 - dense_134_accuracy: 0.5175 - dense_135_accuracy: 0.6924 - dense_136_accuracy: 0.0630 - dense_137_accuracy: 0.0910 - dense_138_accuracy: 0.2796 - dense_139_accuracy: 0.0280 - loss: 2.9022\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 777ms/step\n","k: 0, patience: 0\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - dense_124_accuracy: 0.4545 - dense_125_accuracy: 0.7971 - dense_126_accuracy: 0.8882 - dense_127_accuracy: 0.5559 - dense_128_accuracy: 0.7412 - dense_129_accuracy: 0.7867 - dense_130_accuracy: 0.5104 - dense_131_accuracy: 0.5175 - dense_132_accuracy: 0.8427 - dense_133_accuracy: 0.8427 - dense_134_accuracy: 0.8427 - dense_135_accuracy: 0.7796 - dense_136_accuracy: 0.7412 - dense_137_accuracy: 0.7588 - dense_138_accuracy: 0.2763 - dense_139_accuracy: 0.7588 - loss: 2.0785\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n","k: 1, patience: 0\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - dense_124_accuracy: 0.4720 - dense_125_accuracy: 0.7867 - dense_126_accuracy: 0.8882 - dense_127_accuracy: 0.5455 - dense_128_accuracy: 0.7763 - dense_129_accuracy: 0.7867 - dense_130_accuracy: 0.5104 - dense_131_accuracy: 0.6014 - dense_132_accuracy: 0.8322 - dense_133_accuracy: 0.8322 - dense_134_accuracy: 0.8986 - dense_135_accuracy: 0.7588 - dense_136_accuracy: 0.7692 - dense_137_accuracy: 0.8602 - dense_138_accuracy: 0.5559 - dense_139_accuracy: 0.8706 - loss: 1.6642\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n","k: 2, patience: 1\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - dense_124_accuracy: 0.4090 - dense_125_accuracy: 0.7971 - dense_126_accuracy: 0.8882 - dense_127_accuracy: 0.5280 - dense_128_accuracy: 0.7971 - dense_129_accuracy: 0.7867 - dense_130_accuracy: 0.3427 - dense_131_accuracy: 0.5910 - dense_132_accuracy: 0.8427 - dense_133_accuracy: 0.8322 - dense_134_accuracy: 0.8986 - dense_135_accuracy: 0.7588 - dense_136_accuracy: 0.7588 - dense_137_accuracy: 0.8706 - dense_138_accuracy: 0.5455 - dense_139_accuracy: 0.8810 - loss: 1.6779\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n","k: 3, patience: 2\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 179ms/step - dense_124_accuracy: 0.4337 - dense_125_accuracy: 0.7763 - dense_126_accuracy: 0.8986 - dense_127_accuracy: 0.6573 - dense_128_accuracy: 0.7867 - dense_129_accuracy: 0.7763 - dense_130_accuracy: 0.3498 - dense_131_accuracy: 0.6014 - dense_132_accuracy: 0.8322 - dense_133_accuracy: 0.8427 - dense_134_accuracy: 0.8882 - dense_135_accuracy: 0.7588 - dense_136_accuracy: 0.7588 - dense_137_accuracy: 0.8602 - dense_138_accuracy: 0.5559 - dense_139_accuracy: 0.8706 - loss: 1.6614\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n","k: 4, patience: 0\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 156ms/step - dense_124_accuracy: 0.4441 - dense_125_accuracy: 0.7867 - dense_126_accuracy: 0.8986 - dense_127_accuracy: 0.6398 - dense_128_accuracy: 0.7971 - dense_129_accuracy: 0.7971 - dense_130_accuracy: 0.3043 - dense_131_accuracy: 0.6118 - dense_132_accuracy: 0.8531 - dense_133_accuracy: 0.8427 - dense_134_accuracy: 0.8986 - dense_135_accuracy: 0.7692 - dense_136_accuracy: 0.7692 - dense_137_accuracy: 0.8810 - dense_138_accuracy: 0.5663 - dense_139_accuracy: 0.8706 - loss: 1.5575\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step\n","k: 5, patience: 1\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 167ms/step - dense_124_accuracy: 0.4512 - dense_125_accuracy: 0.7867 - dense_126_accuracy: 0.8882 - dense_127_accuracy: 0.5280 - dense_128_accuracy: 0.7971 - dense_129_accuracy: 0.7971 - dense_130_accuracy: 0.5559 - dense_131_accuracy: 0.5630 - dense_132_accuracy: 0.8531 - dense_133_accuracy: 0.8531 - dense_134_accuracy: 0.8986 - dense_135_accuracy: 0.7763 - dense_136_accuracy: 0.7588 - dense_137_accuracy: 0.8706 - dense_138_accuracy: 0.5104 - dense_139_accuracy: 0.8602 - loss: 1.5635\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step\n","k: 6, patience: 0\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - dense_124_accuracy: 0.5175 - dense_125_accuracy: 0.7867 - dense_126_accuracy: 0.8882 - dense_127_accuracy: 0.5735 - dense_128_accuracy: 0.7971 - dense_129_accuracy: 0.7971 - dense_130_accuracy: 0.5000 - dense_131_accuracy: 0.5280 - dense_132_accuracy: 0.8531 - dense_133_accuracy: 0.8531 - dense_134_accuracy: 0.8986 - dense_135_accuracy: 0.7796 - dense_136_accuracy: 0.7796 - dense_137_accuracy: 0.8706 - dense_138_accuracy: 0.4265 - dense_139_accuracy: 0.8602 - loss: 1.5954\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n","k: 7, patience: 0\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - dense_124_accuracy: 0.4616 - dense_125_accuracy: 0.7763 - dense_126_accuracy: 0.8882 - dense_127_accuracy: 0.6573 - dense_128_accuracy: 0.7308 - dense_129_accuracy: 0.7763 - dense_130_accuracy: 0.4896 - dense_131_accuracy: 0.5910 - dense_132_accuracy: 0.8322 - dense_133_accuracy: 0.8427 - dense_134_accuracy: 0.8882 - dense_135_accuracy: 0.7588 - dense_136_accuracy: 0.7484 - dense_137_accuracy: 0.8602 - dense_138_accuracy: 0.5071 - dense_139_accuracy: 0.8706 - loss: 1.5468\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n","k: 8, patience: 1\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 155ms/step - dense_124_accuracy: 0.4616 - dense_125_accuracy: 0.7763 - dense_126_accuracy: 0.8882 - dense_127_accuracy: 0.5910 - dense_128_accuracy: 0.7763 - dense_129_accuracy: 0.7763 - dense_130_accuracy: 0.4792 - dense_131_accuracy: 0.5806 - dense_132_accuracy: 0.8427 - dense_133_accuracy: 0.8322 - dense_134_accuracy: 0.8882 - dense_135_accuracy: 0.7692 - dense_136_accuracy: 0.7484 - dense_137_accuracy: 0.8706 - dense_138_accuracy: 0.5247 - dense_139_accuracy: 0.8602 - loss: 1.5272\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step\n","k: 9, patience: 2\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 151ms/step - dense_124_accuracy: 0.4232 - dense_125_accuracy: 0.7867 - dense_126_accuracy: 0.8986 - dense_127_accuracy: 0.6086 - dense_128_accuracy: 0.7867 - dense_129_accuracy: 0.7867 - dense_130_accuracy: 0.4896 - dense_131_accuracy: 0.6190 - dense_132_accuracy: 0.8427 - dense_133_accuracy: 0.8427 - dense_134_accuracy: 0.8986 - dense_135_accuracy: 0.7588 - dense_136_accuracy: 0.7588 - dense_137_accuracy: 0.8706 - dense_138_accuracy: 0.5351 - dense_139_accuracy: 0.8706 - loss: 1.5865\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step\n","k: 10, patience: 3\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 159ms/step - dense_124_accuracy: 0.5735 - dense_125_accuracy: 0.8427 - dense_126_accuracy: 0.8986 - dense_127_accuracy: 0.7308 - dense_128_accuracy: 0.7971 - dense_129_accuracy: 0.7867 - dense_130_accuracy: 0.4792 - dense_131_accuracy: 0.6223 - dense_132_accuracy: 0.8147 - dense_133_accuracy: 0.8427 - dense_134_accuracy: 0.8882 - dense_135_accuracy: 0.7588 - dense_136_accuracy: 0.7796 - dense_137_accuracy: 0.8322 - dense_138_accuracy: 0.5663 - dense_139_accuracy: 0.8322 - loss: 1.4392\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step\n","k: 11, patience: 4\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 152ms/step - dense_124_accuracy: 0.5455 - dense_125_accuracy: 0.7763 - dense_126_accuracy: 0.8882 - dense_127_accuracy: 0.6365 - dense_128_accuracy: 0.6924 - dense_129_accuracy: 0.7867 - dense_130_accuracy: 0.5280 - dense_131_accuracy: 0.5351 - dense_132_accuracy: 0.7588 - dense_133_accuracy: 0.7308 - dense_134_accuracy: 0.7763 - dense_135_accuracy: 0.7763 - dense_136_accuracy: 0.7588 - dense_137_accuracy: 0.8147 - dense_138_accuracy: 0.4896 - dense_139_accuracy: 0.8602 - loss: 1.6667\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step\n","k: 12, patience: 0\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - dense_124_accuracy: 0.6502 - dense_125_accuracy: 0.8706 - dense_126_accuracy: 0.8986 - dense_127_accuracy: 0.7796 - dense_128_accuracy: 0.8355 - dense_129_accuracy: 0.7867 - dense_130_accuracy: 0.5943 - dense_131_accuracy: 0.6223 - dense_132_accuracy: 0.8147 - dense_133_accuracy: 0.8427 - dense_134_accuracy: 0.8882 - dense_135_accuracy: 0.7588 - dense_136_accuracy: 0.7484 - dense_137_accuracy: 0.8882 - dense_138_accuracy: 0.5384 - dense_139_accuracy: 0.8322 - loss: 1.4150\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n","k: 13, patience: 1\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - dense_124_accuracy: 0.5839 - dense_125_accuracy: 0.8882 - dense_126_accuracy: 0.8882 - dense_127_accuracy: 0.6853 - dense_128_accuracy: 0.7971 - dense_129_accuracy: 0.7867 - dense_130_accuracy: 0.5559 - dense_131_accuracy: 0.6118 - dense_132_accuracy: 0.8322 - dense_133_accuracy: 0.8322 - dense_134_accuracy: 0.8882 - dense_135_accuracy: 0.7692 - dense_136_accuracy: 0.7484 - dense_137_accuracy: 0.8602 - dense_138_accuracy: 0.5455 - dense_139_accuracy: 0.8602 - loss: 1.4271\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n","k: 14, patience: 2\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - dense_124_accuracy: 0.5839 - dense_125_accuracy: 0.8986 - dense_126_accuracy: 0.8882 - dense_127_accuracy: 0.7029 - dense_128_accuracy: 0.7867 - dense_129_accuracy: 0.7867 - dense_130_accuracy: 0.4616 - dense_131_accuracy: 0.5735 - dense_132_accuracy: 0.8427 - dense_133_accuracy: 0.8427 - dense_134_accuracy: 0.9090 - dense_135_accuracy: 0.7692 - dense_136_accuracy: 0.7588 - dense_137_accuracy: 0.8706 - dense_138_accuracy: 0.5455 - dense_139_accuracy: 0.8706 - loss: 1.4119\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n","k: 15, patience: 3\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - dense_124_accuracy: 0.5839 - dense_125_accuracy: 0.8882 - dense_126_accuracy: 0.8882 - dense_127_accuracy: 0.6749 - dense_128_accuracy: 0.7763 - dense_129_accuracy: 0.7867 - dense_130_accuracy: 0.5104 - dense_131_accuracy: 0.5488 - dense_132_accuracy: 0.8322 - dense_133_accuracy: 0.8322 - dense_134_accuracy: 0.8882 - dense_135_accuracy: 0.7692 - dense_136_accuracy: 0.7796 - dense_137_accuracy: 0.8602 - dense_138_accuracy: 0.5384 - dense_139_accuracy: 0.8706 - loss: 1.4038\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step\n","k: 16, patience: 4\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 164ms/step - dense_124_accuracy: 0.6365 - dense_125_accuracy: 0.9441 - dense_126_accuracy: 0.8882 - dense_127_accuracy: 0.7204 - dense_128_accuracy: 0.7971 - dense_129_accuracy: 0.7763 - dense_130_accuracy: 0.5104 - dense_131_accuracy: 0.6118 - dense_132_accuracy: 0.8322 - dense_133_accuracy: 0.8427 - dense_134_accuracy: 0.9161 - dense_135_accuracy: 0.7588 - dense_136_accuracy: 0.7588 - dense_137_accuracy: 0.8602 - dense_138_accuracy: 0.5175 - dense_139_accuracy: 0.8602 - loss: 1.3869\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step\n","k: 17, patience: 5\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 180ms/step - dense_124_accuracy: 0.7341 - dense_125_accuracy: 0.9090 - dense_126_accuracy: 0.9194 - dense_127_accuracy: 0.8531 - dense_128_accuracy: 0.7867 - dense_129_accuracy: 0.7867 - dense_130_accuracy: 0.5488 - dense_131_accuracy: 0.5735 - dense_132_accuracy: 0.8427 - dense_133_accuracy: 0.8531 - dense_134_accuracy: 0.8986 - dense_135_accuracy: 0.7796 - dense_136_accuracy: 0.7692 - dense_137_accuracy: 0.8706 - dense_138_accuracy: 0.6678 - dense_139_accuracy: 0.8810 - loss: 1.2455\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n","k: 18, patience: 6\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 155ms/step - dense_124_accuracy: 0.6573 - dense_125_accuracy: 0.9370 - dense_126_accuracy: 0.9194 - dense_127_accuracy: 0.7029 - dense_128_accuracy: 0.7971 - dense_129_accuracy: 0.7971 - dense_130_accuracy: 0.5071 - dense_131_accuracy: 0.6223 - dense_132_accuracy: 0.8531 - dense_133_accuracy: 0.8635 - dense_134_accuracy: 0.9265 - dense_135_accuracy: 0.7900 - dense_136_accuracy: 0.7900 - dense_137_accuracy: 0.8706 - dense_138_accuracy: 0.5559 - dense_139_accuracy: 0.8706 - loss: 1.2348\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step\n","k: 19, patience: 7\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 159ms/step - dense_124_accuracy: 0.6190 - dense_125_accuracy: 0.9265 - dense_126_accuracy: 0.8882 - dense_127_accuracy: 0.7133 - dense_128_accuracy: 0.7763 - dense_129_accuracy: 0.7763 - dense_130_accuracy: 0.5526 - dense_131_accuracy: 0.5735 - dense_132_accuracy: 0.8427 - dense_133_accuracy: 0.8427 - dense_134_accuracy: 0.9441 - dense_135_accuracy: 0.7484 - dense_136_accuracy: 0.7484 - dense_137_accuracy: 0.8706 - dense_138_accuracy: 0.4688 - dense_139_accuracy: 0.8602 - loss: 1.3246\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n","k: 20, patience: 8\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - dense_124_accuracy: 0.6749 - dense_125_accuracy: 0.8706 - dense_126_accuracy: 0.8986 - dense_127_accuracy: 0.7692 - dense_128_accuracy: 0.8043 - dense_129_accuracy: 0.7867 - dense_130_accuracy: 0.5175 - dense_131_accuracy: 0.6294 - dense_132_accuracy: 0.8531 - dense_133_accuracy: 0.8531 - dense_134_accuracy: 0.9265 - dense_135_accuracy: 0.7692 - dense_136_accuracy: 0.7867 - dense_137_accuracy: 0.8810 - dense_138_accuracy: 0.5839 - dense_139_accuracy: 0.8706 - loss: 1.2852\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n","k: 21, patience: 9\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - dense_124_accuracy: 0.7308 - dense_125_accuracy: 0.8986 - dense_126_accuracy: 0.8882 - dense_127_accuracy: 0.7692 - dense_128_accuracy: 0.7692 - dense_129_accuracy: 0.7763 - dense_130_accuracy: 0.5104 - dense_131_accuracy: 0.6469 - dense_132_accuracy: 0.8427 - dense_133_accuracy: 0.8531 - dense_134_accuracy: 0.9265 - dense_135_accuracy: 0.7621 - dense_136_accuracy: 0.8147 - dense_137_accuracy: 0.8602 - dense_138_accuracy: 0.5559 - dense_139_accuracy: 0.8706 - loss: 1.2589\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n","k: 22, patience: 10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 805ms/step\n","Test Loss iteration 0: 30.138343932605192\n","input_train : (38, 120, 47)\n","output_train : (38, 16, 4)\n","input_val : (14, 120, 47)\n","output_val : (14, 16, 4)\n","input_test : (16, 120, 47)\n","output_test : (16, 16)\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 98ms/step - dense_144_accuracy: 0.2659 - dense_145_accuracy: 0.2275 - dense_146_accuracy: 0.4616 - dense_147_accuracy: 0.4720 - dense_148_accuracy: 0.5663 - dense_149_accuracy: 0.0630 - dense_150_accuracy: 0.2659 - dense_151_accuracy: 0.3498 - dense_152_accuracy: 0.0526 - dense_153_accuracy: 0.0280 - dense_154_accuracy: 0.5806 - dense_155_accuracy: 0.1190 - dense_156_accuracy: 0.1294 - dense_157_accuracy: 0.2379 - dense_158_accuracy: 0.0910 - dense_159_accuracy: 0.0455 - loss: 3.0598\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 754ms/step\n","k: 0, patience: 0\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - dense_144_accuracy: 0.5280 - dense_145_accuracy: 0.5663 - dense_146_accuracy: 0.8706 - dense_147_accuracy: 0.6118 - dense_148_accuracy: 0.7516 - dense_149_accuracy: 0.3218 - dense_150_accuracy: 0.2516 - dense_151_accuracy: 0.6678 - dense_152_accuracy: 0.8635 - dense_153_accuracy: 0.7412 - dense_154_accuracy: 0.9090 - dense_155_accuracy: 0.7237 - dense_156_accuracy: 0.6957 - dense_157_accuracy: 0.7412 - dense_158_accuracy: 0.3251 - dense_159_accuracy: 0.7308 - loss: 2.3049\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n","k: 1, patience: 0\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - dense_144_accuracy: 0.5175 - dense_145_accuracy: 0.7133 - dense_146_accuracy: 0.8602 - dense_147_accuracy: 0.5910 - dense_148_accuracy: 0.7412 - dense_149_accuracy: 0.8427 - dense_150_accuracy: 0.4616 - dense_151_accuracy: 0.6573 - dense_152_accuracy: 0.8427 - dense_153_accuracy: 0.7867 - dense_154_accuracy: 0.8986 - dense_155_accuracy: 0.7237 - dense_156_accuracy: 0.7516 - dense_157_accuracy: 0.8706 - dense_158_accuracy: 0.2763 - dense_159_accuracy: 0.8602 - loss: 1.8283\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n","k: 2, patience: 1\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - dense_144_accuracy: 0.5559 - dense_145_accuracy: 0.7237 - dense_146_accuracy: 0.8602 - dense_147_accuracy: 0.6223 - dense_148_accuracy: 0.7308 - dense_149_accuracy: 0.8322 - dense_150_accuracy: 0.5839 - dense_151_accuracy: 0.6469 - dense_152_accuracy: 0.8322 - dense_153_accuracy: 0.7763 - dense_154_accuracy: 0.8882 - dense_155_accuracy: 0.6924 - dense_156_accuracy: 0.7204 - dense_157_accuracy: 0.8602 - dense_158_accuracy: 0.5384 - dense_159_accuracy: 0.8706 - loss: 1.6527\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n","k: 3, patience: 2\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - dense_144_accuracy: 0.4649 - dense_145_accuracy: 0.7204 - dense_146_accuracy: 0.8602 - dense_147_accuracy: 0.4825 - dense_148_accuracy: 0.7412 - dense_149_accuracy: 0.8427 - dense_150_accuracy: 0.5735 - dense_151_accuracy: 0.6678 - dense_152_accuracy: 0.8427 - dense_153_accuracy: 0.7867 - dense_154_accuracy: 0.8986 - dense_155_accuracy: 0.7237 - dense_156_accuracy: 0.7516 - dense_157_accuracy: 0.8706 - dense_158_accuracy: 0.5735 - dense_159_accuracy: 0.8602 - loss: 1.6276\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n","k: 4, patience: 3\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - dense_144_accuracy: 0.5768 - dense_145_accuracy: 0.6853 - dense_146_accuracy: 0.8602 - dense_147_accuracy: 0.5280 - dense_148_accuracy: 0.7308 - dense_149_accuracy: 0.8322 - dense_150_accuracy: 0.5735 - dense_151_accuracy: 0.6573 - dense_152_accuracy: 0.8322 - dense_153_accuracy: 0.7867 - dense_154_accuracy: 0.8882 - dense_155_accuracy: 0.7029 - dense_156_accuracy: 0.7308 - dense_157_accuracy: 0.8706 - dense_158_accuracy: 0.3322 - dense_159_accuracy: 0.8602 - loss: 1.6589\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n","k: 5, patience: 4\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - dense_144_accuracy: 0.5175 - dense_145_accuracy: 0.7133 - dense_146_accuracy: 0.8706 - dense_147_accuracy: 0.6294 - dense_148_accuracy: 0.7308 - dense_149_accuracy: 0.8427 - dense_150_accuracy: 0.6853 - dense_151_accuracy: 0.6573 - dense_152_accuracy: 0.8322 - dense_153_accuracy: 0.7867 - dense_154_accuracy: 0.8986 - dense_155_accuracy: 0.7692 - dense_156_accuracy: 0.7308 - dense_157_accuracy: 0.8602 - dense_158_accuracy: 0.4896 - dense_159_accuracy: 0.8706 - loss: 1.5359\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n","k: 6, patience: 5\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - dense_144_accuracy: 0.5559 - dense_145_accuracy: 0.7412 - dense_146_accuracy: 0.8706 - dense_147_accuracy: 0.5839 - dense_148_accuracy: 0.7308 - dense_149_accuracy: 0.8322 - dense_150_accuracy: 0.6118 - dense_151_accuracy: 0.6294 - dense_152_accuracy: 0.8427 - dense_153_accuracy: 0.7971 - dense_154_accuracy: 0.8986 - dense_155_accuracy: 0.6924 - dense_156_accuracy: 0.7204 - dense_157_accuracy: 0.8706 - dense_158_accuracy: 0.5910 - dense_159_accuracy: 0.8602 - loss: 1.5405\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\n","k: 7, patience: 6\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 148ms/step - dense_144_accuracy: 0.6573 - dense_145_accuracy: 0.7588 - dense_146_accuracy: 0.8706 - dense_147_accuracy: 0.5735 - dense_148_accuracy: 0.7412 - dense_149_accuracy: 0.8427 - dense_150_accuracy: 0.6294 - dense_151_accuracy: 0.6749 - dense_152_accuracy: 0.8427 - dense_153_accuracy: 0.7867 - dense_154_accuracy: 0.8986 - dense_155_accuracy: 0.7308 - dense_156_accuracy: 0.7867 - dense_157_accuracy: 0.8810 - dense_158_accuracy: 0.5735 - dense_159_accuracy: 0.8810 - loss: 1.4307\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step\n","k: 8, patience: 7\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 148ms/step - dense_144_accuracy: 0.6190 - dense_145_accuracy: 0.7971 - dense_146_accuracy: 0.8602 - dense_147_accuracy: 0.5839 - dense_148_accuracy: 0.7412 - dense_149_accuracy: 0.8427 - dense_150_accuracy: 0.6014 - dense_151_accuracy: 0.7029 - dense_152_accuracy: 0.8427 - dense_153_accuracy: 0.7412 - dense_154_accuracy: 0.9161 - dense_155_accuracy: 0.6853 - dense_156_accuracy: 0.7133 - dense_157_accuracy: 0.8706 - dense_158_accuracy: 0.5559 - dense_159_accuracy: 0.8706 - loss: 1.4630\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n","k: 9, patience: 8\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 116ms/step - dense_144_accuracy: 0.6573 - dense_145_accuracy: 0.7971 - dense_146_accuracy: 0.8810 - dense_147_accuracy: 0.6190 - dense_148_accuracy: 0.7516 - dense_149_accuracy: 0.8810 - dense_150_accuracy: 0.6014 - dense_151_accuracy: 0.7237 - dense_152_accuracy: 0.8635 - dense_153_accuracy: 0.8251 - dense_154_accuracy: 0.9090 - dense_155_accuracy: 0.7061 - dense_156_accuracy: 0.7621 - dense_157_accuracy: 0.8914 - dense_158_accuracy: 0.6223 - dense_159_accuracy: 0.8810 - loss: 1.4070\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n","k: 10, patience: 9\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - dense_144_accuracy: 0.6678 - dense_145_accuracy: 0.8427 - dense_146_accuracy: 0.8602 - dense_147_accuracy: 0.6294 - dense_148_accuracy: 0.7204 - dense_149_accuracy: 0.8706 - dense_150_accuracy: 0.6014 - dense_151_accuracy: 0.7029 - dense_152_accuracy: 0.8427 - dense_153_accuracy: 0.7484 - dense_154_accuracy: 0.9161 - dense_155_accuracy: 0.7412 - dense_156_accuracy: 0.7763 - dense_157_accuracy: 0.8706 - dense_158_accuracy: 0.5910 - dense_159_accuracy: 0.8882 - loss: 1.4049\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n","k: 11, patience: 10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 788ms/step\n","Test Loss iteration 1: 33.595006697424544\n","input_train : (38, 120, 47)\n","output_train : (38, 16, 4)\n","input_val : (14, 120, 47)\n","output_val : (14, 16, 4)\n","input_test : (16, 120, 47)\n","output_test : (16, 16)\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 84ms/step - dense_164_accuracy: 0.1924 - dense_165_accuracy: 0.0735 - dense_166_accuracy: 0.0175 - dense_167_accuracy: 0.0559 - dense_168_accuracy: 0.0981 - dense_169_accuracy: 0.1716 - dense_170_accuracy: 0.5910 - dense_171_accuracy: 0.2100 - dense_172_accuracy: 0.7133 - dense_173_accuracy: 0.4649 - dense_174_accuracy: 0.3322 - dense_175_accuracy: 0.5630 - dense_176_accuracy: 0.6118 - dense_177_accuracy: 0.4232 - dense_178_accuracy: 0.0839 - dense_179_accuracy: 0.0175 - loss: 3.0632\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 736ms/step\n","k: 0, patience: 0\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - dense_164_accuracy: 0.3147 - dense_165_accuracy: 0.1749 - dense_166_accuracy: 0.2763 - dense_167_accuracy: 0.0839 - dense_168_accuracy: 0.7588 - dense_169_accuracy: 0.7867 - dense_170_accuracy: 0.5384 - dense_171_accuracy: 0.6223 - dense_172_accuracy: 0.8427 - dense_173_accuracy: 0.7971 - dense_174_accuracy: 0.9441 - dense_175_accuracy: 0.8043 - dense_176_accuracy: 0.7692 - dense_177_accuracy: 0.9161 - dense_178_accuracy: 0.3147 - dense_179_accuracy: 0.8043 - loss: 2.3420\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n","k: 1, patience: 0\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - dense_164_accuracy: 0.2867 - dense_165_accuracy: 0.7971 - dense_166_accuracy: 0.8322 - dense_167_accuracy: 0.4441 - dense_168_accuracy: 0.7867 - dense_169_accuracy: 0.7971 - dense_170_accuracy: 0.6014 - dense_171_accuracy: 0.6573 - dense_172_accuracy: 0.8427 - dense_173_accuracy: 0.8147 - dense_174_accuracy: 0.9545 - dense_175_accuracy: 0.8251 - dense_176_accuracy: 0.7588 - dense_177_accuracy: 0.9370 - dense_178_accuracy: 0.4194 - dense_179_accuracy: 0.9161 - loss: 1.6752\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n","k: 2, patience: 1\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 177ms/step - dense_164_accuracy: 0.4128 - dense_165_accuracy: 0.7867 - dense_166_accuracy: 0.8986 - dense_167_accuracy: 0.5280 - dense_168_accuracy: 0.8076 - dense_169_accuracy: 0.7971 - dense_170_accuracy: 0.6223 - dense_171_accuracy: 0.6678 - dense_172_accuracy: 0.8531 - dense_173_accuracy: 0.8355 - dense_174_accuracy: 0.9545 - dense_175_accuracy: 0.8251 - dense_176_accuracy: 0.7588 - dense_177_accuracy: 0.9265 - dense_178_accuracy: 0.5839 - dense_179_accuracy: 0.9265 - loss: 1.4806\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n","k: 3, patience: 2\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 153ms/step - dense_164_accuracy: 0.5943 - dense_165_accuracy: 0.7971 - dense_166_accuracy: 0.8882 - dense_167_accuracy: 0.5559 - dense_168_accuracy: 0.8076 - dense_169_accuracy: 0.7867 - dense_170_accuracy: 0.6014 - dense_171_accuracy: 0.6469 - dense_172_accuracy: 0.8635 - dense_173_accuracy: 0.8043 - dense_174_accuracy: 0.9545 - dense_175_accuracy: 0.8251 - dense_176_accuracy: 0.7588 - dense_177_accuracy: 0.9265 - dense_178_accuracy: 0.5735 - dense_179_accuracy: 0.9265 - loss: 1.4809\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n","k: 4, patience: 3\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 158ms/step - dense_164_accuracy: 0.5175 - dense_165_accuracy: 0.7971 - dense_166_accuracy: 0.8882 - dense_167_accuracy: 0.5630 - dense_168_accuracy: 0.7763 - dense_169_accuracy: 0.7867 - dense_170_accuracy: 0.6223 - dense_171_accuracy: 0.6190 - dense_172_accuracy: 0.8427 - dense_173_accuracy: 0.8043 - dense_174_accuracy: 0.9441 - dense_175_accuracy: 0.8043 - dense_176_accuracy: 0.7484 - dense_177_accuracy: 0.9161 - dense_178_accuracy: 0.6118 - dense_179_accuracy: 0.9161 - loss: 1.5468\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n","k: 5, patience: 4\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 163ms/step - dense_164_accuracy: 0.4370 - dense_165_accuracy: 0.7763 - dense_166_accuracy: 0.8882 - dense_167_accuracy: 0.5839 - dense_168_accuracy: 0.7763 - dense_169_accuracy: 0.7971 - dense_170_accuracy: 0.6223 - dense_171_accuracy: 0.6678 - dense_172_accuracy: 0.8427 - dense_173_accuracy: 0.8043 - dense_174_accuracy: 0.9441 - dense_175_accuracy: 0.8043 - dense_176_accuracy: 0.7692 - dense_177_accuracy: 0.9161 - dense_178_accuracy: 0.5559 - dense_179_accuracy: 0.9161 - loss: 1.4957\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n","k: 6, patience: 5\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - dense_164_accuracy: 0.4720 - dense_165_accuracy: 0.7588 - dense_166_accuracy: 0.8882 - dense_167_accuracy: 0.5559 - dense_168_accuracy: 0.7971 - dense_169_accuracy: 0.8076 - dense_170_accuracy: 0.5384 - dense_171_accuracy: 0.6678 - dense_172_accuracy: 0.8251 - dense_173_accuracy: 0.8043 - dense_174_accuracy: 0.9545 - dense_175_accuracy: 0.8355 - dense_176_accuracy: 0.7692 - dense_177_accuracy: 0.9265 - dense_178_accuracy: 0.5104 - dense_179_accuracy: 0.9265 - loss: 1.4479\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n","k: 7, patience: 6\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - dense_164_accuracy: 0.5208 - dense_165_accuracy: 0.7867 - dense_166_accuracy: 0.8986 - dense_167_accuracy: 0.6190 - dense_168_accuracy: 0.7867 - dense_169_accuracy: 0.8147 - dense_170_accuracy: 0.5104 - dense_171_accuracy: 0.6573 - dense_172_accuracy: 0.8706 - dense_173_accuracy: 0.8147 - dense_174_accuracy: 0.9441 - dense_175_accuracy: 0.8147 - dense_176_accuracy: 0.7867 - dense_177_accuracy: 0.9161 - dense_178_accuracy: 0.5384 - dense_179_accuracy: 0.9265 - loss: 1.3976\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n","k: 8, patience: 7\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - dense_164_accuracy: 0.5280 - dense_165_accuracy: 0.7763 - dense_166_accuracy: 0.8986 - dense_167_accuracy: 0.6957 - dense_168_accuracy: 0.8147 - dense_169_accuracy: 0.7867 - dense_170_accuracy: 0.5071 - dense_171_accuracy: 0.6573 - dense_172_accuracy: 0.8147 - dense_173_accuracy: 0.8251 - dense_174_accuracy: 0.9441 - dense_175_accuracy: 0.8043 - dense_176_accuracy: 0.8427 - dense_177_accuracy: 0.9161 - dense_178_accuracy: 0.5455 - dense_179_accuracy: 0.9161 - loss: 1.3894\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n","k: 9, patience: 8\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - dense_164_accuracy: 0.5839 - dense_165_accuracy: 0.7971 - dense_166_accuracy: 0.8986 - dense_167_accuracy: 0.6365 - dense_168_accuracy: 0.7796 - dense_169_accuracy: 0.8322 - dense_170_accuracy: 0.6118 - dense_171_accuracy: 0.6853 - dense_172_accuracy: 0.8251 - dense_173_accuracy: 0.8147 - dense_174_accuracy: 0.9545 - dense_175_accuracy: 0.8251 - dense_176_accuracy: 0.8531 - dense_177_accuracy: 0.9370 - dense_178_accuracy: 0.5559 - dense_179_accuracy: 0.9370 - loss: 1.3036\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n","k: 10, patience: 9\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - dense_164_accuracy: 0.6118 - dense_165_accuracy: 0.7971 - dense_166_accuracy: 0.8882 - dense_167_accuracy: 0.7237 - dense_168_accuracy: 0.8635 - dense_169_accuracy: 0.8043 - dense_170_accuracy: 0.6327 - dense_171_accuracy: 0.6782 - dense_172_accuracy: 0.8602 - dense_173_accuracy: 0.8322 - dense_174_accuracy: 0.9441 - dense_175_accuracy: 0.8251 - dense_176_accuracy: 0.8427 - dense_177_accuracy: 0.9161 - dense_178_accuracy: 0.6118 - dense_179_accuracy: 0.9265 - loss: 1.2691\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n","k: 11, patience: 10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 841ms/step\n","Test Loss iteration 2: 54.25417564575099\n","input_train : (38, 120, 47)\n","output_train : (38, 16, 4)\n","input_val : (14, 120, 47)\n","output_val : (14, 16, 4)\n","input_test : (16, 120, 47)\n","output_test : (16, 16)\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 105ms/step - dense_184_accuracy: 0.2133 - dense_185_accuracy: 0.4967 - dense_186_accuracy: 0.0559 - dense_187_accuracy: 0.1957 - dense_188_accuracy: 0.5071 - dense_189_accuracy: 0.2275 - dense_190_accuracy: 0.3322 - dense_191_accuracy: 0.2133 - dense_192_accuracy: 0.2834 - dense_193_accuracy: 0.4512 - dense_194_accuracy: 0.0877 - dense_195_accuracy: 0.7484 - dense_196_accuracy: 0.0910 - dense_197_accuracy: 0.0630 - dense_198_accuracy: 0.2834 - dense_199_accuracy: 0.5526 - loss: 3.0069\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 842ms/step\n","k: 0, patience: 0\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - dense_184_accuracy: 0.5104 - dense_185_accuracy: 0.6678 - dense_186_accuracy: 0.1924 - dense_187_accuracy: 0.5839 - dense_188_accuracy: 0.7588 - dense_189_accuracy: 0.8180 - dense_190_accuracy: 0.5559 - dense_191_accuracy: 0.1853 - dense_192_accuracy: 0.7971 - dense_193_accuracy: 0.8810 - dense_194_accuracy: 0.5630 - dense_195_accuracy: 0.7588 - dense_196_accuracy: 0.6294 - dense_197_accuracy: 0.4232 - dense_198_accuracy: 0.5208 - dense_199_accuracy: 0.8427 - loss: 2.2885\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n","k: 1, patience: 0\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - dense_184_accuracy: 0.5208 - dense_185_accuracy: 0.6573 - dense_186_accuracy: 0.8531 - dense_187_accuracy: 0.6118 - dense_188_accuracy: 0.7692 - dense_189_accuracy: 0.7763 - dense_190_accuracy: 0.5663 - dense_191_accuracy: 0.4896 - dense_192_accuracy: 0.7763 - dense_193_accuracy: 0.8810 - dense_194_accuracy: 0.8882 - dense_195_accuracy: 0.7692 - dense_196_accuracy: 0.7308 - dense_197_accuracy: 0.8882 - dense_198_accuracy: 0.5280 - dense_199_accuracy: 0.8427 - loss: 1.7703\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n","k: 2, patience: 1\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - dense_184_accuracy: 0.4649 - dense_185_accuracy: 0.6678 - dense_186_accuracy: 0.9090 - dense_187_accuracy: 0.6502 - dense_188_accuracy: 0.7588 - dense_189_accuracy: 0.7867 - dense_190_accuracy: 0.5455 - dense_191_accuracy: 0.6118 - dense_192_accuracy: 0.7971 - dense_193_accuracy: 0.8706 - dense_194_accuracy: 0.8882 - dense_195_accuracy: 0.7484 - dense_196_accuracy: 0.7308 - dense_197_accuracy: 0.8986 - dense_198_accuracy: 0.5280 - dense_199_accuracy: 0.8427 - loss: 1.6423\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n","k: 3, patience: 2\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - dense_184_accuracy: 0.4545 - dense_185_accuracy: 0.6398 - dense_186_accuracy: 0.8986 - dense_187_accuracy: 0.6294 - dense_188_accuracy: 0.7588 - dense_189_accuracy: 0.7867 - dense_190_accuracy: 0.4896 - dense_191_accuracy: 0.6190 - dense_192_accuracy: 0.7971 - dense_193_accuracy: 0.8602 - dense_194_accuracy: 0.8882 - dense_195_accuracy: 0.7484 - dense_196_accuracy: 0.7204 - dense_197_accuracy: 0.8986 - dense_198_accuracy: 0.5280 - dense_199_accuracy: 0.8322 - loss: 1.6731\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n","k: 4, patience: 3\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 145ms/step - dense_184_accuracy: 0.4896 - dense_185_accuracy: 0.6853 - dense_186_accuracy: 0.8882 - dense_187_accuracy: 0.6398 - dense_188_accuracy: 0.7588 - dense_189_accuracy: 0.7867 - dense_190_accuracy: 0.5455 - dense_191_accuracy: 0.6294 - dense_192_accuracy: 0.7971 - dense_193_accuracy: 0.8602 - dense_194_accuracy: 0.8882 - dense_195_accuracy: 0.7692 - dense_196_accuracy: 0.7412 - dense_197_accuracy: 0.8882 - dense_198_accuracy: 0.5488 - dense_199_accuracy: 0.8322 - loss: 1.6310\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n","k: 5, patience: 4\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 138ms/step - dense_184_accuracy: 0.5071 - dense_185_accuracy: 0.6853 - dense_186_accuracy: 0.8882 - dense_187_accuracy: 0.5735 - dense_188_accuracy: 0.7484 - dense_189_accuracy: 0.7763 - dense_190_accuracy: 0.5455 - dense_191_accuracy: 0.6190 - dense_192_accuracy: 0.7867 - dense_193_accuracy: 0.8602 - dense_194_accuracy: 0.8882 - dense_195_accuracy: 0.7308 - dense_196_accuracy: 0.7308 - dense_197_accuracy: 0.8882 - dense_198_accuracy: 0.5000 - dense_199_accuracy: 0.8531 - loss: 1.6364\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n","k: 6, patience: 5\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 154ms/step - dense_184_accuracy: 0.4896 - dense_185_accuracy: 0.6924 - dense_186_accuracy: 0.8882 - dense_187_accuracy: 0.6469 - dense_188_accuracy: 0.7484 - dense_189_accuracy: 0.7763 - dense_190_accuracy: 0.5455 - dense_191_accuracy: 0.6190 - dense_192_accuracy: 0.7763 - dense_193_accuracy: 0.8602 - dense_194_accuracy: 0.8882 - dense_195_accuracy: 0.7484 - dense_196_accuracy: 0.7204 - dense_197_accuracy: 0.8882 - dense_198_accuracy: 0.4057 - dense_199_accuracy: 0.8427 - loss: 1.6258\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step\n","k: 7, patience: 6\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - dense_184_accuracy: 0.4896 - dense_185_accuracy: 0.6782 - dense_186_accuracy: 0.8986 - dense_187_accuracy: 0.6398 - dense_188_accuracy: 0.7588 - dense_189_accuracy: 0.7867 - dense_190_accuracy: 0.5559 - dense_191_accuracy: 0.6190 - dense_192_accuracy: 0.7763 - dense_193_accuracy: 0.8602 - dense_194_accuracy: 0.8986 - dense_195_accuracy: 0.7588 - dense_196_accuracy: 0.7204 - dense_197_accuracy: 0.8882 - dense_198_accuracy: 0.6118 - dense_199_accuracy: 0.8322 - loss: 1.5815\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n","k: 8, patience: 7\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - dense_184_accuracy: 0.5280 - dense_185_accuracy: 0.7237 - dense_186_accuracy: 0.8882 - dense_187_accuracy: 0.6678 - dense_188_accuracy: 0.7796 - dense_189_accuracy: 0.7867 - dense_190_accuracy: 0.5559 - dense_191_accuracy: 0.6294 - dense_192_accuracy: 0.7971 - dense_193_accuracy: 0.8706 - dense_194_accuracy: 0.8986 - dense_195_accuracy: 0.7588 - dense_196_accuracy: 0.7308 - dense_197_accuracy: 0.8986 - dense_198_accuracy: 0.4792 - dense_199_accuracy: 0.8322 - loss: 1.4894\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n","k: 9, patience: 8\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - dense_184_accuracy: 0.6327 - dense_185_accuracy: 0.6853 - dense_186_accuracy: 0.8986 - dense_187_accuracy: 0.6469 - dense_188_accuracy: 0.8076 - dense_189_accuracy: 0.7971 - dense_190_accuracy: 0.4720 - dense_191_accuracy: 0.6502 - dense_192_accuracy: 0.8180 - dense_193_accuracy: 0.8706 - dense_194_accuracy: 0.9090 - dense_195_accuracy: 0.7692 - dense_196_accuracy: 0.7692 - dense_197_accuracy: 0.9090 - dense_198_accuracy: 0.5280 - dense_199_accuracy: 0.8531 - loss: 1.4475\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n","k: 10, patience: 9\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - dense_184_accuracy: 0.6190 - dense_185_accuracy: 0.5351 - dense_186_accuracy: 0.8882 - dense_187_accuracy: 0.6398 - dense_188_accuracy: 0.6957 - dense_189_accuracy: 0.8076 - dense_190_accuracy: 0.3986 - dense_191_accuracy: 0.5735 - dense_192_accuracy: 0.7692 - dense_193_accuracy: 0.8706 - dense_194_accuracy: 0.8986 - dense_195_accuracy: 0.7692 - dense_196_accuracy: 0.7029 - dense_197_accuracy: 0.8986 - dense_198_accuracy: 0.5663 - dense_199_accuracy: 0.8427 - loss: 1.5316\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n","k: 11, patience: 10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 883ms/step\n","Test Loss iteration 3: 23.073860210203236\n","input_train : (38, 120, 47)\n","output_train : (38, 16, 4)\n","input_val : (14, 120, 47)\n","output_val : (14, 16, 4)\n","input_test : (16, 120, 47)\n","output_test : (16, 16)\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 93ms/step - dense_204_accuracy: 0.4090 - dense_205_accuracy: 0.1398 - dense_206_accuracy: 0.5806 - dense_207_accuracy: 0.1261 - dense_208_accuracy: 0.4792 - dense_209_accuracy: 0.1749 - dense_210_accuracy: 0.1853 - dense_211_accuracy: 0.5839 - dense_212_accuracy: 0.0559 - dense_213_accuracy: 0.5351 - dense_214_accuracy: 0.8043 - dense_215_accuracy: 0.2659 - dense_216_accuracy: 0.1436 - dense_217_accuracy: 0.4792 - dense_218_accuracy: 0.2971 - dense_219_accuracy: 0.6190 - loss: 2.9238\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 781ms/step\n","k: 0, patience: 0\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - dense_204_accuracy: 0.3322 - dense_205_accuracy: 0.7971 - dense_206_accuracy: 0.9441 - dense_207_accuracy: 0.7237 - dense_208_accuracy: 0.7588 - dense_209_accuracy: 0.6645 - dense_210_accuracy: 0.2763 - dense_211_accuracy: 0.6014 - dense_212_accuracy: 0.7763 - dense_213_accuracy: 0.8322 - dense_214_accuracy: 0.9161 - dense_215_accuracy: 0.5000 - dense_216_accuracy: 0.6190 - dense_217_accuracy: 0.8706 - dense_218_accuracy: 0.3602 - dense_219_accuracy: 0.8986 - loss: 2.0687\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n","k: 1, patience: 0\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - dense_204_accuracy: 0.5104 - dense_205_accuracy: 0.7971 - dense_206_accuracy: 0.9545 - dense_207_accuracy: 0.7412 - dense_208_accuracy: 0.7588 - dense_209_accuracy: 0.7692 - dense_210_accuracy: 0.5351 - dense_211_accuracy: 0.6014 - dense_212_accuracy: 0.8147 - dense_213_accuracy: 0.8531 - dense_214_accuracy: 0.9161 - dense_215_accuracy: 0.7133 - dense_216_accuracy: 0.6853 - dense_217_accuracy: 0.8602 - dense_218_accuracy: 0.5104 - dense_219_accuracy: 0.8882 - loss: 1.5733\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n","k: 2, patience: 1\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - dense_204_accuracy: 0.4825 - dense_205_accuracy: 0.8076 - dense_206_accuracy: 0.9545 - dense_207_accuracy: 0.7412 - dense_208_accuracy: 0.7484 - dense_209_accuracy: 0.7588 - dense_210_accuracy: 0.5630 - dense_211_accuracy: 0.5943 - dense_212_accuracy: 0.8043 - dense_213_accuracy: 0.8427 - dense_214_accuracy: 0.9161 - dense_215_accuracy: 0.7029 - dense_216_accuracy: 0.6957 - dense_217_accuracy: 0.8706 - dense_218_accuracy: 0.5104 - dense_219_accuracy: 0.8882 - loss: 1.6053\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n","k: 3, patience: 2\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - dense_204_accuracy: 0.4337 - dense_205_accuracy: 0.7763 - dense_206_accuracy: 0.9441 - dense_207_accuracy: 0.7308 - dense_208_accuracy: 0.7588 - dense_209_accuracy: 0.7692 - dense_210_accuracy: 0.5839 - dense_211_accuracy: 0.6223 - dense_212_accuracy: 0.8043 - dense_213_accuracy: 0.8322 - dense_214_accuracy: 0.9161 - dense_215_accuracy: 0.6749 - dense_216_accuracy: 0.6853 - dense_217_accuracy: 0.8602 - dense_218_accuracy: 0.5104 - dense_219_accuracy: 0.8882 - loss: 1.6528\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n","k: 4, patience: 3\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - dense_204_accuracy: 0.3147 - dense_205_accuracy: 0.8076 - dense_206_accuracy: 0.9441 - dense_207_accuracy: 0.7412 - dense_208_accuracy: 0.7796 - dense_209_accuracy: 0.7796 - dense_210_accuracy: 0.5839 - dense_211_accuracy: 0.5839 - dense_212_accuracy: 0.8355 - dense_213_accuracy: 0.8322 - dense_214_accuracy: 0.9370 - dense_215_accuracy: 0.7061 - dense_216_accuracy: 0.6957 - dense_217_accuracy: 0.8810 - dense_218_accuracy: 0.4825 - dense_219_accuracy: 0.9090 - loss: 1.5691\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n","k: 5, patience: 4\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - dense_204_accuracy: 0.4545 - dense_205_accuracy: 0.7763 - dense_206_accuracy: 0.9441 - dense_207_accuracy: 0.7308 - dense_208_accuracy: 0.7484 - dense_209_accuracy: 0.7692 - dense_210_accuracy: 0.5735 - dense_211_accuracy: 0.4161 - dense_212_accuracy: 0.8043 - dense_213_accuracy: 0.8322 - dense_214_accuracy: 0.9161 - dense_215_accuracy: 0.7204 - dense_216_accuracy: 0.6573 - dense_217_accuracy: 0.8602 - dense_218_accuracy: 0.3914 - dense_219_accuracy: 0.8882 - loss: 1.6149\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n","k: 6, patience: 5\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 171ms/step - dense_204_accuracy: 0.4545 - dense_205_accuracy: 0.7971 - dense_206_accuracy: 0.9441 - dense_207_accuracy: 0.7308 - dense_208_accuracy: 0.7588 - dense_209_accuracy: 0.7796 - dense_210_accuracy: 0.5943 - dense_211_accuracy: 0.6398 - dense_212_accuracy: 0.8251 - dense_213_accuracy: 0.8322 - dense_214_accuracy: 0.9265 - dense_215_accuracy: 0.7133 - dense_216_accuracy: 0.6398 - dense_217_accuracy: 0.8706 - dense_218_accuracy: 0.3602 - dense_219_accuracy: 0.8986 - loss: 1.5376\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step\n","k: 7, patience: 6\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 163ms/step - dense_204_accuracy: 0.4896 - dense_205_accuracy: 0.8147 - dense_206_accuracy: 0.9441 - dense_207_accuracy: 0.7204 - dense_208_accuracy: 0.7484 - dense_209_accuracy: 0.7588 - dense_210_accuracy: 0.5910 - dense_211_accuracy: 0.5910 - dense_212_accuracy: 0.8043 - dense_213_accuracy: 0.8322 - dense_214_accuracy: 0.9161 - dense_215_accuracy: 0.7412 - dense_216_accuracy: 0.6645 - dense_217_accuracy: 0.8602 - dense_218_accuracy: 0.4616 - dense_219_accuracy: 0.8882 - loss: 1.5494\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step\n","k: 8, patience: 7\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 209ms/step - dense_204_accuracy: 0.5000 - dense_205_accuracy: 0.7971 - dense_206_accuracy: 0.9441 - dense_207_accuracy: 0.7621 - dense_208_accuracy: 0.7692 - dense_209_accuracy: 0.7763 - dense_210_accuracy: 0.5839 - dense_211_accuracy: 0.6118 - dense_212_accuracy: 0.8147 - dense_213_accuracy: 0.8427 - dense_214_accuracy: 0.9161 - dense_215_accuracy: 0.7133 - dense_216_accuracy: 0.6853 - dense_217_accuracy: 0.8706 - dense_218_accuracy: 0.5104 - dense_219_accuracy: 0.8986 - loss: 1.5033\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n","k: 9, patience: 8\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - dense_204_accuracy: 0.5104 - dense_205_accuracy: 0.8043 - dense_206_accuracy: 0.9441 - dense_207_accuracy: 0.7204 - dense_208_accuracy: 0.7484 - dense_209_accuracy: 0.7692 - dense_210_accuracy: 0.6118 - dense_211_accuracy: 0.6573 - dense_212_accuracy: 0.8043 - dense_213_accuracy: 0.8322 - dense_214_accuracy: 0.9161 - dense_215_accuracy: 0.6853 - dense_216_accuracy: 0.6853 - dense_217_accuracy: 0.8602 - dense_218_accuracy: 0.4896 - dense_219_accuracy: 0.8882 - loss: 1.4726\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n","k: 10, patience: 9\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - dense_204_accuracy: 0.5384 - dense_205_accuracy: 0.8322 - dense_206_accuracy: 0.9441 - dense_207_accuracy: 0.7308 - dense_208_accuracy: 0.7763 - dense_209_accuracy: 0.7796 - dense_210_accuracy: 0.6573 - dense_211_accuracy: 0.7029 - dense_212_accuracy: 0.8043 - dense_213_accuracy: 0.8427 - dense_214_accuracy: 0.9161 - dense_215_accuracy: 0.7692 - dense_216_accuracy: 0.7308 - dense_217_accuracy: 0.8602 - dense_218_accuracy: 0.5000 - dense_219_accuracy: 0.8882 - loss: 1.3874\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n","k: 11, patience: 10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 867ms/step\n","Test Loss iteration 4: 52.55165815652252\n","average_loss: 38.7226089285013\n"]}],"source":["import tensorflow as tf\n","from keras.models import Model\n","from keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D, LSTM\n","from keras.optimizers import Adam\n","import numpy as np\n","import sklearn\n","from keras.utils import to_categorical\n","\n","total_losses = []\n","\n","# Transformer Encoder\n","def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n","    x = LayerNormalization(epsilon=1e-6)(inputs)\n","    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n","    x = Dropout(dropout)(x)\n","    res = x + inputs\n","\n","    x = LayerNormalization(epsilon=1e-6)(res)\n","    x = Dense(ff_dim, activation=\"relu\")(x)\n","    x = Dropout(dropout)(x)\n","    x = Dense(inputs.shape[-1])(x)\n","    return x + res\n","\n","# Transformer Decoder\n","def transformer_decoder(inputs, enc_outputs, head_size, num_heads, ff_dim, dropout=0):\n","    # Self attention\n","    x = LayerNormalization(epsilon=1e-6)(inputs)\n","    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n","    x = Dropout(dropout)(x)\n","    res = x + inputs\n","\n","    # Cross attention\n","    x = LayerNormalization(epsilon=1e-6)(res)\n","    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, enc_outputs)\n","    x = Dropout(dropout)(x)\n","    res = x + res\n","\n","    # Feed forward\n","    x = LayerNormalization(epsilon=1e-6)(res)\n","    x = Dense(ff_dim, activation=\"relu\")(x)\n","    x = Dropout(dropout)(x)\n","    x = Dense(inputs.shape[-1])(x)\n","    return x + res\n","\n","# Soft Parameter Sharing Regularizer\n","def soft_parameter_sharing_regularizer(model, alpha=0.01):\n","    lstm_layers = [layer for layer in model.layers if isinstance(layer, LSTM)]\n","    if not lstm_layers:\n","        return 0.0\n","\n","    weights = [tf.convert_to_tensor(layer.weights[0]) for layer in lstm_layers]\n","\n","    shared_weights = tf.reduce_mean(tf.stack(weights), axis=0)\n","    return alpha * tf.reduce_sum([tf.reduce_sum(tf.square(w - shared_weights)) for w in weights])\n","\n","# Custom Loss Function\n","def custom_loss(y_true, y_pred, model):\n","    mse = tf.keras.losses.mse(y_true, y_pred)\n","    reg_loss = soft_parameter_sharing_regularizer(model)\n","    return mse + reg_loss\n","\n","# Model Definition\n","def get_soft_shared_model(input_shape1, input_shape2, output_shape):\n","    # Encoder input\n","    enc_inputs = Input(shape=(input_shape1, input_shape2))\n","\n","    # Encoder\n","    enc_outputs = transformer_encoder(enc_inputs, head_size=64, num_heads=4, ff_dim=256, dropout=0.2)\n","\n","    # Decoder input\n","    dec_inputs = Input(shape=(output_shape[1], input_shape2))\n","\n","    # Decoder\n","    dec_outputs = transformer_decoder(dec_inputs, enc_outputs, head_size=64, num_heads=4, ff_dim=256, dropout=0.2)\n","\n","    # Global pooling\n","    x = GlobalAveragePooling1D()(dec_outputs)\n","\n","    # Task-specific layers\n","    task_outputs = []\n","    task_layers = []\n","    for i in range(output_shape[1]):\n","        task_layer = Dense(256, activation='relu', name=f'task_layer_{i}')\n","        task_layers.append(task_layer)\n","        task_x = task_layer(x)\n","        task_x = Dropout(0.2)(task_x)\n","        output = Dense(4, activation='softmax')(task_x)\n","        task_outputs.append(output)\n","\n","    model = Model([enc_inputs, dec_inputs], task_outputs)\n","\n","    return model\n","\n","# Training and Evaluation Loop\n","for iteration in range(5):\n","\n","  # Assuming input_groups and output_groups are lists of arrays for the 5 groups\n","    input_groups = [input_group_1, input_group_2, input_group_3, input_group_4, input_group_5]\n","    output_groups = [output_group_1, output_group_2, output_group_3, output_group_4, output_group_5]\n","    ssq_groups = [output_total_ssq_group_1, output_total_ssq_group_2, output_total_ssq_group_3, output_total_ssq_group_4, output_total_ssq_group_5]\n","\n","    # Initialize lists for storing training, validation, and test sets\n","    X_train, X_val, X_test = [], [], []\n","    y_train, y_val, y_test = [], [], []\n","    ssq_train, ssq_val, ssq_test = [], [], []\n","\n","    # Loop over each group\n","    for input_group, output_group, ssq_group in zip(input_groups, output_groups, ssq_groups):\n","        # Step 1: Split the group into a training/validation set (80%) and a test set (20%)\n","        X_temp, X_test_temp, y_temp, y_test_temp, ssq_temp, ssq_test_temp = train_test_split(\n","            input_group, output_group, ssq_group, test_size=0.2)\n","\n","        # Step 2: Split the training/validation set into a training set (60%) and a validation set (20%)\n","        X_train_temp, X_val_temp, y_train_temp, y_val_temp, ssq_train_temp, ssq_val_temp = train_test_split(\n","            X_temp, y_temp, ssq_temp, test_size=0.25)  # 0.25 * 0.8 = 0.2\n","\n","        # Step 3: Append the results to the corresponding lists\n","        X_train.append(X_train_temp)\n","        X_val.append(X_val_temp)\n","        X_test.append(X_test_temp)\n","\n","        y_train.append(y_train_temp)\n","        y_val.append(y_val_temp)\n","        y_test.append(y_test_temp)\n","\n","        ssq_train.append(ssq_train_temp)\n","        ssq_val.append(ssq_val_temp)\n","        ssq_test.append(ssq_test_temp)\n","\n","    # After the loop, concatenate the data for all groups if needed\n","    input_train = np.concatenate(X_train, axis=0)\n","    input_val = np.concatenate(X_val, axis=0)\n","    input_test = np.concatenate(X_test, axis=0)\n","\n","    output_train = np.concatenate(y_train, axis=0)\n","    output_val = np.concatenate(y_val, axis=0)\n","    output_test = np.concatenate(y_test, axis=0)\n","\n","    output_val_total_ssq = np.concatenate(ssq_val, axis=0)\n","    output_test_total_ssq = np.concatenate(ssq_test, axis=0)\n","    # input_train, input_test= scale_input_data(input_train[:, (60-sample_size):(180-sample_size), :], input_test[:, (60-sample_size):(180-sample_size), :])\n","    # output_train, min_val, max_val = scale_target_var(output_train)\n","\n","\n","    #  this section for scaling both train and validation set simultaniously\n","      # Step 1: Combine the training and validation sets\n","    combined_input = np.concatenate([input_train, input_val], axis=0)\n","    combined_output = np.concatenate([output_train, output_val], axis=0)\n","\n","    # Step 2: Scale the combined input data\n","    # Assuming scale_input_data scales the data based on the combined dataset\n","    combined_input, input_test = scale_input_data(\n","        combined_input[:, (60-sample_size):(180-sample_size), :],\n","        input_test[:, (60-sample_size):(180-sample_size), :]\n","    )\n","\n","    # Convert the original labels to one-hot encoded format\n","    output_train_encoded = to_categorical(combined_output, num_classes=4)\n","\n","    # Step 4: Split the combined data back into training and validation sets\n","    # Use the original shapes of input_train and input_val to slice the combined arrays\n","    input_train = combined_input[:input_train.shape[0], :, :]\n","    input_val = combined_input[input_train.shape[0]:, :, :]\n","\n","    output_train = output_train_encoded[:output_train.shape[0], :]\n","    output_val = output_train_encoded[output_train.shape[0]:, :]\n","\n","\n","\n","    print(\"input_train :\", input_train.shape)\n","    print(\"output_train :\", output_train.shape)\n","    print(\"input_val :\", input_val.shape)\n","    print(\"output_val :\", output_val.shape)\n","    print(\"input_test :\", input_test.shape)\n","    print(\"output_test :\", output_test.shape)\n","\n","    train_input_reshaped = input_train.reshape((input_train.shape[0], input_train.shape[1], input_train.shape[2]))\n","    test_input_reshaped = input_test.reshape((input_test.shape[0], input_test.shape[1], input_test.shape[2]))\n","    val_input_reshaped = input_val.reshape((input_val.shape[0], input_val.shape[1], input_val.shape[2]))\n","\n","    # Create decoder inputs\n","    train_dec_input = np.zeros((train_input_reshaped.shape[0], output_train.shape[1], train_input_reshaped.shape[2]))\n","    val_dec_input = np.zeros((val_input_reshaped.shape[0], output_train.shape[1], val_input_reshaped.shape[2]))\n","    test_dec_input = np.zeros((test_input_reshaped.shape[0], output_test.shape[1], test_input_reshaped.shape[2]))\n","\n","    # Create the soft parameter sharing model\n","    model = get_soft_shared_model(input_train.shape[1], input_train.shape[2], output_train.shape)\n","\n","    # Compile the model with the custom loss function\n","    model.compile(optimizer=Adam(learning_rate=0.001),\n","                  loss=lambda y_true, y_pred: custom_loss(y_true, y_pred, model),\n","                  metrics=[['accuracy'] for _ in range(output_train.shape[1])])\n","\n","    best_val = 1000000\n","    patience = 0\n","    best_model = None\n","\n","    for k in range(200):\n","            model.fit([train_input_reshaped, train_dec_input],\n","                [output_train[:, i] for i in range(output_train.shape[1])],\n","                epochs=1, batch_size=32, verbose=1)\n","            pred_val = np.array(model.predict([val_input_reshaped, val_dec_input]))\n","            pred_val = np.transpose(pred_val.squeeze(), (1, 0, 2))\n","            pred_val = np.argmax(pred_val, axis=-1).reshape(pred_val.shape[:-1])\n","\n","            print(f\"k: {k}, patience: {patience}\")\n","\n","            # Evaluate the model based on SSQ for validation data\n","            losses = []\n","            for i in range(pred_val.shape[0]):\n","                total_ssq = 0\n","                for j in [0, 5, 6, 7, 8, 14, 15]:\n","                    total_ssq += np.sum(pred_val[i, j])\n","\n","                for j in [0, 1, 2, 3, 4, 8, 10]:\n","                    total_ssq += np.sum(pred_val[i, j])\n","\n","                for j in [4, 7, 9, 10, 11, 12, 13]:\n","                    total_ssq += np.sum(pred_val[i, j])\n","\n","                total_ssq *= 3.74\n","                output_val_ssq = output_val_total_ssq[i, 0]\n","                loss = sklearn.metrics.mean_squared_error([total_ssq], [output_val_ssq], squared=False)\n","                losses.append(loss)\n","\n","            tmp_val_loss = np.mean(losses)\n","            if tmp_val_loss <= best_val:\n","                best_val = tmp_val_loss\n","                patience = 0\n","                best_model = model\n","            else:\n","                patience += 1\n","                if patience > 10:\n","                    break\n","\n","    # Predict and evaluate test data using the best model\n","\n","    pred_test = np.array(best_model.predict([test_input_reshaped, test_dec_input]))\n","    pred_test = np.transpose(pred_test.squeeze(), (1, 0, 2))\n","    pred_test = np.argmax(pred_test, axis=-1).reshape(pred_test.shape[:-1])\n","\n","    pred_total_ssq = []\n","    for i in range(pred_test.shape[0]):\n","        total_ssq = 0\n","        for j in [0, 5, 6, 7, 8, 14, 15]:\n","            total_ssq += np.sum(pred_test[i, j])\n","\n","        for j in [0, 1, 2, 3, 4, 8, 10]:\n","            total_ssq += np.sum(pred_test[i, j])\n","\n","        for j in [4, 7, 9, 10, 11, 12, 13]:\n","            total_ssq += np.sum(pred_test[i, j])\n","\n","        total_ssq *= 3.74\n","        pred_total_ssq.append(total_ssq)\n","\n","    # Overall Test Loss\n","    loss = sklearn.metrics.mean_squared_error(pred_total_ssq, output_test_total_ssq, squared=False)\n","    print(f\"Test Loss iteration {iteration}: {loss}\")\n","    total_losses.append(loss)\n","\n","\n","average_loss = sum(total_losses) / len(total_losses)\n","total_losses.append(average_loss)\n","print(\"average_loss:\", average_loss)\n"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOOP1za1IZ3410RsstSzEda"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}