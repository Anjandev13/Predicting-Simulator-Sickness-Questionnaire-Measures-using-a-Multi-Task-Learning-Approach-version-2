{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPC1xlXQ1yFM7blFdzYygNS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"fT_A9oAGAepC","executionInfo":{"status":"ok","timestamp":1724948511419,"user_tz":300,"elapsed":18232,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"outputs":[],"source":["from google.colab import drive\n","import pandas as pd\n","from sklearn.preprocessing import StandardScaler\n","from scipy import stats\n","import numpy as np\n","import logging\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense\n","from datetime import datetime\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import datetime\n","import matplotlib.dates as mdates\n","import os"]},{"cell_type":"code","source":["from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Set the base path to the desired directory on Google Drive\n","base_path = '/content/drive/MyDrive/Study_1_Data/'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TishlaEBAmlN","executionInfo":{"status":"ok","timestamp":1724948512301,"user_tz":300,"elapsed":900,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}},"outputId":"86cbee8d-1439-44cb-91be-701ea9537c78"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["def read_csv(file_path):\n","    data = pd.read_csv(file_path)\n","    return data"],"metadata":{"id":"XeSwJ8oKAt2r","executionInfo":{"status":"ok","timestamp":1724948512301,"user_tz":300,"elapsed":25,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def process_data(data, columns_to_remove):\n","    processed_data = data.drop(columns=columns_to_remove).values\n","    return processed_data"],"metadata":{"id":"BFHyHoFvA5bX","executionInfo":{"status":"ok","timestamp":1724948512302,"user_tz":300,"elapsed":24,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def construct_3d_array(base_dir, participants, simulations, columns_to_remove_hr, columns_to_remove_gsr, columns_to_remove_head, columns_to_remove_eye):\n","    \"\"\"\n","    Construct 3D array from CSV files.\n","    \"\"\"\n","    num_rows = 180  # Define number of rows to keep (last 180 rows)\n","    arrays_3d = []\n","\n","    for participant in participants:\n","        participant_id = f\"{int(participant):02d}\"  # Format participant number to two digits\n","\n","        valid_simulations = []\n","\n","        for simulation in simulations:\n","            hr_file_path = os.path.join(base_dir, participant_id, simulation, f'HR{simulation.capitalize()}.csv')\n","            gsr_file_path = os.path.join(base_dir, participant_id, simulation, f'EDA{simulation.capitalize()}_downsampled.csv')\n","            head_file_path = os.path.join(base_dir, participant_id, simulation, 'head_tracking_downsampled.csv')\n","            eye_file_path = os.path.join(base_dir, participant_id, simulation, 'eye_tracking_downsampled.csv')\n","\n","            # Check if all files exist\n","            if all(os.path.exists(file) for file in [hr_file_path, gsr_file_path, head_file_path, eye_file_path]):\n","                valid_simulations.append(simulation)\n","\n","        num_valid_simulations = len(valid_simulations)\n","        if num_valid_simulations == 0:\n","            continue  # Skip this participant if no valid simulations are found\n","\n","        array_3d = np.zeros((num_valid_simulations, num_rows, 47)) # hr=1, gsr=1, head=15-3, eye=41-8 total columns after removing columns= 48\n","\n","        for s_idx, simulation in enumerate(valid_simulations):\n","            # Process hr data\n","            hr_file_path = os.path.join(base_dir, participant_id, simulation, f'HR{simulation.capitalize()}.csv')\n","            hr_data = read_csv(hr_file_path)\n","            processed_hr_data = process_data(hr_data, columns_to_remove_hr)\n","            processed_hr_data = processed_hr_data[-num_rows:]  # Keep only the last 180 rows\n","\n","            # Process gsr data\n","            gsr_file_path = os.path.join(base_dir, participant_id, simulation, f'EDA{simulation.capitalize()}_downsampled.csv')\n","            gsr_data = read_csv(gsr_file_path)\n","            processed_gsr_data = process_data(gsr_data, columns_to_remove_gsr)\n","            processed_gsr_data = processed_gsr_data[-num_rows:]  # Keep only the last 180 rows\n","\n","            # Process head data\n","            head_file_path = os.path.join(base_dir, participant_id, simulation, 'head_tracking_downsampled.csv')\n","            head_data = read_csv(head_file_path)\n","            processed_head_data = process_data(head_data, columns_to_remove_head)\n","            processed_head_data = processed_head_data[-num_rows:]  # Keep only the last 180 rows\n","\n","            # Process eye data\n","            eye_file_path = os.path.join(base_dir, participant_id, simulation, 'eye_tracking_downsampled.csv')\n","            eye_data = read_csv(eye_file_path)\n","            processed_eye_data = process_data(eye_data, columns_to_remove_eye)\n","            processed_eye_data = processed_eye_data[-num_rows:]  # Keep only the last 180 rows\n","\n","            # Combine processed data\n","            combined_data = np.concatenate((processed_hr_data, processed_gsr_data, processed_head_data, processed_eye_data), axis=1)\n","\n","            array_3d[s_idx, :, :] = combined_data\n","\n","        arrays_3d.append(array_3d)\n","\n","    return arrays_3d\n"],"metadata":{"id":"QeoIWcudA94b","executionInfo":{"status":"ok","timestamp":1724948512303,"user_tz":300,"elapsed":24,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["sample_size=60\n","\n","simulations_train = ['noise','bumps']\n","simulations_test=['flat']\n","val_indices = [4, 10, 11, 26, 28, 31, 33, 37] # for flat\n","train_indices = [0, 1, 2, 3, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 29, 30, 32, 34, 35, 36, 38, 39, 40, 41] # for flat\n","\n","\n","# simulations_test=['noise']\n","# simulations_train = ['flat','bumps']\n","# val_indices = [7, 15, 17, 19, 28, 31, 32, 42, 44, 48] # for noise\n","# train_indices = [0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 16, 18, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 33, 34, 35, 36, 37, 38, 39, 40, 41, 43, 45, 46, 47] # for noise\n","\n","# simulations_test=['bumps']\n","# simulations_train = ['flat','noise']\n","# val_indices = [1, 12, 16, 18, 22, 26, 28, 37, 41] # for speedbumps\n","# train_indices = [0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 17, 19, 20, 21, 23, 24, 25, 27, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 42, 43, 44] # for speedbumps"],"metadata":{"id":"f7k75YNldMiK","executionInfo":{"status":"ok","timestamp":1724948512303,"user_tz":300,"elapsed":21,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["participants = [str(i) for i in range(1, 27)]  # Participants 101 to 123\n","columns_to_remove_hr = []\n","columns_to_remove_gsr = []\n","columns_to_remove_eye = ['#Frame','Time', 'Unnamed: 40','ConvergenceValid','Left_Eye_Closed','Right_Eye_Closed','LocalGazeValid','WorldGazeValid']\n","columns_to_remove_head = ['#Frame','Time', 'Unnamed: 14']"],"metadata":{"id":"fqaeUGUDBCtT","executionInfo":{"status":"ok","timestamp":1724948512304,"user_tz":300,"elapsed":20,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["arrays_train = construct_3d_array(base_path, participants, simulations_train, columns_to_remove_hr, columns_to_remove_gsr, columns_to_remove_head, columns_to_remove_eye)\n","arrays_test = construct_3d_array(base_path, participants, simulations_test, columns_to_remove_hr, columns_to_remove_gsr, columns_to_remove_head, columns_to_remove_eye)"],"metadata":{"id":"DwS92BItBL4B","executionInfo":{"status":"ok","timestamp":1724948515777,"user_tz":300,"elapsed":3491,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Concatenate arrays along the first axis\n","input_train = np.concatenate(arrays_train, axis=0)\n","input_test = np.concatenate(arrays_test, axis=0)\n","\n","# Display the shape of the final concatenated 3D array\n","print(f\"Shape of the final concatenated 3D array: {input_train.shape}\")\n","print(f\"Shape of the final concatenated 3D array: {input_test.shape}\")"],"metadata":{"id":"GgAM9zg_BRe8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724948515780,"user_tz":300,"elapsed":24,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}},"outputId":"08bc893c-41ee-40c7-8f88-8949e994617a"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of the final concatenated 3D array: (42, 180, 47)\n","Shape of the final concatenated 3D array: (26, 180, 47)\n"]}]},{"cell_type":"code","source":["def calculate_total_ssq(csv_file):\n","    # Read the CSV file into a DataFrame\n","    df = pd.read_csv(csv_file)\n","    n_columns = [0, 5, 6, 7, 8, 14, 15]\n","    o_columns = [0, 1, 2, 3, 4, 8, 10]\n","    d_columns = [4, 7, 9, 10, 11, 12, 13]\n","\n","    # Calculate sum for each specified set of columns\n","    n_val = df.iloc[:, n_columns].sum(axis=1)\n","    o_val = df.iloc[:, o_columns].sum(axis=1)\n","    d_val = df.iloc[:, d_columns].sum(axis=1)\n","\n","    return n_val,o_val,d_val"],"metadata":{"id":"vPQyeAYKBYaO","executionInfo":{"status":"ok","timestamp":1724948515781,"user_tz":300,"elapsed":19,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["def merge_ssq_column(conditions,participants):\n","  directories = []\n","  total_ssq_values = []\n","  for participant in participants:\n","      participant = f\"{int(participant):02d}\"\n","      for condition in conditions:\n","          directory = os.path.join(base_path, participant, condition)\n","          directories.append(directory)\n","\n","  # Loop through each directory\n","  for directory in directories:\n","      # Check if the directory exists\n","      if not os.path.exists(directory):\n","          continue\n","\n","      # Get all CSV files in the directory that are named 'ssq.csv'\n","      csv_files = [file for file in os.listdir(directory) if file == 'ssq.csv']\n","\n","      # Loop through each CSV file\n","      for csv_file in csv_files:\n","          csv_path = os.path.join(directory, csv_file)\n","          df = pd.read_csv(csv_path)\n","          n_val,o_val,d_val = calculate_total_ssq(csv_path)\n","          total_ssq_values.append([n_val, o_val, d_val])\n","          #ssq_values_participant = df.iloc[:, 0:17].values.flatten()   # Assuming SSQ values are in columns 1 to 16\n","          #total_ssq_values.append(ssq_values_participant)\n","  ssq_array = np.array(total_ssq_values)\n","  return ssq_array\n","\n","def merge_total_ssq(conditions,participants):\n","  directories = []\n","  total_ssq_values = []\n","  for participant in participants:\n","      participant = f\"{int(participant):02d}\"\n","      for condition in conditions:\n","          directory = os.path.join(base_path, participant, condition)\n","          directories.append(directory)\n","\n","  # Loop through each directory\n","  for directory in directories:\n","      # Check if the directory exists\n","      if not os.path.exists(directory):\n","          continue\n","\n","      # Get all CSV files in the directory that are named 'ssq.csv'\n","      csv_files = [file for file in os.listdir(directory) if file == 'ssq.csv']\n","\n","      # Loop through each CSV file\n","      for csv_file in csv_files:\n","          csv_path = os.path.join(directory, csv_file)\n","          n_val,o_val,d_val = calculate_total_ssq(csv_path)\n","          total_ssq = (n_val+o_val+d_val) * 3.74\n","          df = pd.read_csv(csv_path)\n","          df[\"total-ssq\"] = total_ssq\n","          #print(\"csv_path: \",csv_path,\"   \",total_ssq)\n","          total_ssq_values.append(total_ssq)\n","  # Create a DataFrame from the list of total SSQ values\n","  df_total_ssq = pd.DataFrame(total_ssq_values, columns=[\"total-ssq\"])\n","  # Convert the list of total SSQ values to a NumPy array\n","  total_ssq_array = np.array(total_ssq_values)\n","  return total_ssq_array\n","\n"],"metadata":{"id":"Xpn0lDt0BfvE","executionInfo":{"status":"ok","timestamp":1724948515782,"user_tz":300,"elapsed":18,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["output_train=merge_ssq_column(simulations_train,participants)\n","output_train = np.squeeze(output_train)\n","output_test=merge_ssq_column(simulations_test,participants)\n","output_test = np.squeeze(output_test)\n","output_train_total_ssq=merge_total_ssq(simulations_train,participants)\n","output_test_total_ssq=merge_total_ssq(simulations_test,participants)\n","output_train_total_ssq=output_train_total_ssq.reshape(-1, 1)\n","output_test_total_ssq=output_test_total_ssq.reshape(-1, 1)\n","print(output_train.shape,output_test.shape,output_train_total_ssq.shape,output_test_total_ssq.shape)\n","# print(output_train)\n","# print(output_train_total_ssq)\n"],"metadata":{"id":"7k17K0HrCr6-","executionInfo":{"status":"ok","timestamp":1724948517688,"user_tz":300,"elapsed":1922,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4cf862c4-b4f9-4629-d0df-e7de104b3b56"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["(42, 3) (26, 3) (42, 1) (26, 1)\n"]}]},{"cell_type":"code","source":["def scale_input_data(input_train, input_test):\n","    # Get the shape of the input data\n","    num_samples_train, time_steps_train, num_features = input_train.shape\n","    num_samples_test, time_steps_test, _ = input_test.shape\n","\n","    # Reshape the input data into 2D arrays\n","    flattened_train_data = input_train.reshape(-1, num_features)\n","    flattened_test_data = input_test.reshape(-1, num_features)\n","\n","    # Initialize a MinMaxScaler object\n","    scaler = MinMaxScaler()\n","\n","    # Fit the scaler on the training data and transform both train and test data\n","    scaled_train_data = scaler.fit_transform(flattened_train_data)\n","    scaled_test_data = scaler.transform(flattened_test_data)\n","\n","    # Reshape the scaled data back to its original shape\n","    scaled_train_data = scaled_train_data.reshape(num_samples_train, time_steps_train, num_features)\n","    scaled_test_data = scaled_test_data.reshape(num_samples_test, time_steps_test, num_features)\n","\n","    return scaled_train_data, scaled_test_data\n","\n","def scale_target_var(target_data):\n","    min_val, max_val = np.min(target_data, axis=0), np.max(target_data, axis=0)\n","    target_data = (target_data-min_val)/(max_val-min_val)\n","\n","    return target_data, min_val, max_val"],"metadata":{"id":"26ADF-kiC1EZ","executionInfo":{"status":"ok","timestamp":1724948517689,"user_tz":300,"elapsed":26,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["input_train, input_test= scale_input_data(input_train[:, (60-sample_size):(180-sample_size), :], input_test[:, (60-sample_size):(180-sample_size), :])\n","output_train, min_val, max_val = scale_target_var(output_train)\n","\n","# input_val = input_train[val_indices]\n","# input_train = input_train[train_indices]\n","# output_val = output_train_total_ssq[val_indices]\n","# output_train = output_train[train_indices]\n"],"metadata":{"id":"LyvV6GFDC66F","executionInfo":{"status":"ok","timestamp":1724948517690,"user_tz":300,"elapsed":24,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# print(\"input_train :\", input_train.shape)\n","# print(\"output_train :\", output_train.shape)\n","# print(\"input_val :\", input_val.shape)\n","# print(\"output_val :\", output_val.shape)\n","# print(\"input_test :\", input_test.shape)\n","# print(\"output_test :\", output_test.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"11VwNmhtBSly","executionInfo":{"status":"ok","timestamp":1724948517691,"user_tz":300,"elapsed":22,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}},"outputId":"86e1d0bd-3f16-4837-81fd-5fc0731f521b"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["input_train : (34, 120, 47)\n","output_train : (34, 3)\n","input_val : (8, 120, 47)\n","output_val : (8, 1)\n","input_test : (26, 120, 47)\n","output_test : (26, 3)\n"]}]},{"cell_type":"code","source":["from keras.models import Sequential\n","from keras.layers import Input, LSTM, Dense, Dropout\n","from keras.models import Model\n","from keras.callbacks import EarlyStopping\n","import numpy as np\n","import sklearn\n","\n","total_losses=[]\n","for iteration in range(1):\n","    def get_shared_lstm(input_shape1, input_shape2):\n","        # Define shared GRU model\n","        input_layer = Input(shape=(input_shape1, input_shape2))\n","        x = LSTM(64, return_sequences=False)(input_layer)\n","        x = Dense(256, activation='relu')(x)\n","        x = Dropout(0.2)(x)\n","        shared_model = Model(inputs=input_layer, outputs=x)\n","        return shared_model\n","\n","\n","\n","\n","    def get_output_model(shared_lstm_output, output_shape):\n","        # Define separate output model for each column\n","        output_models = []\n","        for i in range(output_shape[1]):\n","            output_model = Sequential()\n","            output_model.add(Dense(256, activation='relu'))\n","            output_model.add(Dropout(0.2))\n","            output_model.add(Dense(1))  # Output shape is (None, 1) for each column\n","            output_model_output = output_model(shared_lstm_output)\n","            output_models.append(output_model_output)\n","        return output_models\n","\n","    # Assuming train_input, train_output, test_input, test_output are numpy arrays\n","\n","    # Reshape train and test inputs to match GRU input shape\n","    train_input_reshaped = input_train.reshape((input_train.shape[0], input_train.shape[1], input_train.shape[2]))\n","    test_input_reshaped = input_test.reshape((input_test.shape[0], input_test.shape[1], input_test.shape[2]))\n","    # val_input_reshaped = input_val.reshape((input_val.shape[0], input_val.shape[1], input_val.shape[2]))\n","    # Get shared LSTM model\n","    shared_lstm = get_shared_lstm(input_train.shape[1], input_train.shape[2])\n","\n","    # Create separate output models for each column\n","    output_models = get_output_model(shared_lstm.output, output_train.shape)\n","\n","    # Create combined model\n","    model = Model(inputs=shared_lstm.input, outputs=output_models)\n","\n","    # Compile and train the mode\n","    adam_opt = tf.keras.optimizers.Adam(learning_rate=0.0005)\n","    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n","    model.compile(loss='mse', optimizer=adam_opt)\n","    # model.compile(loss='mse', optimizer=adam_opt, metrics=[['mse'] for _ in range(output_train.shape[1])])  # Using MSE as loss and metric\n","\n","    model.fit(train_input_reshaped, [output_train[:, i] for i in range(output_train.shape[1])], epochs=200, batch_size=32, validation_split=0.25,callbacks=[early_stopping])\n","\n","    # Predict test data\n","    pred_test = np.array(model.predict(test_input_reshaped))\n","    pred_test = np.transpose(pred_test.squeeze(), (1, 0))\n","    # Evaluate the model\n","    pred_total_ssq = []\n","    for i in range(pred_test.shape[0]):\n","        total_ssq=0\n","        for j in range(3):\n","            total_ssq=np.sum(pred_test[i,j]*(max_val[j]-min_val[j]) + min_val[j])+total_ssq\n","        total_ssq=total_ssq*3.74\n","\n","        pred_total_ssq.append(total_ssq)\n","\n","    # Overall Test Loss\n","    loss = sklearn.metrics.mean_squared_error(pred_total_ssq, output_test_total_ssq, squared = False)\n","    print(\"Test Loss:\", loss)\n","    total_losses.append(loss)\n","\n","average_loss = sum(total_losses) / len(total_losses)\n","print(\"average_loss:\", average_loss)\n","print(\"total_losses\",total_losses)"],"metadata":{"id":"E6ssyYUeDJwI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724948525641,"user_tz":300,"elapsed":7966,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}},"outputId":"1ed91e8a-d582-4c98-e3e4-1eb8a39b482c"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/200\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step - loss: 0.2006 - val_loss: 0.0684\n","Epoch 2/200\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step - loss: 0.1573 - val_loss: 0.0781\n","Epoch 3/200\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.1611 - val_loss: 0.0756\n","Epoch 4/200\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 0.1527 - val_loss: 0.0716\n","Epoch 5/200\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 0.1264 - val_loss: 0.0715\n","Epoch 6/200\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1448 - val_loss: 0.0729\n","Epoch 7/200\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 0.1186 - val_loss: 0.0750\n","Epoch 8/200\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 0.1199 - val_loss: 0.0768\n","Epoch 9/200\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.1066 - val_loss: 0.0790\n","Epoch 10/200\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 0.1097 - val_loss: 0.0831\n","Epoch 11/200\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 0.0993 - val_loss: 0.0893\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 252ms/step\n","Test Loss: 27.142900846824\n","average_loss: 27.142900846824\n","total_losses [27.142900846824]\n"]}]}]}