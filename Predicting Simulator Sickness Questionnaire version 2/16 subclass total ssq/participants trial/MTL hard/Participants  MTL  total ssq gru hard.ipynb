{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO89utNdlvEe1L1LwU20T2i"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":14,"metadata":{"id":"fT_A9oAGAepC","executionInfo":{"status":"ok","timestamp":1724915242762,"user_tz":300,"elapsed":233,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"outputs":[],"source":["from google.colab import drive\n","import pandas as pd\n","from sklearn.preprocessing import StandardScaler\n","from scipy import stats\n","import numpy as np\n","import logging\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import GRU, Dense\n","from datetime import datetime\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import datetime\n","import matplotlib.dates as mdates\n","import os"]},{"cell_type":"code","source":["from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Set the base path to the desired directory on Google Drive\n","base_path = '/content/drive/MyDrive/Study_1_Data/'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TishlaEBAmlN","executionInfo":{"status":"ok","timestamp":1724915244035,"user_tz":300,"elapsed":1033,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}},"outputId":"307de927-563c-42f3-a514-0572ffbaa1d7"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["def read_csv(file_path):\n","    data = pd.read_csv(file_path)\n","    return data"],"metadata":{"id":"XeSwJ8oKAt2r","executionInfo":{"status":"ok","timestamp":1724915244035,"user_tz":300,"elapsed":5,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["def process_data(data, columns_to_remove):\n","    processed_data = data.drop(columns=columns_to_remove).values\n","    return processed_data"],"metadata":{"id":"BFHyHoFvA5bX","executionInfo":{"status":"ok","timestamp":1724915244035,"user_tz":300,"elapsed":5,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["def construct_3d_array(base_dir, participants, simulations, columns_to_remove_hr, columns_to_remove_gsr, columns_to_remove_head, columns_to_remove_eye):\n","    \"\"\"\n","    Construct 3D array from CSV files.\n","    \"\"\"\n","    num_rows = 180  # Define number of rows to keep (last 180 rows)\n","    arrays_3d = []\n","\n","    for participant in participants:\n","        participant_id = f\"{int(participant):02d}\"  # Format participant number to two digits\n","\n","        valid_simulations = []\n","\n","        for simulation in simulations:\n","            hr_file_path = os.path.join(base_dir, participant_id, simulation, f'HR{simulation.capitalize()}.csv')\n","            gsr_file_path = os.path.join(base_dir, participant_id, simulation, f'EDA{simulation.capitalize()}_downsampled.csv')\n","            head_file_path = os.path.join(base_dir, participant_id, simulation, 'head_tracking_downsampled.csv')\n","            eye_file_path = os.path.join(base_dir, participant_id, simulation, 'eye_tracking_downsampled.csv')\n","\n","            # Check if all files exist\n","            if all(os.path.exists(file) for file in [hr_file_path, gsr_file_path, head_file_path, eye_file_path]):\n","                valid_simulations.append(simulation)\n","\n","        num_valid_simulations = len(valid_simulations)\n","        if num_valid_simulations == 0:\n","            continue  # Skip this participant if no valid simulations are found\n","\n","        array_3d = np.zeros((num_valid_simulations, num_rows, 47)) # hr=1, gsr=1, head=15-3, eye=41-8 total columns after removing columns= 48\n","\n","        for s_idx, simulation in enumerate(valid_simulations):\n","            # Process hr data\n","            hr_file_path = os.path.join(base_dir, participant_id, simulation, f'HR{simulation.capitalize()}.csv')\n","            hr_data = read_csv(hr_file_path)\n","            processed_hr_data = process_data(hr_data, columns_to_remove_hr)\n","            processed_hr_data = processed_hr_data[-num_rows:]  # Keep only the last 180 rows\n","\n","            # Process gsr data\n","            gsr_file_path = os.path.join(base_dir, participant_id, simulation, f'EDA{simulation.capitalize()}_downsampled.csv')\n","            gsr_data = read_csv(gsr_file_path)\n","            processed_gsr_data = process_data(gsr_data, columns_to_remove_gsr)\n","            processed_gsr_data = processed_gsr_data[-num_rows:]  # Keep only the last 180 rows\n","\n","            # Process head data\n","            head_file_path = os.path.join(base_dir, participant_id, simulation, 'head_tracking_downsampled.csv')\n","            head_data = read_csv(head_file_path)\n","            processed_head_data = process_data(head_data, columns_to_remove_head)\n","            processed_head_data = processed_head_data[-num_rows:]  # Keep only the last 180 rows\n","\n","            # Process eye data\n","            eye_file_path = os.path.join(base_dir, participant_id, simulation, 'eye_tracking_downsampled.csv')\n","            eye_data = read_csv(eye_file_path)\n","            processed_eye_data = process_data(eye_data, columns_to_remove_eye)\n","            processed_eye_data = processed_eye_data[-num_rows:]  # Keep only the last 180 rows\n","\n","            # Combine processed data\n","            combined_data = np.concatenate((processed_hr_data, processed_gsr_data, processed_head_data, processed_eye_data), axis=1)\n","\n","            array_3d[s_idx, :, :] = combined_data\n","\n","        arrays_3d.append(array_3d)\n","\n","    return arrays_3d\n"],"metadata":{"id":"QeoIWcudA94b","executionInfo":{"status":"ok","timestamp":1724915244035,"user_tz":300,"elapsed":4,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["sample_size=60\n","simulations = ['flat','noise','bumps']\n","participants = [str(i) for i in range(1, 27)]  # Participants 101 to 127\n","columns_to_remove_hr = []\n","columns_to_remove_gsr = []\n","columns_to_remove_eye = ['#Frame','Time', 'Unnamed: 40','ConvergenceValid','Left_Eye_Closed','Right_Eye_Closed','LocalGazeValid','WorldGazeValid']\n","columns_to_remove_head = ['#Frame','Time', 'Unnamed: 14']"],"metadata":{"id":"fqaeUGUDBCtT","executionInfo":{"status":"ok","timestamp":1724915244216,"user_tz":300,"elapsed":184,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["def calculate_total_ssq(csv_file):\n","    # Read the CSV file into a DataFrame\n","    df = pd.read_csv(csv_file)\n","    n_columns = [0, 5, 6, 7, 8, 14, 15]\n","    o_columns = [0, 1, 2, 3, 4, 8, 10]\n","    d_columns = [4, 7, 9, 10, 11, 12, 13]\n","\n","    # Calculate sum for each specified set of columns\n","    n_val = df.iloc[0, n_columns].sum()\n","    o_val = df.iloc[0, o_columns].sum()\n","    d_val = df.iloc[0, d_columns].sum()\n","\n","    return n_val, o_val, d_val"],"metadata":{"id":"vPQyeAYKBYaO","executionInfo":{"status":"ok","timestamp":1724915244218,"user_tz":300,"elapsed":6,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["def merge_ssq_column(conditions,participants):\n","  directories = []\n","  total_ssq_values = []\n","  for participant in participants:\n","      participant = f\"{int(participant):02d}\"\n","      for condition in conditions:\n","          directory = os.path.join(base_path, participant, condition)\n","          directories.append(directory)\n","\n","  # Loop through each directory\n","  for directory in directories:\n","      # Check if the directory exists\n","      if not os.path.exists(directory):\n","          continue\n","\n","      # Get all CSV files in the directory that are named 'ssq.csv'\n","      csv_files = [file for file in os.listdir(directory) if file == 'ssq.csv']\n","\n","      # Loop through each CSV file\n","      for csv_file in csv_files:\n","          csv_path = os.path.join(directory, csv_file)\n","          df = pd.read_csv(csv_path)\n","          # n_val,o_val,d_val = calculate_total_ssq(csv_path)\n","          # total_ssq_values.append([n_val, o_val, d_val])\n","          ssq_values_participant = df.iloc[:, 0:17].values.flatten()   # Assuming SSQ values are in columns 1 to 16\n","          total_ssq_values.append(ssq_values_participant)\n","  ssq_array = np.array(total_ssq_values)\n","  return ssq_array\n","\n","def merge_total_ssq(conditions,participants):\n","  directories = []\n","  total_ssq_values = []\n","  for participant in participants:\n","      participant = f\"{int(participant):02d}\"\n","      for condition in conditions:\n","          directory = os.path.join(base_path, participant, condition)\n","          directories.append(directory)\n","\n","  # Loop through each directory\n","  for directory in directories:\n","      # Check if the directory exists\n","      if not os.path.exists(directory):\n","          continue\n","\n","      # Get all CSV files in the directory that are named 'ssq.csv'\n","      csv_files = [file for file in os.listdir(directory) if file == 'ssq.csv']\n","\n","      # Loop through each CSV file\n","      for csv_file in csv_files:\n","          csv_path = os.path.join(directory, csv_file)\n","          n_val,o_val,d_val = calculate_total_ssq(csv_path)\n","          total_ssq = (n_val+o_val+d_val) * 3.74\n","          df = pd.read_csv(csv_path)\n","          df[\"total-ssq\"] = total_ssq\n","          #print(\"csv_path: \",csv_path,\"   \",total_ssq)\n","          total_ssq_values.append(total_ssq)\n","  # Create a DataFrame from the list of total SSQ values\n","  df_total_ssq = pd.DataFrame(total_ssq_values, columns=[\"total-ssq\"])\n","  # Convert the list of total SSQ values to a NumPy array\n","  total_ssq_array = np.array(total_ssq_values)\n","  return total_ssq_array\n","\n"],"metadata":{"id":"Xpn0lDt0BfvE","executionInfo":{"status":"ok","timestamp":1724915244218,"user_tz":300,"elapsed":5,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["participants_group_1 = [1,3,4,11,25]\n","participants_group_2 = [2,7,8,9,17]\n","participants_group_3 = [10,12,13,22,23]\n","participants_group_4 = [5,14,18,20,21]\n","participants_group_5 = [6,15,16,19,24,26]\n","\n","arrays_group_1 = construct_3d_array(base_path, participants_group_1, simulations, columns_to_remove_hr, columns_to_remove_gsr, columns_to_remove_head, columns_to_remove_eye)\n","arrays_group_2 = construct_3d_array(base_path, participants_group_2, simulations, columns_to_remove_hr, columns_to_remove_gsr, columns_to_remove_head, columns_to_remove_eye)\n","arrays_group_3 = construct_3d_array(base_path, participants_group_3, simulations, columns_to_remove_hr, columns_to_remove_gsr, columns_to_remove_head, columns_to_remove_eye)\n","arrays_group_4 = construct_3d_array(base_path, participants_group_4, simulations, columns_to_remove_hr, columns_to_remove_gsr, columns_to_remove_head, columns_to_remove_eye)\n","arrays_group_5 = construct_3d_array(base_path, participants_group_5, simulations, columns_to_remove_hr, columns_to_remove_gsr, columns_to_remove_head, columns_to_remove_eye)"],"metadata":{"id":"7k17K0HrCr6-","executionInfo":{"status":"ok","timestamp":1724915248634,"user_tz":300,"elapsed":4420,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["# Concatenate arrays along the first axis\n","input_group_1 = np.concatenate(arrays_group_1, axis=0)\n","input_group_2 = np.concatenate(arrays_group_2, axis=0)\n","input_group_3 = np.concatenate(arrays_group_3, axis=0)\n","input_group_4 = np.concatenate(arrays_group_4, axis=0)\n","input_group_5 = np.concatenate(arrays_group_5, axis=0)\n"],"metadata":{"id":"Kp16zScDmxap","executionInfo":{"status":"ok","timestamp":1724915248635,"user_tz":300,"elapsed":5,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["output_group_1=merge_ssq_column(simulations,participants_group_1)\n","output_group_2=merge_ssq_column(simulations,participants_group_2)\n","output_group_3=merge_ssq_column(simulations,participants_group_3)\n","output_group_4=merge_ssq_column(simulations,participants_group_4)\n","output_group_5=merge_ssq_column(simulations,participants_group_5)\n","\n","output_group_1 = np.squeeze(output_group_1)\n","output_group_2 = np.squeeze(output_group_2)\n","output_group_3 = np.squeeze(output_group_3)\n","output_group_4 = np.squeeze(output_group_4)\n","output_group_5 = np.squeeze(output_group_5)\n","\n","\n","output_total_ssq_group_1=merge_total_ssq(simulations,participants_group_1)\n","output_total_ssq_group_2=merge_total_ssq(simulations,participants_group_2)\n","output_total_ssq_group_3=merge_total_ssq(simulations,participants_group_3)\n","output_total_ssq_group_4=merge_total_ssq(simulations,participants_group_4)\n","output_total_ssq_group_5=merge_total_ssq(simulations,participants_group_5)\n","\n","output_total_ssq_group_1=output_total_ssq_group_1.reshape(-1, 1)\n","output_total_ssq_group_2=output_total_ssq_group_2.reshape(-1, 1)\n","output_total_ssq_group_3=output_total_ssq_group_3.reshape(-1, 1)\n","output_total_ssq_group_4=output_total_ssq_group_4.reshape(-1, 1)\n","output_total_ssq_group_5=output_total_ssq_group_5.reshape(-1, 1)\n","\n"],"metadata":{"id":"mMYkeYtMm2b8","executionInfo":{"status":"ok","timestamp":1724915251139,"user_tz":300,"elapsed":2508,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["def scale_input_data(input_train, input_test):\n","    # Get the shape of the input data\n","    num_samples_train, time_steps_train, num_features = input_train.shape\n","    num_samples_test, time_steps_test, _ = input_test.shape\n","\n","    # Reshape the input data into 2D arrays\n","    flattened_train_data = input_train.reshape(-1, num_features)\n","    flattened_test_data = input_test.reshape(-1, num_features)\n","\n","    # Initialize a MinMaxScaler object\n","    scaler = MinMaxScaler()\n","\n","    # Fit the scaler on the training data and transform both train and test data\n","    scaled_train_data = scaler.fit_transform(flattened_train_data)\n","    scaled_test_data = scaler.transform(flattened_test_data)\n","\n","    # Reshape the scaled data back to its original shape\n","    scaled_train_data = scaled_train_data.reshape(num_samples_train, time_steps_train, num_features)\n","    scaled_test_data = scaled_test_data.reshape(num_samples_test, time_steps_test, num_features)\n","\n","    return scaled_train_data, scaled_test_data\n","\n","def scale_target_var(target_data):\n","    min_val, max_val = np.min(target_data, axis=0), np.max(target_data, axis=0)\n","    target_data = (target_data-min_val)/(max_val-min_val)\n","\n","    return target_data, min_val, max_val"],"metadata":{"id":"26ADF-kiC1EZ","executionInfo":{"status":"ok","timestamp":1724915251140,"user_tz":300,"elapsed":6,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["from keras.models import Sequential\n","from keras.layers import Input, GRU, Dense, Dropout\n","from keras.models import Model\n","import numpy as np\n","import sklearn\n","\n","total_losses=[]\n","def get_shared_GRU(input_shape1, input_shape2):\n","    # Define shared GRU model\n","    input_layer = Input(shape=(input_shape1, input_shape2))\n","    x = GRU(64, return_sequences=False)(input_layer)\n","    x = Dense(256, activation='relu')(x)\n","    x = Dropout(0.2)(x)\n","    shared_model = Model(inputs=input_layer, outputs=x)\n","    return shared_model\n","\n","\n","\n","\n","def get_output_model(shared_GRU_output, output_shape):\n","    # Define separate output model for each column\n","    output_models = []\n","    for i in range(output_shape[1]):\n","        output_model = Sequential()\n","        output_model.add(Dense(256, activation='relu'))\n","        output_model.add(Dropout(0.2))\n","        output_model.add(Dense(1))  # Output shape is (None, 1) for each column\n","        output_model_output = output_model(shared_GRU_output)\n","        output_models.append(output_model_output)\n","    return output_models\n","input_groups = [input_group_1, input_group_2, input_group_3, input_group_4, input_group_5]\n","output_groups = [output_group_1, output_group_2, output_group_3, output_group_4, output_group_5]\n","ssq_groups = [output_total_ssq_group_1, output_total_ssq_group_2, output_total_ssq_group_3, output_total_ssq_group_4, output_total_ssq_group_5]\n","\n","\n","# Specify the number of samples to select for each group in each iteration\n","samples_per_iteration = [\n","    [3, 3, 3, 3, 2],  # For input_group_1\n","    [3, 3, 3, 3, 2],  # For input_group_2\n","    [3, 3, 3, 2, 2],  # For input_group_3\n","    [2, 2, 2, 2, 3],  # For input_group_4\n","    [3, 3, 3, 3, 4]   # For input_group_5\n","]\n","\n","# Initialize a list of global indices arrays, one for each group\n","global_indices = [[] for _ in range(len(input_groups))]\n","print(\"global_indices\",global_indices)\n","\n","# Outer loop to repeat the sampling process for 5 iterations\n","for iteration in range(5):\n","  X_train, X_val, X_test = [], [], []\n","  y_train, y_val, y_test = [], [], []\n","  ssq_train, ssq_val, ssq_test = [], [], []\n","  print(f\"Iteration {iteration + 1}\")\n","  print(\"global_indices\",global_indices)\n","  # Loop over each group\n","  for i, (input_group, output_group, ssq_group) in enumerate(zip(input_groups, output_groups, ssq_groups)):\n","      num_samples = samples_per_iteration[i][iteration]  # Number of samples to select for the current group and iteration\n","\n","      # Create a set of available indices that haven't been selected yet for the current group\n","      available_indices = list(set(range(len(input_group))) - set(global_indices[i]))\n","\n","      # Check if there are fewer available indices than needed\n","      if len(available_indices) < num_samples:\n","          print(f\"Not enough indices left in group {i + 1} to select {num_samples} new samples.\")\n","          num_samples = len(available_indices)  # Adjust to take whatever is left\n","\n","      # Select the required number of samples from the available indices for the current group\n","      selected_indices = np.random.choice(available_indices, num_samples, replace=False)\n","      global_indices[i].extend(selected_indices)  # Add these indices to the group's global list\n","\n","      # Remove these selected samples from the input, output, and SSQ groups\n","      X_test_temp = input_group[selected_indices]\n","      y_test_temp = output_group[selected_indices]\n","      ssq_test_temp = ssq_group[selected_indices]\n","\n","      X_temp = np.delete(input_group, selected_indices, axis=0)\n","      y_temp = np.delete(output_group, selected_indices, axis=0)\n","      ssq_temp = np.delete(ssq_group, selected_indices, axis=0)\n","\n","      # Split the remaining data into a training set (60%) and a validation set (40%)\n","      X_train_temp, X_val_temp, y_train_temp, y_val_temp, ssq_train_temp, ssq_val_temp = train_test_split(\n","          X_temp, y_temp, ssq_temp, test_size=0.2)\n","\n","      # Append the results to the corresponding lists\n","      X_train.append(X_train_temp)\n","      X_val.append(X_val_temp)\n","      X_test.append(X_test_temp)\n","\n","      y_train.append(y_train_temp)\n","      y_val.append(y_val_temp)\n","      y_test.append(y_test_temp)\n","\n","      ssq_train.append(ssq_train_temp)\n","      ssq_val.append(ssq_val_temp)\n","      ssq_test.append(ssq_test_temp)\n","\n","  # After the loop, concatenate the data for all groups if needed\n","  input_train = np.concatenate(X_train, axis=0)\n","  input_val = np.concatenate(X_val, axis=0)\n","  input_test = np.concatenate(X_test, axis=0)\n","\n","  output_train = np.concatenate(y_train, axis=0)\n","  output_val = np.concatenate(y_val, axis=0)\n","  output_test = np.concatenate(y_test, axis=0)\n","\n","  output_test_total_ssq = np.concatenate(ssq_test, axis=0)\n","\n","\n","  #  this section for scaling both train and validation set simultaniously\n","  # Step 1: Combine the training and validation sets\n","  combined_input = np.concatenate([input_train, input_val], axis=0)\n","  combined_output = np.concatenate([output_train, output_val], axis=0)\n","\n","  # Step 2: Scale the combined input data\n","  # Assuming scale_input_data scales the data based on the combined dataset\n","  combined_input, input_test = scale_input_data(\n","      combined_input[:, (60-sample_size):(180-sample_size), :],\n","      input_test[:, (60-sample_size):(180-sample_size), :]\n","  )\n","\n","  # Step 3: Scale the combined output data\n","  # Assuming scale_target_var scales the data and returns min_val, max_val\n","  combined_output, min_val, max_val = scale_target_var(combined_output)\n","\n","  # Step 4: Split the combined data back into training and validation sets\n","  # Use the original shapes of input_train and input_val to slice the combined arrays\n","  input_train = combined_input[:input_train.shape[0], :, :]\n","  input_val = combined_input[input_train.shape[0]:, :, :]\n","\n","  output_train = combined_output[:output_train.shape[0], :]\n","  output_val = combined_output[output_train.shape[0]:, :]\n","\n","\n","\n","  print(\"input_train :\", input_train.shape)\n","  print(\"output_train :\", output_train.shape)\n","  print(\"input_val :\", input_val.shape)\n","  print(\"output_val :\", output_val.shape)\n","  print(\"input_test :\", input_test.shape)\n","  print(\"output_test :\", output_test.shape)\n","\n","\n","\n","\n","\n","\n","  # Reshape train and test inputs to match GRU input shape\n","  train_input_reshaped = input_train.reshape((input_train.shape[0], input_train.shape[1], input_train.shape[2]))\n","  test_input_reshaped = input_test.reshape((input_test.shape[0], input_test.shape[1], input_test.shape[2]))\n","  val_input_reshaped = input_val.reshape((input_val.shape[0], input_val.shape[1], input_val.shape[2]))\n","  # Get shared GRU model\n","  shared_GRU = get_shared_GRU(input_train.shape[1], input_train.shape[2])\n","\n","  # Create separate output models for each column\n","  output_models = get_output_model(shared_GRU.output, output_train.shape)\n","\n","  # Create combined model\n","  model = Model(inputs=shared_GRU.input, outputs=output_models)\n","\n","  # Compile and train the model\n","  model.compile(loss='mse', optimizer='adam', metrics=[['mse'] for _ in range(output_train.shape[1])])  # Using MSE as loss and metric\n","  best_val=1000000\n","  patience=0\n","  best_model = None\n","\n","  for k in range(200):\n","      # Predict test data\n","      model.fit(train_input_reshaped, [output_train[:, i] for i in range(output_train.shape[1])], epochs=1, batch_size=32)\n","      pred_val = np.array(model.predict(val_input_reshaped))\n","      pred_val = np.transpose(pred_val.squeeze(), (1, 0))\n","      print(\"k:\", k, \"patience:\", patience)\n","      # Evaluate the model\n","      losses = []\n","      for i in range(pred_val.shape[0]):\n","        total_ssq=0\n","        for j in [0,5,6,7,8,14,15]:\n","          total_ssq=np.sum(pred_val[i,j]*(max_val[j]-min_val[j]) + min_val[j])+total_ssq\n","\n","        for j in [0,1,2,3,4,8,10]:\n","          total_ssq=np.sum(pred_val[i,j]*(max_val[j]-min_val[j]) + min_val[j])+total_ssq\n","\n","        for j in [4,7,9,10,11,12,13]:\n","          total_ssq=np.sum(pred_val[i,j]*(max_val[j]-min_val[j]) + min_val[j])+total_ssq\n","        total_ssq=total_ssq*3.74\n","        output_val_ssq= output_val[i,0]\n","        #print(\"total_ssq\",total_ssq)\n","        #print(\"output_val_ssq\",output_val_ssq)\n","        loss = sklearn.metrics.mean_squared_error([total_ssq], [output_val_ssq], squared=False)\n","        losses.append(loss)\n","      tmp_val_loss = np.mean(losses)\n","      if tmp_val_loss <= best_val:\n","          best_val = tmp_val_loss\n","          patience = 0\n","          best_model = model\n","      else:\n","          patience +=1\n","          if patience > 10:\n","            break\n","\n","    # Predict test data\n","      pred_test = np.array(best_model.predict(test_input_reshaped))\n","      pred_test = np.transpose(pred_test.squeeze(), (1, 0))\n","      # Evaluate the model\n","      pred_total_ssq = []\n","      #losses=[]\n","      for i in range(pred_test.shape[0]):\n","          total_ssq=0\n","          for j in [0,5,6,7,8,14,15]:\n","            total_ssq=np.sum(pred_test[i,j]*(max_val[j]-min_val[j]) + min_val[j])+total_ssq\n","\n","          for j in [0,1,2,3,4,8,10]:\n","            total_ssq=np.sum(pred_test[i,j]*(max_val[j]-min_val[j]) + min_val[j])+total_ssq\n","\n","          for j in [4,7,9,10,11,12,13]:\n","            total_ssq=np.sum(pred_test[i,j]*(max_val[j]-min_val[j]) + min_val[j])+total_ssq\n","          total_ssq=total_ssq*3.74\n","\n","          pred_total_ssq.append(total_ssq)\n","\n","\n","      # Overall Test Loss\n","      loss = sklearn.metrics.mean_squared_error(pred_total_ssq, output_test_total_ssq, squared = False)\n","      print(\"Test Loss no \",iteration,\":\" ,loss)\n","      total_losses.append(loss)\n","\n","average_loss = sum(total_losses) / len(total_losses)\n","total_losses.append(average_loss)\n","print(\"average_loss:\", average_loss)"],"metadata":{"id":"E6ssyYUeDJwI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724915415225,"user_tz":300,"elapsed":164090,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}},"outputId":"42374f9f-f3d6-4c57-be92-e140b5dac6e3"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["global_indices [[], [], [], [], []]\n","Iteration 1\n","global_indices [[], [], [], [], []]\n","input_train : (41, 120, 47)\n","output_train : (41, 16)\n","input_val : (13, 120, 47)\n","output_val : (13, 16)\n","input_test : (14, 120, 47)\n","output_test : (14, 16)\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 111ms/step - loss: 1.5370 - sequential_80_mse: 0.1214 - sequential_81_mse: 0.2484 - sequential_82_mse: 0.0356 - sequential_83_mse: 0.1880 - sequential_84_mse: 0.0281 - sequential_85_mse: 0.0604 - sequential_86_mse: 0.1600 - sequential_87_mse: 0.1374 - sequential_88_mse: 0.0311 - sequential_89_mse: 0.0442 - sequential_90_mse: 0.0623 - sequential_91_mse: 0.0580 - sequential_92_mse: 0.0470 - sequential_93_mse: 0.0611 - sequential_94_mse: 0.1785 - sequential_95_mse: 0.0754\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 598ms/step\n","k: 0 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 621ms/step\n","Test Loss no  0 : 23.882931621855878\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - loss: 1.0128 - sequential_80_mse: 0.0851 - sequential_81_mse: 0.0717 - sequential_82_mse: 0.0410 - sequential_83_mse: 0.1207 - sequential_84_mse: 0.0291 - sequential_85_mse: 0.0610 - sequential_86_mse: 0.1060 - sequential_87_mse: 0.0911 - sequential_88_mse: 0.0448 - sequential_89_mse: 0.0330 - sequential_90_mse: 0.0461 - sequential_91_mse: 0.0519 - sequential_92_mse: 0.0447 - sequential_93_mse: 0.0433 - sequential_94_mse: 0.0799 - sequential_95_mse: 0.0635\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n","k: 1 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n","Test Loss no  0 : 22.044121874828072\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 225ms/step - loss: 0.9529 - sequential_80_mse: 0.0770 - sequential_81_mse: 0.0915 - sequential_82_mse: 0.0346 - sequential_83_mse: 0.1005 - sequential_84_mse: 0.0207 - sequential_85_mse: 0.0546 - sequential_86_mse: 0.1079 - sequential_87_mse: 0.0894 - sequential_88_mse: 0.0251 - sequential_89_mse: 0.0406 - sequential_90_mse: 0.0282 - sequential_91_mse: 0.0618 - sequential_92_mse: 0.0349 - sequential_93_mse: 0.0346 - sequential_94_mse: 0.1055 - sequential_95_mse: 0.0459\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step\n","k: 2 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n","Test Loss no  0 : 17.60434860176942\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 269ms/step - loss: 0.8186 - sequential_80_mse: 0.0685 - sequential_81_mse: 0.0620 - sequential_82_mse: 0.0381 - sequential_83_mse: 0.0825 - sequential_84_mse: 0.0277 - sequential_85_mse: 0.0384 - sequential_86_mse: 0.0825 - sequential_87_mse: 0.0734 - sequential_88_mse: 0.0261 - sequential_89_mse: 0.0322 - sequential_90_mse: 0.0265 - sequential_91_mse: 0.0478 - sequential_92_mse: 0.0451 - sequential_93_mse: 0.0302 - sequential_94_mse: 0.0739 - sequential_95_mse: 0.0637\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n","k: 3 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n","Test Loss no  0 : 17.28038355018327\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - loss: 0.7389 - sequential_80_mse: 0.0762 - sequential_81_mse: 0.0493 - sequential_82_mse: 0.0257 - sequential_83_mse: 0.0668 - sequential_84_mse: 0.0200 - sequential_85_mse: 0.0425 - sequential_86_mse: 0.0839 - sequential_87_mse: 0.0649 - sequential_88_mse: 0.0185 - sequential_89_mse: 0.0268 - sequential_90_mse: 0.0266 - sequential_91_mse: 0.0448 - sequential_92_mse: 0.0307 - sequential_93_mse: 0.0321 - sequential_94_mse: 0.0776 - sequential_95_mse: 0.0525\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n","k: 4 patience: 1\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n","Test Loss no  0 : 18.47821922276834\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 0.6644 - sequential_80_mse: 0.0627 - sequential_81_mse: 0.0433 - sequential_82_mse: 0.0252 - sequential_83_mse: 0.0682 - sequential_84_mse: 0.0203 - sequential_85_mse: 0.0464 - sequential_86_mse: 0.0641 - sequential_87_mse: 0.0515 - sequential_88_mse: 0.0207 - sequential_89_mse: 0.0236 - sequential_90_mse: 0.0202 - sequential_91_mse: 0.0377 - sequential_92_mse: 0.0289 - sequential_93_mse: 0.0234 - sequential_94_mse: 0.0724 - sequential_95_mse: 0.0559\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n","k: 5 patience: 2\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n","Test Loss no  0 : 21.726884623710735\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 0.6753 - sequential_80_mse: 0.0641 - sequential_81_mse: 0.0432 - sequential_82_mse: 0.0333 - sequential_83_mse: 0.0597 - sequential_84_mse: 0.0198 - sequential_85_mse: 0.0357 - sequential_86_mse: 0.0657 - sequential_87_mse: 0.0700 - sequential_88_mse: 0.0223 - sequential_89_mse: 0.0245 - sequential_90_mse: 0.0211 - sequential_91_mse: 0.0460 - sequential_92_mse: 0.0307 - sequential_93_mse: 0.0252 - sequential_94_mse: 0.0691 - sequential_95_mse: 0.0449\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n","k: 6 patience: 3\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n","Test Loss no  0 : 20.368242720640886\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 0.6156 - sequential_80_mse: 0.0589 - sequential_81_mse: 0.0365 - sequential_82_mse: 0.0219 - sequential_83_mse: 0.0611 - sequential_84_mse: 0.0172 - sequential_85_mse: 0.0356 - sequential_86_mse: 0.0550 - sequential_87_mse: 0.0646 - sequential_88_mse: 0.0153 - sequential_89_mse: 0.0205 - sequential_90_mse: 0.0200 - sequential_91_mse: 0.0402 - sequential_92_mse: 0.0288 - sequential_93_mse: 0.0238 - sequential_94_mse: 0.0647 - sequential_95_mse: 0.0513\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n","k: 7 patience: 4\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n","Test Loss no  0 : 16.04659573277959\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 0.5940 - sequential_80_mse: 0.0648 - sequential_81_mse: 0.0370 - sequential_82_mse: 0.0222 - sequential_83_mse: 0.0475 - sequential_84_mse: 0.0183 - sequential_85_mse: 0.0308 - sequential_86_mse: 0.0682 - sequential_87_mse: 0.0486 - sequential_88_mse: 0.0121 - sequential_89_mse: 0.0241 - sequential_90_mse: 0.0282 - sequential_91_mse: 0.0390 - sequential_92_mse: 0.0259 - sequential_93_mse: 0.0212 - sequential_94_mse: 0.0632 - sequential_95_mse: 0.0429\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n","k: 8 patience: 5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n","Test Loss no  0 : 15.758118311388557\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 0.5276 - sequential_80_mse: 0.0401 - sequential_81_mse: 0.0291 - sequential_82_mse: 0.0199 - sequential_83_mse: 0.0583 - sequential_84_mse: 0.0160 - sequential_85_mse: 0.0290 - sequential_86_mse: 0.0516 - sequential_87_mse: 0.0495 - sequential_88_mse: 0.0161 - sequential_89_mse: 0.0216 - sequential_90_mse: 0.0233 - sequential_91_mse: 0.0336 - sequential_92_mse: 0.0232 - sequential_93_mse: 0.0220 - sequential_94_mse: 0.0436 - sequential_95_mse: 0.0506\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n","k: 9 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n","Test Loss no  0 : 15.335878592650449\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 0.4939 - sequential_80_mse: 0.0440 - sequential_81_mse: 0.0304 - sequential_82_mse: 0.0164 - sequential_83_mse: 0.0506 - sequential_84_mse: 0.0156 - sequential_85_mse: 0.0266 - sequential_86_mse: 0.0428 - sequential_87_mse: 0.0490 - sequential_88_mse: 0.0139 - sequential_89_mse: 0.0240 - sequential_90_mse: 0.0209 - sequential_91_mse: 0.0354 - sequential_92_mse: 0.0233 - sequential_93_mse: 0.0188 - sequential_94_mse: 0.0439 - sequential_95_mse: 0.0383\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n","k: 10 patience: 1\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n","Test Loss no  0 : 16.538020811916873\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 0.4707 - sequential_80_mse: 0.0366 - sequential_81_mse: 0.0288 - sequential_82_mse: 0.0164 - sequential_83_mse: 0.0513 - sequential_84_mse: 0.0134 - sequential_85_mse: 0.0294 - sequential_86_mse: 0.0442 - sequential_87_mse: 0.0414 - sequential_88_mse: 0.0103 - sequential_89_mse: 0.0230 - sequential_90_mse: 0.0141 - sequential_91_mse: 0.0336 - sequential_92_mse: 0.0228 - sequential_93_mse: 0.0182 - sequential_94_mse: 0.0454 - sequential_95_mse: 0.0418\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n","k: 11 patience: 2\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n","Test Loss no  0 : 16.79803895213355\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - loss: 0.4294 - sequential_80_mse: 0.0368 - sequential_81_mse: 0.0287 - sequential_82_mse: 0.0162 - sequential_83_mse: 0.0483 - sequential_84_mse: 0.0126 - sequential_85_mse: 0.0247 - sequential_86_mse: 0.0411 - sequential_87_mse: 0.0396 - sequential_88_mse: 0.0099 - sequential_89_mse: 0.0196 - sequential_90_mse: 0.0148 - sequential_91_mse: 0.0310 - sequential_92_mse: 0.0178 - sequential_93_mse: 0.0154 - sequential_94_mse: 0.0436 - sequential_95_mse: 0.0292\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n","k: 12 patience: 3\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n","Test Loss no  0 : 15.322728822615005\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 0.4383 - sequential_80_mse: 0.0377 - sequential_81_mse: 0.0309 - sequential_82_mse: 0.0185 - sequential_83_mse: 0.0512 - sequential_84_mse: 0.0106 - sequential_85_mse: 0.0193 - sequential_86_mse: 0.0356 - sequential_87_mse: 0.0314 - sequential_88_mse: 0.0126 - sequential_89_mse: 0.0227 - sequential_90_mse: 0.0134 - sequential_91_mse: 0.0325 - sequential_92_mse: 0.0238 - sequential_93_mse: 0.0214 - sequential_94_mse: 0.0453 - sequential_95_mse: 0.0315\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n","k: 13 patience: 4\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n","Test Loss no  0 : 15.239632172274202\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.3867 - sequential_80_mse: 0.0361 - sequential_81_mse: 0.0268 - sequential_82_mse: 0.0135 - sequential_83_mse: 0.0457 - sequential_84_mse: 0.0104 - sequential_85_mse: 0.0191 - sequential_86_mse: 0.0374 - sequential_87_mse: 0.0340 - sequential_88_mse: 0.0101 - sequential_89_mse: 0.0135 - sequential_90_mse: 0.0125 - sequential_91_mse: 0.0299 - sequential_92_mse: 0.0195 - sequential_93_mse: 0.0115 - sequential_94_mse: 0.0344 - sequential_95_mse: 0.0323\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n","k: 14 patience: 5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n","Test Loss no  0 : 15.757756670595\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 0.3521 - sequential_80_mse: 0.0240 - sequential_81_mse: 0.0247 - sequential_82_mse: 0.0117 - sequential_83_mse: 0.0397 - sequential_84_mse: 0.0142 - sequential_85_mse: 0.0152 - sequential_86_mse: 0.0268 - sequential_87_mse: 0.0261 - sequential_88_mse: 0.0116 - sequential_89_mse: 0.0215 - sequential_90_mse: 0.0173 - sequential_91_mse: 0.0284 - sequential_92_mse: 0.0203 - sequential_93_mse: 0.0176 - sequential_94_mse: 0.0300 - sequential_95_mse: 0.0232\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n","k: 15 patience: 6\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n","Test Loss no  0 : 16.982391836548313\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 0.3229 - sequential_80_mse: 0.0240 - sequential_81_mse: 0.0257 - sequential_82_mse: 0.0079 - sequential_83_mse: 0.0334 - sequential_84_mse: 0.0110 - sequential_85_mse: 0.0199 - sequential_86_mse: 0.0324 - sequential_87_mse: 0.0167 - sequential_88_mse: 0.0091 - sequential_89_mse: 0.0170 - sequential_90_mse: 0.0132 - sequential_91_mse: 0.0239 - sequential_92_mse: 0.0195 - sequential_93_mse: 0.0148 - sequential_94_mse: 0.0283 - sequential_95_mse: 0.0261\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n","k: 16 patience: 7\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n","Test Loss no  0 : 14.9623777722298\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 0.3255 - sequential_80_mse: 0.0272 - sequential_81_mse: 0.0201 - sequential_82_mse: 0.0105 - sequential_83_mse: 0.0262 - sequential_84_mse: 0.0091 - sequential_85_mse: 0.0180 - sequential_86_mse: 0.0338 - sequential_87_mse: 0.0255 - sequential_88_mse: 0.0092 - sequential_89_mse: 0.0186 - sequential_90_mse: 0.0116 - sequential_91_mse: 0.0288 - sequential_92_mse: 0.0193 - sequential_93_mse: 0.0184 - sequential_94_mse: 0.0278 - sequential_95_mse: 0.0214\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n","k: 17 patience: 8\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n","Test Loss no  0 : 14.349615195462079\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.3178 - sequential_80_mse: 0.0234 - sequential_81_mse: 0.0201 - sequential_82_mse: 0.0165 - sequential_83_mse: 0.0313 - sequential_84_mse: 0.0084 - sequential_85_mse: 0.0136 - sequential_86_mse: 0.0306 - sequential_87_mse: 0.0274 - sequential_88_mse: 0.0099 - sequential_89_mse: 0.0173 - sequential_90_mse: 0.0136 - sequential_91_mse: 0.0255 - sequential_92_mse: 0.0160 - sequential_93_mse: 0.0147 - sequential_94_mse: 0.0292 - sequential_95_mse: 0.0200\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n","k: 18 patience: 9\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n","Test Loss no  0 : 14.303302386928335\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 0.3065 - sequential_80_mse: 0.0212 - sequential_81_mse: 0.0196 - sequential_82_mse: 0.0110 - sequential_83_mse: 0.0351 - sequential_84_mse: 0.0127 - sequential_85_mse: 0.0143 - sequential_86_mse: 0.0314 - sequential_87_mse: 0.0209 - sequential_88_mse: 0.0095 - sequential_89_mse: 0.0188 - sequential_90_mse: 0.0139 - sequential_91_mse: 0.0269 - sequential_92_mse: 0.0151 - sequential_93_mse: 0.0140 - sequential_94_mse: 0.0246 - sequential_95_mse: 0.0177\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n","k: 19 patience: 10\n","Iteration 2\n","global_indices [[13, 1, 6], [10, 0, 4], [6, 2, 3], [10, 0], [7, 13, 6]]\n","input_train : (41, 120, 47)\n","output_train : (41, 16)\n","input_val : (13, 120, 47)\n","output_val : (13, 16)\n","input_test : (14, 120, 47)\n","output_test : (14, 16)\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 102ms/step - loss: 1.5795 - sequential_100_mse: 0.1794 - sequential_101_mse: 0.0674 - sequential_102_mse: 0.1727 - sequential_103_mse: 0.1513 - sequential_104_mse: 0.0548 - sequential_105_mse: 0.0767 - sequential_106_mse: 0.0522 - sequential_107_mse: 0.1124 - sequential_108_mse: 0.0887 - sequential_109_mse: 0.0557 - sequential_110_mse: 0.1246 - sequential_111_mse: 0.0938 - sequential_96_mse: 0.1113 - sequential_97_mse: 0.0598 - sequential_98_mse: 0.0467 - sequential_99_mse: 0.1319\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 521ms/step\n","k: 0 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 567ms/step\n","Test Loss no  1 : 36.73978266214489\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 1.4240 - sequential_100_mse: 0.0698 - sequential_101_mse: 0.0989 - sequential_102_mse: 0.1243 - sequential_103_mse: 0.1220 - sequential_104_mse: 0.0735 - sequential_105_mse: 0.1062 - sequential_106_mse: 0.0485 - sequential_107_mse: 0.0898 - sequential_108_mse: 0.0539 - sequential_109_mse: 0.0751 - sequential_110_mse: 0.1060 - sequential_111_mse: 0.0800 - sequential_96_mse: 0.1162 - sequential_97_mse: 0.0661 - sequential_98_mse: 0.0533 - sequential_99_mse: 0.1403\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n","k: 1 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n","Test Loss no  1 : 32.30409789034443\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 1.1628 - sequential_100_mse: 0.0934 - sequential_101_mse: 0.0683 - sequential_102_mse: 0.1079 - sequential_103_mse: 0.1035 - sequential_104_mse: 0.0452 - sequential_105_mse: 0.0583 - sequential_106_mse: 0.0455 - sequential_107_mse: 0.0824 - sequential_108_mse: 0.0506 - sequential_109_mse: 0.0434 - sequential_110_mse: 0.0953 - sequential_111_mse: 0.0808 - sequential_96_mse: 0.0878 - sequential_97_mse: 0.0745 - sequential_98_mse: 0.0389 - sequential_99_mse: 0.0871\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n","k: 2 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n","Test Loss no  1 : 30.814114777351893\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 1.1371 - sequential_100_mse: 0.0590 - sequential_101_mse: 0.0638 - sequential_102_mse: 0.1101 - sequential_103_mse: 0.0986 - sequential_104_mse: 0.0462 - sequential_105_mse: 0.0588 - sequential_106_mse: 0.0385 - sequential_107_mse: 0.0724 - sequential_108_mse: 0.0539 - sequential_109_mse: 0.0513 - sequential_110_mse: 0.1218 - sequential_111_mse: 0.0804 - sequential_96_mse: 0.0899 - sequential_97_mse: 0.0679 - sequential_98_mse: 0.0315 - sequential_99_mse: 0.0932\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n","k: 3 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n","Test Loss no  1 : 31.79993743581971\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 1.0380 - sequential_100_mse: 0.0553 - sequential_101_mse: 0.0662 - sequential_102_mse: 0.0991 - sequential_103_mse: 0.0939 - sequential_104_mse: 0.0395 - sequential_105_mse: 0.0531 - sequential_106_mse: 0.0335 - sequential_107_mse: 0.0687 - sequential_108_mse: 0.0456 - sequential_109_mse: 0.0519 - sequential_110_mse: 0.1114 - sequential_111_mse: 0.0645 - sequential_96_mse: 0.0779 - sequential_97_mse: 0.0540 - sequential_98_mse: 0.0253 - sequential_99_mse: 0.0981\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n","k: 4 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n","Test Loss no  1 : 30.451686453326317\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 0.9794 - sequential_100_mse: 0.0543 - sequential_101_mse: 0.0694 - sequential_102_mse: 0.1024 - sequential_103_mse: 0.0942 - sequential_104_mse: 0.0437 - sequential_105_mse: 0.0450 - sequential_106_mse: 0.0342 - sequential_107_mse: 0.0695 - sequential_108_mse: 0.0393 - sequential_109_mse: 0.0378 - sequential_110_mse: 0.0963 - sequential_111_mse: 0.0655 - sequential_96_mse: 0.0816 - sequential_97_mse: 0.0445 - sequential_98_mse: 0.0312 - sequential_99_mse: 0.0703\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n","k: 5 patience: 1\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n","Test Loss no  1 : 30.01194994725509\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 0.8719 - sequential_100_mse: 0.0440 - sequential_101_mse: 0.0479 - sequential_102_mse: 0.0882 - sequential_103_mse: 0.0777 - sequential_104_mse: 0.0273 - sequential_105_mse: 0.0391 - sequential_106_mse: 0.0255 - sequential_107_mse: 0.0489 - sequential_108_mse: 0.0457 - sequential_109_mse: 0.0354 - sequential_110_mse: 0.0871 - sequential_111_mse: 0.0701 - sequential_96_mse: 0.0712 - sequential_97_mse: 0.0551 - sequential_98_mse: 0.0262 - sequential_99_mse: 0.0824\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n","k: 6 patience: 2\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n","Test Loss no  1 : 29.978593469199765\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.8783 - sequential_100_mse: 0.0526 - sequential_101_mse: 0.0591 - sequential_102_mse: 0.0837 - sequential_103_mse: 0.0753 - sequential_104_mse: 0.0349 - sequential_105_mse: 0.0440 - sequential_106_mse: 0.0302 - sequential_107_mse: 0.0653 - sequential_108_mse: 0.0319 - sequential_109_mse: 0.0383 - sequential_110_mse: 0.0902 - sequential_111_mse: 0.0616 - sequential_96_mse: 0.0614 - sequential_97_mse: 0.0465 - sequential_98_mse: 0.0221 - sequential_99_mse: 0.0813\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n","k: 7 patience: 3\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n","Test Loss no  1 : 29.557464479841975\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - loss: 0.8958 - sequential_100_mse: 0.0455 - sequential_101_mse: 0.0564 - sequential_102_mse: 0.0880 - sequential_103_mse: 0.0840 - sequential_104_mse: 0.0289 - sequential_105_mse: 0.0508 - sequential_106_mse: 0.0275 - sequential_107_mse: 0.0631 - sequential_108_mse: 0.0423 - sequential_109_mse: 0.0382 - sequential_110_mse: 0.0882 - sequential_111_mse: 0.0553 - sequential_96_mse: 0.0646 - sequential_97_mse: 0.0479 - sequential_98_mse: 0.0375 - sequential_99_mse: 0.0775\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n","k: 8 patience: 4\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n","Test Loss no  1 : 29.260881944732084\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: 0.7682 - sequential_100_mse: 0.0371 - sequential_101_mse: 0.0555 - sequential_102_mse: 0.0712 - sequential_103_mse: 0.0681 - sequential_104_mse: 0.0282 - sequential_105_mse: 0.0397 - sequential_106_mse: 0.0270 - sequential_107_mse: 0.0496 - sequential_108_mse: 0.0308 - sequential_109_mse: 0.0374 - sequential_110_mse: 0.0892 - sequential_111_mse: 0.0518 - sequential_96_mse: 0.0599 - sequential_97_mse: 0.0369 - sequential_98_mse: 0.0254 - sequential_99_mse: 0.0603\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n","k: 9 patience: 5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n","Test Loss no  1 : 29.53436591557524\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 0.8261 - sequential_100_mse: 0.0371 - sequential_101_mse: 0.0505 - sequential_102_mse: 0.0730 - sequential_103_mse: 0.0751 - sequential_104_mse: 0.0312 - sequential_105_mse: 0.0433 - sequential_106_mse: 0.0297 - sequential_107_mse: 0.0636 - sequential_108_mse: 0.0414 - sequential_109_mse: 0.0402 - sequential_110_mse: 0.0943 - sequential_111_mse: 0.0515 - sequential_96_mse: 0.0622 - sequential_97_mse: 0.0429 - sequential_98_mse: 0.0274 - sequential_99_mse: 0.0625\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n","k: 10 patience: 6\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n","Test Loss no  1 : 30.04415882076246\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 0.7927 - sequential_100_mse: 0.0393 - sequential_101_mse: 0.0569 - sequential_102_mse: 0.0728 - sequential_103_mse: 0.0714 - sequential_104_mse: 0.0297 - sequential_105_mse: 0.0430 - sequential_106_mse: 0.0313 - sequential_107_mse: 0.0587 - sequential_108_mse: 0.0402 - sequential_109_mse: 0.0357 - sequential_110_mse: 0.0789 - sequential_111_mse: 0.0448 - sequential_96_mse: 0.0552 - sequential_97_mse: 0.0460 - sequential_98_mse: 0.0258 - sequential_99_mse: 0.0628\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n","k: 11 patience: 7\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n","Test Loss no  1 : 29.747118508024734\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 0.7427 - sequential_100_mse: 0.0401 - sequential_101_mse: 0.0525 - sequential_102_mse: 0.0716 - sequential_103_mse: 0.0734 - sequential_104_mse: 0.0245 - sequential_105_mse: 0.0360 - sequential_106_mse: 0.0231 - sequential_107_mse: 0.0569 - sequential_108_mse: 0.0333 - sequential_109_mse: 0.0352 - sequential_110_mse: 0.0793 - sequential_111_mse: 0.0440 - sequential_96_mse: 0.0552 - sequential_97_mse: 0.0412 - sequential_98_mse: 0.0238 - sequential_99_mse: 0.0525\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n","k: 12 patience: 8\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n","Test Loss no  1 : 29.466052620792645\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - loss: 0.6763 - sequential_100_mse: 0.0359 - sequential_101_mse: 0.0436 - sequential_102_mse: 0.0589 - sequential_103_mse: 0.0645 - sequential_104_mse: 0.0245 - sequential_105_mse: 0.0290 - sequential_106_mse: 0.0239 - sequential_107_mse: 0.0578 - sequential_108_mse: 0.0314 - sequential_109_mse: 0.0332 - sequential_110_mse: 0.0675 - sequential_111_mse: 0.0405 - sequential_96_mse: 0.0422 - sequential_97_mse: 0.0377 - sequential_98_mse: 0.0268 - sequential_99_mse: 0.0589\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n","k: 13 patience: 9\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n","Test Loss no  1 : 29.351919945366614\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 0.6439 - sequential_100_mse: 0.0423 - sequential_101_mse: 0.0406 - sequential_102_mse: 0.0498 - sequential_103_mse: 0.0559 - sequential_104_mse: 0.0275 - sequential_105_mse: 0.0308 - sequential_106_mse: 0.0237 - sequential_107_mse: 0.0514 - sequential_108_mse: 0.0266 - sequential_109_mse: 0.0347 - sequential_110_mse: 0.0624 - sequential_111_mse: 0.0370 - sequential_96_mse: 0.0443 - sequential_97_mse: 0.0386 - sequential_98_mse: 0.0191 - sequential_99_mse: 0.0591\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n","k: 14 patience: 10\n","Iteration 3\n","global_indices [[13, 1, 6, 2, 3, 4], [10, 0, 4, 13, 8, 6], [6, 2, 3, 5, 12, 7], [10, 0, 3, 2], [7, 13, 6, 12, 10, 5]]\n","input_train : (41, 120, 47)\n","output_train : (41, 16)\n","input_val : (13, 120, 47)\n","output_val : (13, 16)\n","input_test : (14, 120, 47)\n","output_test : (14, 16)\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 103ms/step - loss: 1.6079 - sequential_112_mse: 0.1414 - sequential_113_mse: 0.1283 - sequential_114_mse: 0.0509 - sequential_115_mse: 0.0943 - sequential_116_mse: 0.0488 - sequential_117_mse: 0.0773 - sequential_118_mse: 0.1645 - sequential_119_mse: 0.1130 - sequential_120_mse: 0.0546 - sequential_121_mse: 0.0700 - sequential_122_mse: 0.1354 - sequential_123_mse: 0.1140 - sequential_124_mse: 0.0713 - sequential_125_mse: 0.0912 - sequential_126_mse: 0.1932 - sequential_127_mse: 0.0597\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 549ms/step\n","k: 0 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 580ms/step\n","Test Loss no  2 : 26.014431637315933\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - loss: 1.3636 - sequential_112_mse: 0.1164 - sequential_113_mse: 0.1458 - sequential_114_mse: 0.0575 - sequential_115_mse: 0.1050 - sequential_116_mse: 0.0703 - sequential_117_mse: 0.0781 - sequential_118_mse: 0.1456 - sequential_119_mse: 0.0946 - sequential_120_mse: 0.0544 - sequential_121_mse: 0.0507 - sequential_122_mse: 0.0727 - sequential_123_mse: 0.1150 - sequential_124_mse: 0.0655 - sequential_125_mse: 0.0480 - sequential_126_mse: 0.0894 - sequential_127_mse: 0.0545\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n","k: 1 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n","Test Loss no  2 : 26.03542326543987\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 1.2320 - sequential_112_mse: 0.1015 - sequential_113_mse: 0.0657 - sequential_114_mse: 0.0467 - sequential_115_mse: 0.1292 - sequential_116_mse: 0.0600 - sequential_117_mse: 0.0848 - sequential_118_mse: 0.1258 - sequential_119_mse: 0.1100 - sequential_120_mse: 0.0470 - sequential_121_mse: 0.0518 - sequential_122_mse: 0.0452 - sequential_123_mse: 0.0945 - sequential_124_mse: 0.0542 - sequential_125_mse: 0.0682 - sequential_126_mse: 0.0849 - sequential_127_mse: 0.0624\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n","k: 2 patience: 1\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n","Test Loss no  2 : 25.07979444734337\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - loss: 0.9595 - sequential_112_mse: 0.0954 - sequential_113_mse: 0.0439 - sequential_114_mse: 0.0301 - sequential_115_mse: 0.1232 - sequential_116_mse: 0.0525 - sequential_117_mse: 0.0554 - sequential_118_mse: 0.0935 - sequential_119_mse: 0.0803 - sequential_120_mse: 0.0341 - sequential_121_mse: 0.0460 - sequential_122_mse: 0.0364 - sequential_123_mse: 0.0898 - sequential_124_mse: 0.0398 - sequential_125_mse: 0.0384 - sequential_126_mse: 0.0676 - sequential_127_mse: 0.0331\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n","k: 3 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n","Test Loss no  2 : 25.664311709264897\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 0.9969 - sequential_112_mse: 0.0958 - sequential_113_mse: 0.0532 - sequential_114_mse: 0.0327 - sequential_115_mse: 0.0822 - sequential_116_mse: 0.0441 - sequential_117_mse: 0.0676 - sequential_118_mse: 0.1062 - sequential_119_mse: 0.0967 - sequential_120_mse: 0.0385 - sequential_121_mse: 0.0481 - sequential_122_mse: 0.0488 - sequential_123_mse: 0.0880 - sequential_124_mse: 0.0399 - sequential_125_mse: 0.0460 - sequential_126_mse: 0.0689 - sequential_127_mse: 0.0405\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n","k: 4 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n","Test Loss no  2 : 23.818010403803733\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 0.9185 - sequential_112_mse: 0.0761 - sequential_113_mse: 0.0493 - sequential_114_mse: 0.0390 - sequential_115_mse: 0.0877 - sequential_116_mse: 0.0375 - sequential_117_mse: 0.0607 - sequential_118_mse: 0.0878 - sequential_119_mse: 0.0746 - sequential_120_mse: 0.0419 - sequential_121_mse: 0.0457 - sequential_122_mse: 0.0438 - sequential_123_mse: 0.0799 - sequential_124_mse: 0.0418 - sequential_125_mse: 0.0414 - sequential_126_mse: 0.0713 - sequential_127_mse: 0.0400\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n","k: 5 patience: 1\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n","Test Loss no  2 : 23.451431684792762\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 0.8460 - sequential_112_mse: 0.0814 - sequential_113_mse: 0.0401 - sequential_114_mse: 0.0240 - sequential_115_mse: 0.0792 - sequential_116_mse: 0.0410 - sequential_117_mse: 0.0692 - sequential_118_mse: 0.0836 - sequential_119_mse: 0.0829 - sequential_120_mse: 0.0235 - sequential_121_mse: 0.0300 - sequential_122_mse: 0.0273 - sequential_123_mse: 0.0802 - sequential_124_mse: 0.0349 - sequential_125_mse: 0.0353 - sequential_126_mse: 0.0729 - sequential_127_mse: 0.0404\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n","k: 6 patience: 2\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n","Test Loss no  2 : 24.01164433205051\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.8495 - sequential_112_mse: 0.0737 - sequential_113_mse: 0.0451 - sequential_114_mse: 0.0298 - sequential_115_mse: 0.0823 - sequential_116_mse: 0.0368 - sequential_117_mse: 0.0659 - sequential_118_mse: 0.0878 - sequential_119_mse: 0.0707 - sequential_120_mse: 0.0261 - sequential_121_mse: 0.0330 - sequential_122_mse: 0.0404 - sequential_123_mse: 0.0770 - sequential_124_mse: 0.0371 - sequential_125_mse: 0.0395 - sequential_126_mse: 0.0668 - sequential_127_mse: 0.0375\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","k: 7 patience: 3\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n","Test Loss no  2 : 22.84802073278737\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 0.7884 - sequential_112_mse: 0.0633 - sequential_113_mse: 0.0384 - sequential_114_mse: 0.0317 - sequential_115_mse: 0.0676 - sequential_116_mse: 0.0388 - sequential_117_mse: 0.0559 - sequential_118_mse: 0.0840 - sequential_119_mse: 0.0757 - sequential_120_mse: 0.0230 - sequential_121_mse: 0.0350 - sequential_122_mse: 0.0349 - sequential_123_mse: 0.0844 - sequential_124_mse: 0.0377 - sequential_125_mse: 0.0298 - sequential_126_mse: 0.0567 - sequential_127_mse: 0.0316\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n","k: 8 patience: 4\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n","Test Loss no  2 : 23.19060172092529\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 0.7770 - sequential_112_mse: 0.0626 - sequential_113_mse: 0.0396 - sequential_114_mse: 0.0306 - sequential_115_mse: 0.0827 - sequential_116_mse: 0.0402 - sequential_117_mse: 0.0558 - sequential_118_mse: 0.0759 - sequential_119_mse: 0.0718 - sequential_120_mse: 0.0280 - sequential_121_mse: 0.0365 - sequential_122_mse: 0.0273 - sequential_123_mse: 0.0698 - sequential_124_mse: 0.0344 - sequential_125_mse: 0.0305 - sequential_126_mse: 0.0596 - sequential_127_mse: 0.0317\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n","k: 9 patience: 5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n","Test Loss no  2 : 24.02096017524279\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.7017 - sequential_112_mse: 0.0507 - sequential_113_mse: 0.0378 - sequential_114_mse: 0.0309 - sequential_115_mse: 0.0584 - sequential_116_mse: 0.0351 - sequential_117_mse: 0.0537 - sequential_118_mse: 0.0684 - sequential_119_mse: 0.0626 - sequential_120_mse: 0.0292 - sequential_121_mse: 0.0336 - sequential_122_mse: 0.0285 - sequential_123_mse: 0.0636 - sequential_124_mse: 0.0354 - sequential_125_mse: 0.0255 - sequential_126_mse: 0.0554 - sequential_127_mse: 0.0331\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n","k: 10 patience: 6\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n","Test Loss no  2 : 23.050581442186676\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 0.6180 - sequential_112_mse: 0.0512 - sequential_113_mse: 0.0212 - sequential_114_mse: 0.0299 - sequential_115_mse: 0.0588 - sequential_116_mse: 0.0273 - sequential_117_mse: 0.0484 - sequential_118_mse: 0.0634 - sequential_119_mse: 0.0668 - sequential_120_mse: 0.0190 - sequential_121_mse: 0.0278 - sequential_122_mse: 0.0231 - sequential_123_mse: 0.0706 - sequential_124_mse: 0.0242 - sequential_125_mse: 0.0212 - sequential_126_mse: 0.0452 - sequential_127_mse: 0.0197\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n","k: 11 patience: 7\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n","Test Loss no  2 : 26.50626766125779\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 0.6663 - sequential_112_mse: 0.0523 - sequential_113_mse: 0.0423 - sequential_114_mse: 0.0277 - sequential_115_mse: 0.0492 - sequential_116_mse: 0.0318 - sequential_117_mse: 0.0464 - sequential_118_mse: 0.0628 - sequential_119_mse: 0.0610 - sequential_120_mse: 0.0268 - sequential_121_mse: 0.0302 - sequential_122_mse: 0.0279 - sequential_123_mse: 0.0670 - sequential_124_mse: 0.0293 - sequential_125_mse: 0.0305 - sequential_126_mse: 0.0516 - sequential_127_mse: 0.0294\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n","k: 12 patience: 8\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n","Test Loss no  2 : 28.185003757228735\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.6334 - sequential_112_mse: 0.0554 - sequential_113_mse: 0.0389 - sequential_114_mse: 0.0295 - sequential_115_mse: 0.0621 - sequential_116_mse: 0.0255 - sequential_117_mse: 0.0433 - sequential_118_mse: 0.0653 - sequential_119_mse: 0.0549 - sequential_120_mse: 0.0227 - sequential_121_mse: 0.0329 - sequential_122_mse: 0.0228 - sequential_123_mse: 0.0554 - sequential_124_mse: 0.0188 - sequential_125_mse: 0.0267 - sequential_126_mse: 0.0449 - sequential_127_mse: 0.0341\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n","k: 13 patience: 9\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n","Test Loss no  2 : 24.771545876254454\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.5988 - sequential_112_mse: 0.0455 - sequential_113_mse: 0.0361 - sequential_114_mse: 0.0262 - sequential_115_mse: 0.0513 - sequential_116_mse: 0.0237 - sequential_117_mse: 0.0450 - sequential_118_mse: 0.0678 - sequential_119_mse: 0.0555 - sequential_120_mse: 0.0239 - sequential_121_mse: 0.0273 - sequential_122_mse: 0.0284 - sequential_123_mse: 0.0549 - sequential_124_mse: 0.0233 - sequential_125_mse: 0.0210 - sequential_126_mse: 0.0440 - sequential_127_mse: 0.0248\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n","k: 14 patience: 10\n","Iteration 4\n","global_indices [[13, 1, 6, 2, 3, 4, 11, 10, 5], [10, 0, 4, 13, 8, 6, 2, 12, 11], [6, 2, 3, 5, 12, 7, 4, 1, 0], [10, 0, 3, 2, 5, 4], [7, 13, 6, 12, 10, 5, 9, 3, 8]]\n","input_train : (41, 120, 47)\n","output_train : (41, 16)\n","input_val : (14, 120, 47)\n","output_val : (14, 16)\n","input_test : (13, 120, 47)\n","output_test : (13, 16)\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 121ms/step - loss: 2.6206 - sequential_128_mse: 0.1559 - sequential_129_mse: 0.1094 - sequential_130_mse: 0.0696 - sequential_131_mse: 0.1434 - sequential_132_mse: 0.1715 - sequential_133_mse: 0.0638 - sequential_134_mse: 0.1288 - sequential_135_mse: 0.1396 - sequential_136_mse: 0.2411 - sequential_137_mse: 0.1870 - sequential_138_mse: 0.1056 - sequential_139_mse: 0.1018 - sequential_140_mse: 0.1819 - sequential_141_mse: 0.2809 - sequential_142_mse: 0.5125 - sequential_143_mse: 0.0280\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 532ms/step\n","k: 0 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 544ms/step\n","Test Loss no  3 : 51.13233449775518\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 1.6243 - sequential_128_mse: 0.1215 - sequential_129_mse: 0.0736 - sequential_130_mse: 0.0594 - sequential_131_mse: 0.1012 - sequential_132_mse: 0.1025 - sequential_133_mse: 0.0711 - sequential_134_mse: 0.1364 - sequential_135_mse: 0.1298 - sequential_136_mse: 0.1686 - sequential_137_mse: 0.1511 - sequential_138_mse: 0.0847 - sequential_139_mse: 0.0971 - sequential_140_mse: 0.0938 - sequential_141_mse: 0.1043 - sequential_142_mse: 0.1093 - sequential_143_mse: 0.0198\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n","k: 1 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n","Test Loss no  3 : 50.85072826251928\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 1.3713 - sequential_128_mse: 0.0918 - sequential_129_mse: 0.0588 - sequential_130_mse: 0.0268 - sequential_131_mse: 0.0804 - sequential_132_mse: 0.0931 - sequential_133_mse: 0.0524 - sequential_134_mse: 0.0900 - sequential_135_mse: 0.0818 - sequential_136_mse: 0.1613 - sequential_137_mse: 0.1208 - sequential_138_mse: 0.0552 - sequential_139_mse: 0.0806 - sequential_140_mse: 0.0951 - sequential_141_mse: 0.1322 - sequential_142_mse: 0.1238 - sequential_143_mse: 0.0272\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n","k: 2 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n","Test Loss no  3 : 51.398572431779826\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 1.2183 - sequential_128_mse: 0.0871 - sequential_129_mse: 0.0642 - sequential_130_mse: 0.0345 - sequential_131_mse: 0.0610 - sequential_132_mse: 0.0681 - sequential_133_mse: 0.0441 - sequential_134_mse: 0.1003 - sequential_135_mse: 0.0976 - sequential_136_mse: 0.1440 - sequential_137_mse: 0.1368 - sequential_138_mse: 0.0443 - sequential_139_mse: 0.0679 - sequential_140_mse: 0.0548 - sequential_141_mse: 0.0948 - sequential_142_mse: 0.1009 - sequential_143_mse: 0.0179\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n","k: 3 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n","Test Loss no  3 : 53.09475641136737\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 1.1605 - sequential_128_mse: 0.0787 - sequential_129_mse: 0.0628 - sequential_130_mse: 0.0417 - sequential_131_mse: 0.0734 - sequential_132_mse: 0.0792 - sequential_133_mse: 0.0415 - sequential_134_mse: 0.0946 - sequential_135_mse: 0.0969 - sequential_136_mse: 0.1254 - sequential_137_mse: 0.1165 - sequential_138_mse: 0.0449 - sequential_139_mse: 0.0745 - sequential_140_mse: 0.0703 - sequential_141_mse: 0.0672 - sequential_142_mse: 0.0724 - sequential_143_mse: 0.0203\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n","k: 4 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n","Test Loss no  3 : 53.05832269003295\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 1.0925 - sequential_128_mse: 0.0778 - sequential_129_mse: 0.0552 - sequential_130_mse: 0.0221 - sequential_131_mse: 0.0798 - sequential_132_mse: 0.0737 - sequential_133_mse: 0.0486 - sequential_134_mse: 0.0649 - sequential_135_mse: 0.0729 - sequential_136_mse: 0.1273 - sequential_137_mse: 0.1147 - sequential_138_mse: 0.0455 - sequential_139_mse: 0.0724 - sequential_140_mse: 0.0664 - sequential_141_mse: 0.0686 - sequential_142_mse: 0.0808 - sequential_143_mse: 0.0219\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n","k: 5 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n","Test Loss no  3 : 51.177680844768105\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 0.9296 - sequential_128_mse: 0.0471 - sequential_129_mse: 0.0440 - sequential_130_mse: 0.0247 - sequential_131_mse: 0.0623 - sequential_132_mse: 0.0656 - sequential_133_mse: 0.0373 - sequential_134_mse: 0.0627 - sequential_135_mse: 0.0522 - sequential_136_mse: 0.1183 - sequential_137_mse: 0.1068 - sequential_138_mse: 0.0414 - sequential_139_mse: 0.0662 - sequential_140_mse: 0.0494 - sequential_141_mse: 0.0687 - sequential_142_mse: 0.0640 - sequential_143_mse: 0.0189\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n","k: 6 patience: 1\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n","Test Loss no  3 : 49.692488971787775\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - loss: 0.8979 - sequential_128_mse: 0.0470 - sequential_129_mse: 0.0500 - sequential_130_mse: 0.0263 - sequential_131_mse: 0.0574 - sequential_132_mse: 0.0440 - sequential_133_mse: 0.0340 - sequential_134_mse: 0.0702 - sequential_135_mse: 0.0531 - sequential_136_mse: 0.1129 - sequential_137_mse: 0.1101 - sequential_138_mse: 0.0371 - sequential_139_mse: 0.0535 - sequential_140_mse: 0.0549 - sequential_141_mse: 0.0666 - sequential_142_mse: 0.0687 - sequential_143_mse: 0.0120\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n","k: 7 patience: 2\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n","Test Loss no  3 : 49.116801718312374\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - loss: 0.9209 - sequential_128_mse: 0.0490 - sequential_129_mse: 0.0524 - sequential_130_mse: 0.0271 - sequential_131_mse: 0.0594 - sequential_132_mse: 0.0559 - sequential_133_mse: 0.0446 - sequential_134_mse: 0.0713 - sequential_135_mse: 0.0737 - sequential_136_mse: 0.1090 - sequential_137_mse: 0.1076 - sequential_138_mse: 0.0333 - sequential_139_mse: 0.0542 - sequential_140_mse: 0.0421 - sequential_141_mse: 0.0498 - sequential_142_mse: 0.0732 - sequential_143_mse: 0.0182\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n","k: 8 patience: 3\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n","Test Loss no  3 : 48.92043146005732\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 0.9135 - sequential_128_mse: 0.0579 - sequential_129_mse: 0.0402 - sequential_130_mse: 0.0250 - sequential_131_mse: 0.0482 - sequential_132_mse: 0.0566 - sequential_133_mse: 0.0426 - sequential_134_mse: 0.0712 - sequential_135_mse: 0.0653 - sequential_136_mse: 0.1069 - sequential_137_mse: 0.1016 - sequential_138_mse: 0.0452 - sequential_139_mse: 0.0604 - sequential_140_mse: 0.0405 - sequential_141_mse: 0.0564 - sequential_142_mse: 0.0811 - sequential_143_mse: 0.0143\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n","k: 9 patience: 4\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n","Test Loss no  3 : 49.618113598152895\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 0.8668 - sequential_128_mse: 0.0458 - sequential_129_mse: 0.0589 - sequential_130_mse: 0.0224 - sequential_131_mse: 0.0565 - sequential_132_mse: 0.0573 - sequential_133_mse: 0.0380 - sequential_134_mse: 0.0492 - sequential_135_mse: 0.0513 - sequential_136_mse: 0.1096 - sequential_137_mse: 0.0967 - sequential_138_mse: 0.0403 - sequential_139_mse: 0.0659 - sequential_140_mse: 0.0444 - sequential_141_mse: 0.0568 - sequential_142_mse: 0.0586 - sequential_143_mse: 0.0153\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n","k: 10 patience: 5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n","Test Loss no  3 : 50.58648818770073\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - loss: 0.7365 - sequential_128_mse: 0.0368 - sequential_129_mse: 0.0382 - sequential_130_mse: 0.0179 - sequential_131_mse: 0.0414 - sequential_132_mse: 0.0512 - sequential_133_mse: 0.0349 - sequential_134_mse: 0.0461 - sequential_135_mse: 0.0487 - sequential_136_mse: 0.0985 - sequential_137_mse: 0.0760 - sequential_138_mse: 0.0267 - sequential_139_mse: 0.0555 - sequential_140_mse: 0.0359 - sequential_141_mse: 0.0495 - sequential_142_mse: 0.0663 - sequential_143_mse: 0.0129\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n","k: 11 patience: 6\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n","Test Loss no  3 : 49.08363124561661\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - loss: 0.7197 - sequential_128_mse: 0.0349 - sequential_129_mse: 0.0341 - sequential_130_mse: 0.0213 - sequential_131_mse: 0.0378 - sequential_132_mse: 0.0432 - sequential_133_mse: 0.0335 - sequential_134_mse: 0.0441 - sequential_135_mse: 0.0552 - sequential_136_mse: 0.0787 - sequential_137_mse: 0.0792 - sequential_138_mse: 0.0314 - sequential_139_mse: 0.0575 - sequential_140_mse: 0.0443 - sequential_141_mse: 0.0430 - sequential_142_mse: 0.0664 - sequential_143_mse: 0.0149\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n","k: 12 patience: 7\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n","Test Loss no  3 : 48.22714135033336\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 0.7926 - sequential_128_mse: 0.0408 - sequential_129_mse: 0.0370 - sequential_130_mse: 0.0169 - sequential_131_mse: 0.0518 - sequential_132_mse: 0.0402 - sequential_133_mse: 0.0401 - sequential_134_mse: 0.0486 - sequential_135_mse: 0.0479 - sequential_136_mse: 0.0962 - sequential_137_mse: 0.1008 - sequential_138_mse: 0.0328 - sequential_139_mse: 0.0561 - sequential_140_mse: 0.0566 - sequential_141_mse: 0.0537 - sequential_142_mse: 0.0623 - sequential_143_mse: 0.0109\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n","k: 13 patience: 8\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n","Test Loss no  3 : 48.187616491714216\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 0.7183 - sequential_128_mse: 0.0388 - sequential_129_mse: 0.0392 - sequential_130_mse: 0.0155 - sequential_131_mse: 0.0476 - sequential_132_mse: 0.0435 - sequential_133_mse: 0.0275 - sequential_134_mse: 0.0426 - sequential_135_mse: 0.0467 - sequential_136_mse: 0.0730 - sequential_137_mse: 0.0872 - sequential_138_mse: 0.0328 - sequential_139_mse: 0.0593 - sequential_140_mse: 0.0418 - sequential_141_mse: 0.0447 - sequential_142_mse: 0.0608 - sequential_143_mse: 0.0170\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n","k: 14 patience: 9\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n","Test Loss no  3 : 48.94869766818691\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.6159 - sequential_128_mse: 0.0310 - sequential_129_mse: 0.0282 - sequential_130_mse: 0.0117 - sequential_131_mse: 0.0375 - sequential_132_mse: 0.0384 - sequential_133_mse: 0.0289 - sequential_134_mse: 0.0342 - sequential_135_mse: 0.0373 - sequential_136_mse: 0.0609 - sequential_137_mse: 0.0814 - sequential_138_mse: 0.0259 - sequential_139_mse: 0.0505 - sequential_140_mse: 0.0364 - sequential_141_mse: 0.0467 - sequential_142_mse: 0.0537 - sequential_143_mse: 0.0132\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n","k: 15 patience: 10\n","Iteration 5\n","global_indices [[13, 1, 6, 2, 3, 4, 11, 10, 5, 0, 7, 8], [10, 0, 4, 13, 8, 6, 2, 12, 11, 9, 7, 5], [6, 2, 3, 5, 12, 7, 4, 1, 0, 9, 11], [10, 0, 3, 2, 5, 4, 7, 1], [7, 13, 6, 12, 10, 5, 9, 3, 8, 4, 15, 2]]\n","input_train : (41, 120, 47)\n","output_train : (41, 16)\n","input_val : (14, 120, 47)\n","output_val : (14, 16)\n","input_test : (13, 120, 47)\n","output_test : (13, 16)\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 122ms/step - loss: 1.5181 - sequential_144_mse: 0.0994 - sequential_145_mse: 0.0826 - sequential_146_mse: 0.0463 - sequential_147_mse: 0.0734 - sequential_148_mse: 0.0902 - sequential_149_mse: 0.0874 - sequential_150_mse: 0.1666 - sequential_151_mse: 0.1109 - sequential_152_mse: 0.0537 - sequential_153_mse: 0.0405 - sequential_154_mse: 0.0540 - sequential_155_mse: 0.0979 - sequential_156_mse: 0.1855 - sequential_157_mse: 0.0559 - sequential_158_mse: 0.1044 - sequential_159_mse: 0.1694\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 501ms/step\n","k: 0 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 525ms/step\n","Test Loss no  4 : 30.19071800913216\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - loss: 1.7922 - sequential_144_mse: 0.1111 - sequential_145_mse: 0.0991 - sequential_146_mse: 0.1052 - sequential_147_mse: 0.1110 - sequential_148_mse: 0.0742 - sequential_149_mse: 0.1044 - sequential_150_mse: 0.1684 - sequential_151_mse: 0.1980 - sequential_152_mse: 0.0572 - sequential_153_mse: 0.1102 - sequential_154_mse: 0.0672 - sequential_155_mse: 0.0983 - sequential_156_mse: 0.1121 - sequential_157_mse: 0.1373 - sequential_158_mse: 0.1036 - sequential_159_mse: 0.1348\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n","k: 1 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n","Test Loss no  4 : 27.892520294366893\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 1.1242 - sequential_144_mse: 0.0756 - sequential_145_mse: 0.0669 - sequential_146_mse: 0.0440 - sequential_147_mse: 0.0775 - sequential_148_mse: 0.0546 - sequential_149_mse: 0.0581 - sequential_150_mse: 0.1224 - sequential_151_mse: 0.0915 - sequential_152_mse: 0.0407 - sequential_153_mse: 0.0331 - sequential_154_mse: 0.0513 - sequential_155_mse: 0.0665 - sequential_156_mse: 0.0825 - sequential_157_mse: 0.0598 - sequential_158_mse: 0.0946 - sequential_159_mse: 0.1050\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n","k: 2 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n","Test Loss no  4 : 25.96125071279589\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 1.1237 - sequential_144_mse: 0.0790 - sequential_145_mse: 0.0810 - sequential_146_mse: 0.0498 - sequential_147_mse: 0.0728 - sequential_148_mse: 0.0515 - sequential_149_mse: 0.0635 - sequential_150_mse: 0.0969 - sequential_151_mse: 0.0826 - sequential_152_mse: 0.0509 - sequential_153_mse: 0.0640 - sequential_154_mse: 0.0457 - sequential_155_mse: 0.0804 - sequential_156_mse: 0.0709 - sequential_157_mse: 0.0697 - sequential_158_mse: 0.0940 - sequential_159_mse: 0.0711\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n","k: 3 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n","Test Loss no  4 : 27.294105076630707\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: 1.1001 - sequential_144_mse: 0.0842 - sequential_145_mse: 0.0733 - sequential_146_mse: 0.0411 - sequential_147_mse: 0.0659 - sequential_148_mse: 0.0450 - sequential_149_mse: 0.0604 - sequential_150_mse: 0.1151 - sequential_151_mse: 0.1052 - sequential_152_mse: 0.0419 - sequential_153_mse: 0.0438 - sequential_154_mse: 0.0370 - sequential_155_mse: 0.0666 - sequential_156_mse: 0.0643 - sequential_157_mse: 0.0668 - sequential_158_mse: 0.0880 - sequential_159_mse: 0.1016\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n","k: 4 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n","Test Loss no  4 : 26.77063201046211\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 0.9583 - sequential_144_mse: 0.0714 - sequential_145_mse: 0.0739 - sequential_146_mse: 0.0329 - sequential_147_mse: 0.0545 - sequential_148_mse: 0.0444 - sequential_149_mse: 0.0519 - sequential_150_mse: 0.0915 - sequential_151_mse: 0.0829 - sequential_152_mse: 0.0399 - sequential_153_mse: 0.0391 - sequential_154_mse: 0.0381 - sequential_155_mse: 0.0647 - sequential_156_mse: 0.0604 - sequential_157_mse: 0.0493 - sequential_158_mse: 0.0827 - sequential_159_mse: 0.0808\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n","k: 5 patience: 1\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n","Test Loss no  4 : 25.7446617280801\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - loss: 0.7982 - sequential_144_mse: 0.0682 - sequential_145_mse: 0.0613 - sequential_146_mse: 0.0244 - sequential_147_mse: 0.0535 - sequential_148_mse: 0.0416 - sequential_149_mse: 0.0425 - sequential_150_mse: 0.0811 - sequential_151_mse: 0.0705 - sequential_152_mse: 0.0333 - sequential_153_mse: 0.0335 - sequential_154_mse: 0.0256 - sequential_155_mse: 0.0545 - sequential_156_mse: 0.0459 - sequential_157_mse: 0.0406 - sequential_158_mse: 0.0660 - sequential_159_mse: 0.0557\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n","k: 6 patience: 2\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n","Test Loss no  4 : 26.696240081932483\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.8548 - sequential_144_mse: 0.0605 - sequential_145_mse: 0.0626 - sequential_146_mse: 0.0320 - sequential_147_mse: 0.0674 - sequential_148_mse: 0.0464 - sequential_149_mse: 0.0526 - sequential_150_mse: 0.0709 - sequential_151_mse: 0.0695 - sequential_152_mse: 0.0335 - sequential_153_mse: 0.0331 - sequential_154_mse: 0.0259 - sequential_155_mse: 0.0664 - sequential_156_mse: 0.0542 - sequential_157_mse: 0.0445 - sequential_158_mse: 0.0604 - sequential_159_mse: 0.0750\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n","k: 7 patience: 3\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n","Test Loss no  4 : 28.819089034213917\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - loss: 0.8424 - sequential_144_mse: 0.0608 - sequential_145_mse: 0.0537 - sequential_146_mse: 0.0255 - sequential_147_mse: 0.0610 - sequential_148_mse: 0.0427 - sequential_149_mse: 0.0492 - sequential_150_mse: 0.0839 - sequential_151_mse: 0.0732 - sequential_152_mse: 0.0414 - sequential_153_mse: 0.0286 - sequential_154_mse: 0.0350 - sequential_155_mse: 0.0530 - sequential_156_mse: 0.0492 - sequential_157_mse: 0.0453 - sequential_158_mse: 0.0713 - sequential_159_mse: 0.0687\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n","k: 8 patience: 4\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n","Test Loss no  4 : 27.657371827724475\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - loss: 0.7788 - sequential_144_mse: 0.0549 - sequential_145_mse: 0.0512 - sequential_146_mse: 0.0262 - sequential_147_mse: 0.0531 - sequential_148_mse: 0.0383 - sequential_149_mse: 0.0413 - sequential_150_mse: 0.0753 - sequential_151_mse: 0.0681 - sequential_152_mse: 0.0299 - sequential_153_mse: 0.0345 - sequential_154_mse: 0.0231 - sequential_155_mse: 0.0546 - sequential_156_mse: 0.0439 - sequential_157_mse: 0.0445 - sequential_158_mse: 0.0701 - sequential_159_mse: 0.0699\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n","k: 9 patience: 5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n","Test Loss no  4 : 27.167776808104612\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.7383 - sequential_144_mse: 0.0527 - sequential_145_mse: 0.0457 - sequential_146_mse: 0.0291 - sequential_147_mse: 0.0561 - sequential_148_mse: 0.0400 - sequential_149_mse: 0.0454 - sequential_150_mse: 0.0705 - sequential_151_mse: 0.0559 - sequential_152_mse: 0.0348 - sequential_153_mse: 0.0334 - sequential_154_mse: 0.0257 - sequential_155_mse: 0.0535 - sequential_156_mse: 0.0393 - sequential_157_mse: 0.0377 - sequential_158_mse: 0.0643 - sequential_159_mse: 0.0542\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n","k: 10 patience: 6\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n","Test Loss no  4 : 27.61160349792598\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 0.7552 - sequential_144_mse: 0.0446 - sequential_145_mse: 0.0553 - sequential_146_mse: 0.0281 - sequential_147_mse: 0.0533 - sequential_148_mse: 0.0427 - sequential_149_mse: 0.0520 - sequential_150_mse: 0.0682 - sequential_151_mse: 0.0563 - sequential_152_mse: 0.0429 - sequential_153_mse: 0.0335 - sequential_154_mse: 0.0277 - sequential_155_mse: 0.0577 - sequential_156_mse: 0.0368 - sequential_157_mse: 0.0381 - sequential_158_mse: 0.0678 - sequential_159_mse: 0.0502\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n","k: 11 patience: 7\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n","Test Loss no  4 : 28.5526800966827\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 0.6056 - sequential_144_mse: 0.0380 - sequential_145_mse: 0.0394 - sequential_146_mse: 0.0206 - sequential_147_mse: 0.0430 - sequential_148_mse: 0.0327 - sequential_149_mse: 0.0286 - sequential_150_mse: 0.0555 - sequential_151_mse: 0.0519 - sequential_152_mse: 0.0332 - sequential_153_mse: 0.0268 - sequential_154_mse: 0.0220 - sequential_155_mse: 0.0500 - sequential_156_mse: 0.0302 - sequential_157_mse: 0.0394 - sequential_158_mse: 0.0516 - sequential_159_mse: 0.0427\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n","k: 12 patience: 8\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n","Test Loss no  4 : 32.30568844403162\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 0.6256 - sequential_144_mse: 0.0470 - sequential_145_mse: 0.0446 - sequential_146_mse: 0.0201 - sequential_147_mse: 0.0467 - sequential_148_mse: 0.0332 - sequential_149_mse: 0.0331 - sequential_150_mse: 0.0587 - sequential_151_mse: 0.0435 - sequential_152_mse: 0.0271 - sequential_153_mse: 0.0283 - sequential_154_mse: 0.0225 - sequential_155_mse: 0.0517 - sequential_156_mse: 0.0318 - sequential_157_mse: 0.0294 - sequential_158_mse: 0.0570 - sequential_159_mse: 0.0509\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n","k: 13 patience: 9\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n","Test Loss no  4 : 31.441272329495103\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 0.5666 - sequential_144_mse: 0.0387 - sequential_145_mse: 0.0325 - sequential_146_mse: 0.0208 - sequential_147_mse: 0.0370 - sequential_148_mse: 0.0334 - sequential_149_mse: 0.0286 - sequential_150_mse: 0.0501 - sequential_151_mse: 0.0489 - sequential_152_mse: 0.0265 - sequential_153_mse: 0.0275 - sequential_154_mse: 0.0186 - sequential_155_mse: 0.0507 - sequential_156_mse: 0.0373 - sequential_157_mse: 0.0314 - sequential_158_mse: 0.0373 - sequential_159_mse: 0.0474\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n","k: 14 patience: 10\n","average_loss: 29.627488933833863\n"]}]}]}