{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOVOTEtUs3vhdG/qe6xrFWk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":14,"metadata":{"id":"fT_A9oAGAepC","executionInfo":{"status":"ok","timestamp":1724915223320,"user_tz":300,"elapsed":177,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"outputs":[],"source":["from google.colab import drive\n","import pandas as pd\n","from sklearn.preprocessing import StandardScaler\n","from scipy import stats\n","import numpy as np\n","import logging\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense\n","from datetime import datetime\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import datetime\n","import matplotlib.dates as mdates\n","import os"]},{"cell_type":"code","source":["from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Set the base path to the desired directory on Google Drive\n","base_path = '/content/drive/MyDrive/Study_1_Data/'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TishlaEBAmlN","executionInfo":{"status":"ok","timestamp":1724915224382,"user_tz":300,"elapsed":902,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}},"outputId":"c80cc87d-33c0-49f5-a748-3742e44f7bb8"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["def read_csv(file_path):\n","    data = pd.read_csv(file_path)\n","    return data"],"metadata":{"id":"XeSwJ8oKAt2r","executionInfo":{"status":"ok","timestamp":1724915224382,"user_tz":300,"elapsed":8,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["def process_data(data, columns_to_remove):\n","    processed_data = data.drop(columns=columns_to_remove).values\n","    return processed_data"],"metadata":{"id":"BFHyHoFvA5bX","executionInfo":{"status":"ok","timestamp":1724915224382,"user_tz":300,"elapsed":7,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["def construct_3d_array(base_dir, participants, simulations, columns_to_remove_hr, columns_to_remove_gsr, columns_to_remove_head, columns_to_remove_eye):\n","    \"\"\"\n","    Construct 3D array from CSV files.\n","    \"\"\"\n","    num_rows = 180  # Define number of rows to keep (last 180 rows)\n","    arrays_3d = []\n","\n","    for participant in participants:\n","        participant_id = f\"{int(participant):02d}\"  # Format participant number to two digits\n","\n","        valid_simulations = []\n","\n","        for simulation in simulations:\n","            hr_file_path = os.path.join(base_dir, participant_id, simulation, f'HR{simulation.capitalize()}.csv')\n","            gsr_file_path = os.path.join(base_dir, participant_id, simulation, f'EDA{simulation.capitalize()}_downsampled.csv')\n","            head_file_path = os.path.join(base_dir, participant_id, simulation, 'head_tracking_downsampled.csv')\n","            eye_file_path = os.path.join(base_dir, participant_id, simulation, 'eye_tracking_downsampled.csv')\n","\n","            # Check if all files exist\n","            if all(os.path.exists(file) for file in [hr_file_path, gsr_file_path, head_file_path, eye_file_path]):\n","                valid_simulations.append(simulation)\n","\n","        num_valid_simulations = len(valid_simulations)\n","        if num_valid_simulations == 0:\n","            continue  # Skip this participant if no valid simulations are found\n","\n","        array_3d = np.zeros((num_valid_simulations, num_rows, 47)) # hr=1, gsr=1, head=15-3, eye=41-8 total columns after removing columns= 48\n","\n","        for s_idx, simulation in enumerate(valid_simulations):\n","            # Process hr data\n","            hr_file_path = os.path.join(base_dir, participant_id, simulation, f'HR{simulation.capitalize()}.csv')\n","            hr_data = read_csv(hr_file_path)\n","            processed_hr_data = process_data(hr_data, columns_to_remove_hr)\n","            processed_hr_data = processed_hr_data[-num_rows:]  # Keep only the last 180 rows\n","\n","            # Process gsr data\n","            gsr_file_path = os.path.join(base_dir, participant_id, simulation, f'EDA{simulation.capitalize()}_downsampled.csv')\n","            gsr_data = read_csv(gsr_file_path)\n","            processed_gsr_data = process_data(gsr_data, columns_to_remove_gsr)\n","            processed_gsr_data = processed_gsr_data[-num_rows:]  # Keep only the last 180 rows\n","\n","            # Process head data\n","            head_file_path = os.path.join(base_dir, participant_id, simulation, 'head_tracking_downsampled.csv')\n","            head_data = read_csv(head_file_path)\n","            processed_head_data = process_data(head_data, columns_to_remove_head)\n","            processed_head_data = processed_head_data[-num_rows:]  # Keep only the last 180 rows\n","\n","            # Process eye data\n","            eye_file_path = os.path.join(base_dir, participant_id, simulation, 'eye_tracking_downsampled.csv')\n","            eye_data = read_csv(eye_file_path)\n","            processed_eye_data = process_data(eye_data, columns_to_remove_eye)\n","            processed_eye_data = processed_eye_data[-num_rows:]  # Keep only the last 180 rows\n","\n","            # Combine processed data\n","            combined_data = np.concatenate((processed_hr_data, processed_gsr_data, processed_head_data, processed_eye_data), axis=1)\n","\n","            array_3d[s_idx, :, :] = combined_data\n","\n","        arrays_3d.append(array_3d)\n","\n","    return arrays_3d\n"],"metadata":{"id":"QeoIWcudA94b","executionInfo":{"status":"ok","timestamp":1724915224383,"user_tz":300,"elapsed":8,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["sample_size=60\n","simulations = ['flat','noise','bumps']\n","participants = [str(i) for i in range(1, 27)]  # Participants 101 to 127\n","columns_to_remove_hr = []\n","columns_to_remove_gsr = []\n","columns_to_remove_eye = ['#Frame','Time', 'Unnamed: 40','ConvergenceValid','Left_Eye_Closed','Right_Eye_Closed','LocalGazeValid','WorldGazeValid']\n","columns_to_remove_head = ['#Frame','Time', 'Unnamed: 14']"],"metadata":{"id":"fqaeUGUDBCtT","executionInfo":{"status":"ok","timestamp":1724915224383,"user_tz":300,"elapsed":7,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["def calculate_total_ssq(csv_file):\n","    # Read the CSV file into a DataFrame\n","    df = pd.read_csv(csv_file)\n","    n_columns = [0, 5, 6, 7, 8, 14, 15]\n","    o_columns = [0, 1, 2, 3, 4, 8, 10]\n","    d_columns = [4, 7, 9, 10, 11, 12, 13]\n","\n","    # Calculate sum for each specified set of columns\n","    n_val = df.iloc[0, n_columns].sum()\n","    o_val = df.iloc[0, o_columns].sum()\n","    d_val = df.iloc[0, d_columns].sum()\n","\n","    return n_val, o_val, d_val"],"metadata":{"id":"vPQyeAYKBYaO","executionInfo":{"status":"ok","timestamp":1724915224383,"user_tz":300,"elapsed":7,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["def merge_ssq_column(conditions,participants):\n","  directories = []\n","  total_ssq_values = []\n","  for participant in participants:\n","      participant = f\"{int(participant):02d}\"\n","      for condition in conditions:\n","          directory = os.path.join(base_path, participant, condition)\n","          directories.append(directory)\n","\n","  # Loop through each directory\n","  for directory in directories:\n","      # Check if the directory exists\n","      if not os.path.exists(directory):\n","          continue\n","\n","      # Get all CSV files in the directory that are named 'ssq.csv'\n","      csv_files = [file for file in os.listdir(directory) if file == 'ssq.csv']\n","\n","      # Loop through each CSV file\n","      for csv_file in csv_files:\n","          csv_path = os.path.join(directory, csv_file)\n","          df = pd.read_csv(csv_path)\n","          # n_val,o_val,d_val = calculate_total_ssq(csv_path)\n","          # total_ssq_values.append([n_val, o_val, d_val])\n","          ssq_values_participant = df.iloc[:, 0:17].values.flatten()   # Assuming SSQ values are in columns 1 to 16\n","          total_ssq_values.append(ssq_values_participant)\n","  ssq_array = np.array(total_ssq_values)\n","  return ssq_array\n","\n","def merge_total_ssq(conditions,participants):\n","  directories = []\n","  total_ssq_values = []\n","  for participant in participants:\n","      participant = f\"{int(participant):02d}\"\n","      for condition in conditions:\n","          directory = os.path.join(base_path, participant, condition)\n","          directories.append(directory)\n","\n","  # Loop through each directory\n","  for directory in directories:\n","      # Check if the directory exists\n","      if not os.path.exists(directory):\n","          continue\n","\n","      # Get all CSV files in the directory that are named 'ssq.csv'\n","      csv_files = [file for file in os.listdir(directory) if file == 'ssq.csv']\n","\n","      # Loop through each CSV file\n","      for csv_file in csv_files:\n","          csv_path = os.path.join(directory, csv_file)\n","          n_val,o_val,d_val = calculate_total_ssq(csv_path)\n","          total_ssq = (n_val+o_val+d_val) * 3.74\n","          df = pd.read_csv(csv_path)\n","          df[\"total-ssq\"] = total_ssq\n","          #print(\"csv_path: \",csv_path,\"   \",total_ssq)\n","          total_ssq_values.append(total_ssq)\n","  # Create a DataFrame from the list of total SSQ values\n","  df_total_ssq = pd.DataFrame(total_ssq_values, columns=[\"total-ssq\"])\n","  # Convert the list of total SSQ values to a NumPy array\n","  total_ssq_array = np.array(total_ssq_values)\n","  return total_ssq_array\n","\n"],"metadata":{"id":"Xpn0lDt0BfvE","executionInfo":{"status":"ok","timestamp":1724915224383,"user_tz":300,"elapsed":6,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["participants_group_1 = [1,3,4,11,25]\n","participants_group_2 = [2,7,8,9,17]\n","participants_group_3 = [10,12,13,22,23]\n","participants_group_4 = [5,14,18,20,21]\n","participants_group_5 = [6,15,16,19,24,26]\n","\n","arrays_group_1 = construct_3d_array(base_path, participants_group_1, simulations, columns_to_remove_hr, columns_to_remove_gsr, columns_to_remove_head, columns_to_remove_eye)\n","arrays_group_2 = construct_3d_array(base_path, participants_group_2, simulations, columns_to_remove_hr, columns_to_remove_gsr, columns_to_remove_head, columns_to_remove_eye)\n","arrays_group_3 = construct_3d_array(base_path, participants_group_3, simulations, columns_to_remove_hr, columns_to_remove_gsr, columns_to_remove_head, columns_to_remove_eye)\n","arrays_group_4 = construct_3d_array(base_path, participants_group_4, simulations, columns_to_remove_hr, columns_to_remove_gsr, columns_to_remove_head, columns_to_remove_eye)\n","arrays_group_5 = construct_3d_array(base_path, participants_group_5, simulations, columns_to_remove_hr, columns_to_remove_gsr, columns_to_remove_head, columns_to_remove_eye)"],"metadata":{"id":"7k17K0HrCr6-","executionInfo":{"status":"ok","timestamp":1724915227116,"user_tz":300,"elapsed":2738,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["# Concatenate arrays along the first axis\n","input_group_1 = np.concatenate(arrays_group_1, axis=0)\n","input_group_2 = np.concatenate(arrays_group_2, axis=0)\n","input_group_3 = np.concatenate(arrays_group_3, axis=0)\n","input_group_4 = np.concatenate(arrays_group_4, axis=0)\n","input_group_5 = np.concatenate(arrays_group_5, axis=0)\n"],"metadata":{"id":"rRIv2QVJmygH","executionInfo":{"status":"ok","timestamp":1724915227117,"user_tz":300,"elapsed":10,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["output_group_1=merge_ssq_column(simulations,participants_group_1)\n","output_group_2=merge_ssq_column(simulations,participants_group_2)\n","output_group_3=merge_ssq_column(simulations,participants_group_3)\n","output_group_4=merge_ssq_column(simulations,participants_group_4)\n","output_group_5=merge_ssq_column(simulations,participants_group_5)\n","\n","output_group_1 = np.squeeze(output_group_1)\n","output_group_2 = np.squeeze(output_group_2)\n","output_group_3 = np.squeeze(output_group_3)\n","output_group_4 = np.squeeze(output_group_4)\n","output_group_5 = np.squeeze(output_group_5)\n","\n","\n","output_total_ssq_group_1=merge_total_ssq(simulations,participants_group_1)\n","output_total_ssq_group_2=merge_total_ssq(simulations,participants_group_2)\n","output_total_ssq_group_3=merge_total_ssq(simulations,participants_group_3)\n","output_total_ssq_group_4=merge_total_ssq(simulations,participants_group_4)\n","output_total_ssq_group_5=merge_total_ssq(simulations,participants_group_5)\n","\n","output_total_ssq_group_1=output_total_ssq_group_1.reshape(-1, 1)\n","output_total_ssq_group_2=output_total_ssq_group_2.reshape(-1, 1)\n","output_total_ssq_group_3=output_total_ssq_group_3.reshape(-1, 1)\n","output_total_ssq_group_4=output_total_ssq_group_4.reshape(-1, 1)\n","output_total_ssq_group_5=output_total_ssq_group_5.reshape(-1, 1)\n","\n"],"metadata":{"id":"Giawelxqm4DE","executionInfo":{"status":"ok","timestamp":1724915232180,"user_tz":300,"elapsed":5070,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["def scale_input_data(input_train, input_test):\n","    # Get the shape of the input data\n","    num_samples_train, time_steps_train, num_features = input_train.shape\n","    num_samples_test, time_steps_test, _ = input_test.shape\n","\n","    # Reshape the input data into 2D arrays\n","    flattened_train_data = input_train.reshape(-1, num_features)\n","    flattened_test_data = input_test.reshape(-1, num_features)\n","\n","    # Initialize a MinMaxScaler object\n","    scaler = MinMaxScaler()\n","\n","    # Fit the scaler on the training data and transform both train and test data\n","    scaled_train_data = scaler.fit_transform(flattened_train_data)\n","    scaled_test_data = scaler.transform(flattened_test_data)\n","\n","    # Reshape the scaled data back to its original shape\n","    scaled_train_data = scaled_train_data.reshape(num_samples_train, time_steps_train, num_features)\n","    scaled_test_data = scaled_test_data.reshape(num_samples_test, time_steps_test, num_features)\n","\n","    return scaled_train_data, scaled_test_data\n","\n","def scale_target_var(target_data):\n","    min_val, max_val = np.min(target_data, axis=0), np.max(target_data, axis=0)\n","    target_data = (target_data-min_val)/(max_val-min_val)\n","\n","    return target_data, min_val, max_val"],"metadata":{"id":"26ADF-kiC1EZ","executionInfo":{"status":"ok","timestamp":1724915232181,"user_tz":300,"elapsed":13,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["from keras.models import Sequential\n","from keras.layers import Input, GRU, Dense, Dropout\n","from keras.models import Model\n","import numpy as np\n","import sklearn\n","\n","total_losses=[]\n","def get_shared_lstm(input_shape1, input_shape2):\n","    # Define shared GRU model\n","    input_layer = Input(shape=(input_shape1, input_shape2))\n","    x = LSTM(64, return_sequences=False)(input_layer)\n","    x = Dense(256, activation='relu')(x)\n","    x = Dropout(0.2)(x)\n","    shared_model = Model(inputs=input_layer, outputs=x)\n","    return shared_model\n","\n","\n","\n","\n","def get_output_model(shared_lstm_output, output_shape):\n","    # Define separate output model for each column\n","    output_models = []\n","    for i in range(output_shape[1]):\n","        output_model = Sequential()\n","        output_model.add(Dense(256, activation='relu'))\n","        output_model.add(Dropout(0.2))\n","        output_model.add(Dense(1))  # Output shape is (None, 1) for each column\n","        output_model_output = output_model(shared_lstm_output)\n","        output_models.append(output_model_output)\n","    return output_models\n","input_groups = [input_group_1, input_group_2, input_group_3, input_group_4, input_group_5]\n","output_groups = [output_group_1, output_group_2, output_group_3, output_group_4, output_group_5]\n","ssq_groups = [output_total_ssq_group_1, output_total_ssq_group_2, output_total_ssq_group_3, output_total_ssq_group_4, output_total_ssq_group_5]\n","\n","\n","# Specify the number of samples to select for each group in each iteration\n","samples_per_iteration = [\n","    [3, 3, 3, 3, 2],  # For input_group_1\n","    [3, 3, 3, 3, 2],  # For input_group_2\n","    [3, 3, 3, 2, 2],  # For input_group_3\n","    [2, 2, 2, 2, 3],  # For input_group_4\n","    [3, 3, 3, 3, 4]   # For input_group_5\n","]\n","\n","# Initialize a list of global indices arrays, one for each group\n","global_indices = [[] for _ in range(len(input_groups))]\n","print(\"global_indices\",global_indices)\n","\n","# Outer loop to repeat the sampling process for 5 iterations\n","for iteration in range(5):\n","  X_train, X_val, X_test = [], [], []\n","  y_train, y_val, y_test = [], [], []\n","  ssq_train, ssq_val, ssq_test = [], [], []\n","  print(f\"Iteration {iteration + 1}\")\n","  print(\"global_indices\",global_indices)\n","  # Loop over each group\n","  for i, (input_group, output_group, ssq_group) in enumerate(zip(input_groups, output_groups, ssq_groups)):\n","      num_samples = samples_per_iteration[i][iteration]  # Number of samples to select for the current group and iteration\n","\n","      # Create a set of available indices that haven't been selected yet for the current group\n","      available_indices = list(set(range(len(input_group))) - set(global_indices[i]))\n","\n","      # Check if there are fewer available indices than needed\n","      if len(available_indices) < num_samples:\n","          print(f\"Not enough indices left in group {i + 1} to select {num_samples} new samples.\")\n","          num_samples = len(available_indices)  # Adjust to take whatever is left\n","\n","      # Select the required number of samples from the available indices for the current group\n","      selected_indices = np.random.choice(available_indices, num_samples, replace=False)\n","      global_indices[i].extend(selected_indices)  # Add these indices to the group's global list\n","\n","      # Remove these selected samples from the input, output, and SSQ groups\n","      X_test_temp = input_group[selected_indices]\n","      y_test_temp = output_group[selected_indices]\n","      ssq_test_temp = ssq_group[selected_indices]\n","\n","      X_temp = np.delete(input_group, selected_indices, axis=0)\n","      y_temp = np.delete(output_group, selected_indices, axis=0)\n","      ssq_temp = np.delete(ssq_group, selected_indices, axis=0)\n","\n","      # Split the remaining data into a training set (60%) and a validation set (40%)\n","      X_train_temp, X_val_temp, y_train_temp, y_val_temp, ssq_train_temp, ssq_val_temp = train_test_split(\n","          X_temp, y_temp, ssq_temp, test_size=0.2)\n","\n","      # Append the results to the corresponding lists\n","      X_train.append(X_train_temp)\n","      X_val.append(X_val_temp)\n","      X_test.append(X_test_temp)\n","\n","      y_train.append(y_train_temp)\n","      y_val.append(y_val_temp)\n","      y_test.append(y_test_temp)\n","\n","      ssq_train.append(ssq_train_temp)\n","      ssq_val.append(ssq_val_temp)\n","      ssq_test.append(ssq_test_temp)\n","\n","  # After the loop, concatenate the data for all groups if needed\n","  input_train = np.concatenate(X_train, axis=0)\n","  input_val = np.concatenate(X_val, axis=0)\n","  input_test = np.concatenate(X_test, axis=0)\n","\n","  output_train = np.concatenate(y_train, axis=0)\n","  output_val = np.concatenate(y_val, axis=0)\n","  output_test = np.concatenate(y_test, axis=0)\n","\n","  output_test_total_ssq = np.concatenate(ssq_test, axis=0)\n","\n","\n","  #  this section for scaling both train and validation set simultaniously\n","  # Step 1: Combine the training and validation sets\n","  combined_input = np.concatenate([input_train, input_val], axis=0)\n","  combined_output = np.concatenate([output_train, output_val], axis=0)\n","\n","  # Step 2: Scale the combined input data\n","  # Assuming scale_input_data scales the data based on the combined dataset\n","  combined_input, input_test = scale_input_data(\n","      combined_input[:, (60-sample_size):(180-sample_size), :],\n","      input_test[:, (60-sample_size):(180-sample_size), :]\n","  )\n","\n","  # Step 3: Scale the combined output data\n","  # Assuming scale_target_var scales the data and returns min_val, max_val\n","  combined_output, min_val, max_val = scale_target_var(combined_output)\n","\n","  # Step 4: Split the combined data back into training and validation sets\n","  # Use the original shapes of input_train and input_val to slice the combined arrays\n","  input_train = combined_input[:input_train.shape[0], :, :]\n","  input_val = combined_input[input_train.shape[0]:, :, :]\n","\n","  output_train = combined_output[:output_train.shape[0], :]\n","  output_val = combined_output[output_train.shape[0]:, :]\n","\n","\n","\n","  print(\"input_train :\", input_train.shape)\n","  print(\"output_train :\", output_train.shape)\n","  print(\"input_val :\", input_val.shape)\n","  print(\"output_val :\", output_val.shape)\n","  print(\"input_test :\", input_test.shape)\n","  print(\"output_test :\", output_test.shape)\n","\n","\n","\n","\n","\n","\n","  # Reshape train and test inputs to match GRU input shape\n","  train_input_reshaped = input_train.reshape((input_train.shape[0], input_train.shape[1], input_train.shape[2]))\n","  test_input_reshaped = input_test.reshape((input_test.shape[0], input_test.shape[1], input_test.shape[2]))\n","  val_input_reshaped = input_val.reshape((input_val.shape[0], input_val.shape[1], input_val.shape[2]))\n","  # Get shared LSTM model\n","  shared_lstm = get_shared_lstm(input_train.shape[1], input_train.shape[2])\n","\n","  # Create separate output models for each column\n","  output_models = get_output_model(shared_lstm.output, output_train.shape)\n","\n","  # Create combined model\n","  model = Model(inputs=shared_lstm.input, outputs=output_models)\n","\n","  # Compile and train the model\n","  model.compile(loss='mse', optimizer='adam', metrics=[['mse'] for _ in range(output_train.shape[1])])  # Using MSE as loss and metric\n","  best_val=1000000\n","  patience=0\n","  best_model = None\n","\n","  for k in range(200):\n","      # Predict test data\n","      model.fit(train_input_reshaped, [output_train[:, i] for i in range(output_train.shape[1])], epochs=1, batch_size=32)\n","      pred_val = np.array(model.predict(val_input_reshaped))\n","      pred_val = np.transpose(pred_val.squeeze(), (1, 0))\n","      print(\"k:\", k, \"patience:\", patience)\n","      # Evaluate the model\n","      losses = []\n","      for i in range(pred_val.shape[0]):\n","        total_ssq=0\n","        for j in [0,5,6,7,8,14,15]:\n","          total_ssq=np.sum(pred_val[i,j]*(max_val[j]-min_val[j]) + min_val[j])+total_ssq\n","\n","        for j in [0,1,2,3,4,8,10]:\n","          total_ssq=np.sum(pred_val[i,j]*(max_val[j]-min_val[j]) + min_val[j])+total_ssq\n","\n","        for j in [4,7,9,10,11,12,13]:\n","          total_ssq=np.sum(pred_val[i,j]*(max_val[j]-min_val[j]) + min_val[j])+total_ssq\n","        total_ssq=total_ssq*3.74\n","        output_val_ssq= output_val[i,0]\n","        #print(\"total_ssq\",total_ssq)\n","        #print(\"output_val_ssq\",output_val_ssq)\n","        loss = sklearn.metrics.mean_squared_error([total_ssq], [output_val_ssq], squared=False)\n","        losses.append(loss)\n","      tmp_val_loss = np.mean(losses)\n","      if tmp_val_loss <= best_val:\n","          best_val = tmp_val_loss\n","          patience = 0\n","          best_model = model\n","      else:\n","          patience +=1\n","          if patience > 10:\n","            break\n","\n","      # Predict test data\n","      pred_test = np.array(best_model.predict(test_input_reshaped))\n","      pred_test = np.transpose(pred_test.squeeze(), (1, 0))\n","      # Evaluate the model\n","      pred_total_ssq = []\n","      #losses=[]\n","      for i in range(pred_test.shape[0]):\n","          total_ssq=0\n","          for j in [0,5,6,7,8,14,15]:\n","            total_ssq=np.sum(pred_test[i,j]*(max_val[j]-min_val[j]) + min_val[j])+total_ssq\n","\n","          for j in [0,1,2,3,4,8,10]:\n","            total_ssq=np.sum(pred_test[i,j]*(max_val[j]-min_val[j]) + min_val[j])+total_ssq\n","\n","          for j in [4,7,9,10,11,12,13]:\n","            total_ssq=np.sum(pred_test[i,j]*(max_val[j]-min_val[j]) + min_val[j])+total_ssq\n","          total_ssq=total_ssq*3.74\n","\n","          pred_total_ssq.append(total_ssq)\n","\n","\n","      # Overall Test Loss\n","      loss = sklearn.metrics.mean_squared_error(pred_total_ssq, output_test_total_ssq, squared = False)\n","      print(\"Test Loss no \",iteration,\":\" ,loss)\n","      total_losses.append(loss)\n","\n","average_loss = sum(total_losses) / len(total_losses)\n","total_losses.append(average_loss)\n","print(\"average_loss:\", average_loss)"],"metadata":{"id":"E6ssyYUeDJwI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724915404269,"user_tz":300,"elapsed":172099,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}},"outputId":"097503b8-d741-4fb4-8461-edb9f187ddee"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["global_indices [[], [], [], [], []]\n","Iteration 1\n","global_indices [[], [], [], [], []]\n","input_train : (41, 120, 47)\n","output_train : (41, 16)\n","input_val : (13, 120, 47)\n","output_val : (13, 16)\n","input_test : (14, 120, 47)\n","output_test : (14, 16)\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 87ms/step - loss: 1.5336 - sequential_80_mse: 0.1683 - sequential_81_mse: 0.1997 - sequential_82_mse: 0.0619 - sequential_83_mse: 0.1037 - sequential_84_mse: 0.0321 - sequential_85_mse: 0.0718 - sequential_86_mse: 0.1385 - sequential_87_mse: 0.1844 - sequential_88_mse: 0.0450 - sequential_89_mse: 0.0536 - sequential_90_mse: 0.0422 - sequential_91_mse: 0.0855 - sequential_92_mse: 0.0992 - sequential_93_mse: 0.0902 - sequential_94_mse: 0.1120 - sequential_95_mse: 0.0457\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 522ms/step\n","k: 0 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 508ms/step\n","Test Loss no  0 : 26.03496664470233\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 1.1032 - sequential_80_mse: 0.1087 - sequential_81_mse: 0.0780 - sequential_82_mse: 0.0608 - sequential_83_mse: 0.1178 - sequential_84_mse: 0.0227 - sequential_85_mse: 0.0610 - sequential_86_mse: 0.1169 - sequential_87_mse: 0.0967 - sequential_88_mse: 0.0247 - sequential_89_mse: 0.0377 - sequential_90_mse: 0.0233 - sequential_91_mse: 0.0900 - sequential_92_mse: 0.0402 - sequential_93_mse: 0.0611 - sequential_94_mse: 0.1057 - sequential_95_mse: 0.0579\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n","k: 1 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n","Test Loss no  0 : 24.495882833294306\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.9338 - sequential_80_mse: 0.0880 - sequential_81_mse: 0.0721 - sequential_82_mse: 0.0483 - sequential_83_mse: 0.0771 - sequential_84_mse: 0.0265 - sequential_85_mse: 0.0575 - sequential_86_mse: 0.0952 - sequential_87_mse: 0.0988 - sequential_88_mse: 0.0181 - sequential_89_mse: 0.0405 - sequential_90_mse: 0.0234 - sequential_91_mse: 0.0717 - sequential_92_mse: 0.0340 - sequential_93_mse: 0.0462 - sequential_94_mse: 0.0864 - sequential_95_mse: 0.0500\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n","k: 2 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n","Test Loss no  0 : 21.47902099515229\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.8476 - sequential_80_mse: 0.0836 - sequential_81_mse: 0.0620 - sequential_82_mse: 0.0597 - sequential_83_mse: 0.0794 - sequential_84_mse: 0.0251 - sequential_85_mse: 0.0484 - sequential_86_mse: 0.0894 - sequential_87_mse: 0.0823 - sequential_88_mse: 0.0209 - sequential_89_mse: 0.0276 - sequential_90_mse: 0.0168 - sequential_91_mse: 0.0628 - sequential_92_mse: 0.0276 - sequential_93_mse: 0.0343 - sequential_94_mse: 0.0812 - sequential_95_mse: 0.0464\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n","k: 3 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n","Test Loss no  0 : 21.19312771335138\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 0.8498 - sequential_80_mse: 0.0926 - sequential_81_mse: 0.0608 - sequential_82_mse: 0.0477 - sequential_83_mse: 0.0820 - sequential_84_mse: 0.0180 - sequential_85_mse: 0.0500 - sequential_86_mse: 0.1006 - sequential_87_mse: 0.0904 - sequential_88_mse: 0.0187 - sequential_89_mse: 0.0297 - sequential_90_mse: 0.0153 - sequential_91_mse: 0.0501 - sequential_92_mse: 0.0294 - sequential_93_mse: 0.0357 - sequential_94_mse: 0.0920 - sequential_95_mse: 0.0370\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n","k: 4 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n","Test Loss no  0 : 21.25287799956685\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 0.7464 - sequential_80_mse: 0.0699 - sequential_81_mse: 0.0563 - sequential_82_mse: 0.0522 - sequential_83_mse: 0.0735 - sequential_84_mse: 0.0171 - sequential_85_mse: 0.0427 - sequential_86_mse: 0.0745 - sequential_87_mse: 0.0638 - sequential_88_mse: 0.0186 - sequential_89_mse: 0.0277 - sequential_90_mse: 0.0191 - sequential_91_mse: 0.0607 - sequential_92_mse: 0.0201 - sequential_93_mse: 0.0340 - sequential_94_mse: 0.0807 - sequential_95_mse: 0.0356\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n","k: 5 patience: 1\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n","Test Loss no  0 : 22.918731494969723\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 0.7656 - sequential_80_mse: 0.0711 - sequential_81_mse: 0.0498 - sequential_82_mse: 0.0490 - sequential_83_mse: 0.0727 - sequential_84_mse: 0.0175 - sequential_85_mse: 0.0473 - sequential_86_mse: 0.0720 - sequential_87_mse: 0.0787 - sequential_88_mse: 0.0148 - sequential_89_mse: 0.0326 - sequential_90_mse: 0.0197 - sequential_91_mse: 0.0570 - sequential_92_mse: 0.0258 - sequential_93_mse: 0.0277 - sequential_94_mse: 0.0872 - sequential_95_mse: 0.0428\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n","k: 6 patience: 2\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n","Test Loss no  0 : 25.46169410729938\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.7535 - sequential_80_mse: 0.0684 - sequential_81_mse: 0.0487 - sequential_82_mse: 0.0419 - sequential_83_mse: 0.0644 - sequential_84_mse: 0.0164 - sequential_85_mse: 0.0459 - sequential_86_mse: 0.0743 - sequential_87_mse: 0.0767 - sequential_88_mse: 0.0167 - sequential_89_mse: 0.0302 - sequential_90_mse: 0.0190 - sequential_91_mse: 0.0631 - sequential_92_mse: 0.0281 - sequential_93_mse: 0.0341 - sequential_94_mse: 0.0785 - sequential_95_mse: 0.0470\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n","k: 7 patience: 3\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n","Test Loss no  0 : 24.14909862903084\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.6900 - sequential_80_mse: 0.0627 - sequential_81_mse: 0.0480 - sequential_82_mse: 0.0503 - sequential_83_mse: 0.0649 - sequential_84_mse: 0.0152 - sequential_85_mse: 0.0430 - sequential_86_mse: 0.0653 - sequential_87_mse: 0.0671 - sequential_88_mse: 0.0159 - sequential_89_mse: 0.0307 - sequential_90_mse: 0.0153 - sequential_91_mse: 0.0500 - sequential_92_mse: 0.0245 - sequential_93_mse: 0.0295 - sequential_94_mse: 0.0757 - sequential_95_mse: 0.0319\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n","k: 8 patience: 4\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n","Test Loss no  0 : 21.725212384738978\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.6695 - sequential_80_mse: 0.0651 - sequential_81_mse: 0.0438 - sequential_82_mse: 0.0437 - sequential_83_mse: 0.0666 - sequential_84_mse: 0.0144 - sequential_85_mse: 0.0417 - sequential_86_mse: 0.0633 - sequential_87_mse: 0.0642 - sequential_88_mse: 0.0129 - sequential_89_mse: 0.0303 - sequential_90_mse: 0.0145 - sequential_91_mse: 0.0489 - sequential_92_mse: 0.0226 - sequential_93_mse: 0.0224 - sequential_94_mse: 0.0727 - sequential_95_mse: 0.0425\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n","k: 9 patience: 5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n","Test Loss no  0 : 21.75266437666858\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.6298 - sequential_80_mse: 0.0537 - sequential_81_mse: 0.0431 - sequential_82_mse: 0.0456 - sequential_83_mse: 0.0604 - sequential_84_mse: 0.0156 - sequential_85_mse: 0.0396 - sequential_86_mse: 0.0588 - sequential_87_mse: 0.0528 - sequential_88_mse: 0.0144 - sequential_89_mse: 0.0295 - sequential_90_mse: 0.0149 - sequential_91_mse: 0.0521 - sequential_92_mse: 0.0217 - sequential_93_mse: 0.0271 - sequential_94_mse: 0.0712 - sequential_95_mse: 0.0293\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n","k: 10 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n","Test Loss no  0 : 22.602865993694653\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.5929 - sequential_80_mse: 0.0534 - sequential_81_mse: 0.0438 - sequential_82_mse: 0.0416 - sequential_83_mse: 0.0536 - sequential_84_mse: 0.0120 - sequential_85_mse: 0.0383 - sequential_86_mse: 0.0635 - sequential_87_mse: 0.0497 - sequential_88_mse: 0.0136 - sequential_89_mse: 0.0276 - sequential_90_mse: 0.0106 - sequential_91_mse: 0.0415 - sequential_92_mse: 0.0208 - sequential_93_mse: 0.0238 - sequential_94_mse: 0.0689 - sequential_95_mse: 0.0302\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n","k: 11 patience: 1\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n","Test Loss no  0 : 23.893715995962765\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.6460 - sequential_80_mse: 0.0617 - sequential_81_mse: 0.0396 - sequential_82_mse: 0.0462 - sequential_83_mse: 0.0563 - sequential_84_mse: 0.0136 - sequential_85_mse: 0.0408 - sequential_86_mse: 0.0653 - sequential_87_mse: 0.0661 - sequential_88_mse: 0.0148 - sequential_89_mse: 0.0277 - sequential_90_mse: 0.0117 - sequential_91_mse: 0.0488 - sequential_92_mse: 0.0183 - sequential_93_mse: 0.0280 - sequential_94_mse: 0.0787 - sequential_95_mse: 0.0283\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n","k: 12 patience: 2\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n","Test Loss no  0 : 22.464056694781277\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.5239 - sequential_80_mse: 0.0463 - sequential_81_mse: 0.0345 - sequential_82_mse: 0.0328 - sequential_83_mse: 0.0580 - sequential_84_mse: 0.0141 - sequential_85_mse: 0.0274 - sequential_86_mse: 0.0543 - sequential_87_mse: 0.0459 - sequential_88_mse: 0.0122 - sequential_89_mse: 0.0252 - sequential_90_mse: 0.0107 - sequential_91_mse: 0.0460 - sequential_92_mse: 0.0155 - sequential_93_mse: 0.0229 - sequential_94_mse: 0.0507 - sequential_95_mse: 0.0273\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n","k: 13 patience: 3\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n","Test Loss no  0 : 22.201906207010413\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.5179 - sequential_80_mse: 0.0444 - sequential_81_mse: 0.0348 - sequential_82_mse: 0.0284 - sequential_83_mse: 0.0439 - sequential_84_mse: 0.0123 - sequential_85_mse: 0.0321 - sequential_86_mse: 0.0598 - sequential_87_mse: 0.0398 - sequential_88_mse: 0.0133 - sequential_89_mse: 0.0265 - sequential_90_mse: 0.0104 - sequential_91_mse: 0.0418 - sequential_92_mse: 0.0180 - sequential_93_mse: 0.0227 - sequential_94_mse: 0.0645 - sequential_95_mse: 0.0255\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n","k: 14 patience: 4\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n","Test Loss no  0 : 24.618043443238278\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.5472 - sequential_80_mse: 0.0506 - sequential_81_mse: 0.0409 - sequential_82_mse: 0.0385 - sequential_83_mse: 0.0524 - sequential_84_mse: 0.0132 - sequential_85_mse: 0.0341 - sequential_86_mse: 0.0468 - sequential_87_mse: 0.0467 - sequential_88_mse: 0.0088 - sequential_89_mse: 0.0297 - sequential_90_mse: 0.0098 - sequential_91_mse: 0.0457 - sequential_92_mse: 0.0195 - sequential_93_mse: 0.0225 - sequential_94_mse: 0.0649 - sequential_95_mse: 0.0231\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n","k: 15 patience: 5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n","Test Loss no  0 : 25.500299616029835\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 0.4665 - sequential_80_mse: 0.0469 - sequential_81_mse: 0.0285 - sequential_82_mse: 0.0273 - sequential_83_mse: 0.0351 - sequential_84_mse: 0.0097 - sequential_85_mse: 0.0287 - sequential_86_mse: 0.0503 - sequential_87_mse: 0.0436 - sequential_88_mse: 0.0115 - sequential_89_mse: 0.0201 - sequential_90_mse: 0.0095 - sequential_91_mse: 0.0442 - sequential_92_mse: 0.0129 - sequential_93_mse: 0.0203 - sequential_94_mse: 0.0565 - sequential_95_mse: 0.0214\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n","k: 16 patience: 6\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n","Test Loss no  0 : 22.58482318289243\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.4629 - sequential_80_mse: 0.0410 - sequential_81_mse: 0.0243 - sequential_82_mse: 0.0306 - sequential_83_mse: 0.0314 - sequential_84_mse: 0.0112 - sequential_85_mse: 0.0343 - sequential_86_mse: 0.0607 - sequential_87_mse: 0.0345 - sequential_88_mse: 0.0080 - sequential_89_mse: 0.0233 - sequential_90_mse: 0.0086 - sequential_91_mse: 0.0390 - sequential_92_mse: 0.0146 - sequential_93_mse: 0.0212 - sequential_94_mse: 0.0571 - sequential_95_mse: 0.0232\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n","k: 17 patience: 7\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n","Test Loss no  0 : 20.967396837372824\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.4712 - sequential_80_mse: 0.0381 - sequential_81_mse: 0.0255 - sequential_82_mse: 0.0395 - sequential_83_mse: 0.0395 - sequential_84_mse: 0.0118 - sequential_85_mse: 0.0334 - sequential_86_mse: 0.0446 - sequential_87_mse: 0.0342 - sequential_88_mse: 0.0123 - sequential_89_mse: 0.0209 - sequential_90_mse: 0.0100 - sequential_91_mse: 0.0462 - sequential_92_mse: 0.0192 - sequential_93_mse: 0.0202 - sequential_94_mse: 0.0570 - sequential_95_mse: 0.0187\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n","k: 18 patience: 8\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n","Test Loss no  0 : 21.353831081744573\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.3962 - sequential_80_mse: 0.0395 - sequential_81_mse: 0.0267 - sequential_82_mse: 0.0264 - sequential_83_mse: 0.0326 - sequential_84_mse: 0.0118 - sequential_85_mse: 0.0193 - sequential_86_mse: 0.0434 - sequential_87_mse: 0.0221 - sequential_88_mse: 0.0119 - sequential_89_mse: 0.0225 - sequential_90_mse: 0.0103 - sequential_91_mse: 0.0382 - sequential_92_mse: 0.0153 - sequential_93_mse: 0.0204 - sequential_94_mse: 0.0383 - sequential_95_mse: 0.0174\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n","k: 19 patience: 9\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n","Test Loss no  0 : 23.9934283293532\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.3735 - sequential_80_mse: 0.0325 - sequential_81_mse: 0.0203 - sequential_82_mse: 0.0220 - sequential_83_mse: 0.0274 - sequential_84_mse: 0.0089 - sequential_85_mse: 0.0272 - sequential_86_mse: 0.0433 - sequential_87_mse: 0.0268 - sequential_88_mse: 0.0089 - sequential_89_mse: 0.0177 - sequential_90_mse: 0.0089 - sequential_91_mse: 0.0352 - sequential_92_mse: 0.0095 - sequential_93_mse: 0.0221 - sequential_94_mse: 0.0438 - sequential_95_mse: 0.0190\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n","k: 20 patience: 10\n","Iteration 2\n","global_indices [[9, 7, 1], [6, 8, 12], [11, 6, 4], [8, 2], [15, 8, 6]]\n","input_train : (41, 120, 47)\n","output_train : (41, 16)\n","input_val : (13, 120, 47)\n","output_val : (13, 16)\n","input_test : (14, 120, 47)\n","output_test : (14, 16)\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 97ms/step - loss: 1.4388 - sequential_100_mse: 0.0643 - sequential_101_mse: 0.0782 - sequential_102_mse: 0.1021 - sequential_103_mse: 0.1083 - sequential_104_mse: 0.0823 - sequential_105_mse: 0.0547 - sequential_106_mse: 0.0562 - sequential_107_mse: 0.0952 - sequential_108_mse: 0.0515 - sequential_109_mse: 0.0714 - sequential_110_mse: 0.1534 - sequential_111_mse: 0.0879 - sequential_96_mse: 0.1378 - sequential_97_mse: 0.1010 - sequential_98_mse: 0.0514 - sequential_99_mse: 0.1432\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 727ms/step\n","k: 0 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 729ms/step\n","Test Loss no  1 : 31.337054581142045\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 1.3299 - sequential_100_mse: 0.0551 - sequential_101_mse: 0.0925 - sequential_102_mse: 0.1129 - sequential_103_mse: 0.1011 - sequential_104_mse: 0.0544 - sequential_105_mse: 0.0559 - sequential_106_mse: 0.0666 - sequential_107_mse: 0.0931 - sequential_108_mse: 0.0606 - sequential_109_mse: 0.0624 - sequential_110_mse: 0.1377 - sequential_111_mse: 0.0977 - sequential_96_mse: 0.1028 - sequential_97_mse: 0.0695 - sequential_98_mse: 0.0523 - sequential_99_mse: 0.1153\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n","k: 1 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n","Test Loss no  1 : 29.72550808400065\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 1.1622 - sequential_100_mse: 0.0595 - sequential_101_mse: 0.0897 - sequential_102_mse: 0.0953 - sequential_103_mse: 0.0909 - sequential_104_mse: 0.0473 - sequential_105_mse: 0.0438 - sequential_106_mse: 0.0494 - sequential_107_mse: 0.1055 - sequential_108_mse: 0.0467 - sequential_109_mse: 0.0531 - sequential_110_mse: 0.0979 - sequential_111_mse: 0.0773 - sequential_96_mse: 0.0999 - sequential_97_mse: 0.0682 - sequential_98_mse: 0.0479 - sequential_99_mse: 0.0898\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n","k: 2 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n","Test Loss no  1 : 28.112466567649562\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 0.9949 - sequential_100_mse: 0.0540 - sequential_101_mse: 0.0669 - sequential_102_mse: 0.0931 - sequential_103_mse: 0.0873 - sequential_104_mse: 0.0368 - sequential_105_mse: 0.0425 - sequential_106_mse: 0.0422 - sequential_107_mse: 0.1014 - sequential_108_mse: 0.0466 - sequential_109_mse: 0.0480 - sequential_110_mse: 0.0806 - sequential_111_mse: 0.0581 - sequential_96_mse: 0.0804 - sequential_97_mse: 0.0599 - sequential_98_mse: 0.0400 - sequential_99_mse: 0.0572\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n","k: 3 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n","Test Loss no  1 : 27.999977967690263\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 1.0366 - sequential_100_mse: 0.0552 - sequential_101_mse: 0.0663 - sequential_102_mse: 0.1013 - sequential_103_mse: 0.1036 - sequential_104_mse: 0.0385 - sequential_105_mse: 0.0364 - sequential_106_mse: 0.0391 - sequential_107_mse: 0.0874 - sequential_108_mse: 0.0528 - sequential_109_mse: 0.0513 - sequential_110_mse: 0.1033 - sequential_111_mse: 0.0586 - sequential_96_mse: 0.0834 - sequential_97_mse: 0.0623 - sequential_98_mse: 0.0321 - sequential_99_mse: 0.0650\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n","k: 4 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n","Test Loss no  1 : 26.38382180479388\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 0.9026 - sequential_100_mse: 0.0520 - sequential_101_mse: 0.0508 - sequential_102_mse: 0.0831 - sequential_103_mse: 0.0757 - sequential_104_mse: 0.0403 - sequential_105_mse: 0.0340 - sequential_106_mse: 0.0417 - sequential_107_mse: 0.0884 - sequential_108_mse: 0.0419 - sequential_109_mse: 0.0417 - sequential_110_mse: 0.0791 - sequential_111_mse: 0.0629 - sequential_96_mse: 0.0717 - sequential_97_mse: 0.0548 - sequential_98_mse: 0.0283 - sequential_99_mse: 0.0562\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n","k: 5 patience: 1\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n","Test Loss no  1 : 27.90386703808617\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.9566 - sequential_100_mse: 0.0444 - sequential_101_mse: 0.0621 - sequential_102_mse: 0.0840 - sequential_103_mse: 0.0842 - sequential_104_mse: 0.0400 - sequential_105_mse: 0.0329 - sequential_106_mse: 0.0357 - sequential_107_mse: 0.0937 - sequential_108_mse: 0.0463 - sequential_109_mse: 0.0435 - sequential_110_mse: 0.0840 - sequential_111_mse: 0.0686 - sequential_96_mse: 0.0875 - sequential_97_mse: 0.0542 - sequential_98_mse: 0.0366 - sequential_99_mse: 0.0591\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n","k: 6 patience: 2\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n","Test Loss no  1 : 28.72641545670782\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 0.8353 - sequential_100_mse: 0.0497 - sequential_101_mse: 0.0565 - sequential_102_mse: 0.0730 - sequential_103_mse: 0.0716 - sequential_104_mse: 0.0303 - sequential_105_mse: 0.0337 - sequential_106_mse: 0.0363 - sequential_107_mse: 0.0840 - sequential_108_mse: 0.0376 - sequential_109_mse: 0.0450 - sequential_110_mse: 0.0691 - sequential_111_mse: 0.0462 - sequential_96_mse: 0.0648 - sequential_97_mse: 0.0497 - sequential_98_mse: 0.0338 - sequential_99_mse: 0.0541\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n","k: 7 patience: 3\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n","Test Loss no  1 : 26.968664457294047\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.8096 - sequential_100_mse: 0.0446 - sequential_101_mse: 0.0534 - sequential_102_mse: 0.0768 - sequential_103_mse: 0.0658 - sequential_104_mse: 0.0291 - sequential_105_mse: 0.0334 - sequential_106_mse: 0.0327 - sequential_107_mse: 0.0658 - sequential_108_mse: 0.0430 - sequential_109_mse: 0.0457 - sequential_110_mse: 0.0691 - sequential_111_mse: 0.0525 - sequential_96_mse: 0.0653 - sequential_97_mse: 0.0495 - sequential_98_mse: 0.0336 - sequential_99_mse: 0.0492\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n","k: 8 patience: 4\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n","Test Loss no  1 : 26.23057585996272\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 0.8539 - sequential_100_mse: 0.0462 - sequential_101_mse: 0.0589 - sequential_102_mse: 0.0824 - sequential_103_mse: 0.0733 - sequential_104_mse: 0.0323 - sequential_105_mse: 0.0345 - sequential_106_mse: 0.0302 - sequential_107_mse: 0.0797 - sequential_108_mse: 0.0389 - sequential_109_mse: 0.0401 - sequential_110_mse: 0.0689 - sequential_111_mse: 0.0529 - sequential_96_mse: 0.0706 - sequential_97_mse: 0.0493 - sequential_98_mse: 0.0381 - sequential_99_mse: 0.0576\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n","k: 9 patience: 5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n","Test Loss no  1 : 26.782130055607375\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 0.8028 - sequential_100_mse: 0.0448 - sequential_101_mse: 0.0475 - sequential_102_mse: 0.0725 - sequential_103_mse: 0.0636 - sequential_104_mse: 0.0375 - sequential_105_mse: 0.0301 - sequential_106_mse: 0.0370 - sequential_107_mse: 0.0779 - sequential_108_mse: 0.0406 - sequential_109_mse: 0.0465 - sequential_110_mse: 0.0652 - sequential_111_mse: 0.0514 - sequential_96_mse: 0.0578 - sequential_97_mse: 0.0528 - sequential_98_mse: 0.0348 - sequential_99_mse: 0.0428\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n","k: 10 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n","Test Loss no  1 : 26.370220782781843\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.7746 - sequential_100_mse: 0.0442 - sequential_101_mse: 0.0528 - sequential_102_mse: 0.0760 - sequential_103_mse: 0.0689 - sequential_104_mse: 0.0300 - sequential_105_mse: 0.0285 - sequential_106_mse: 0.0261 - sequential_107_mse: 0.0633 - sequential_108_mse: 0.0348 - sequential_109_mse: 0.0460 - sequential_110_mse: 0.0644 - sequential_111_mse: 0.0517 - sequential_96_mse: 0.0673 - sequential_97_mse: 0.0465 - sequential_98_mse: 0.0283 - sequential_99_mse: 0.0458\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n","k: 11 patience: 1\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n","Test Loss no  1 : 27.334778540187475\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 0.7521 - sequential_100_mse: 0.0430 - sequential_101_mse: 0.0547 - sequential_102_mse: 0.0738 - sequential_103_mse: 0.0689 - sequential_104_mse: 0.0290 - sequential_105_mse: 0.0268 - sequential_106_mse: 0.0310 - sequential_107_mse: 0.0670 - sequential_108_mse: 0.0403 - sequential_109_mse: 0.0350 - sequential_110_mse: 0.0604 - sequential_111_mse: 0.0529 - sequential_96_mse: 0.0619 - sequential_97_mse: 0.0466 - sequential_98_mse: 0.0198 - sequential_99_mse: 0.0410\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n","k: 12 patience: 2\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n","Test Loss no  1 : 26.857321172410476\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 0.6701 - sequential_100_mse: 0.0348 - sequential_101_mse: 0.0436 - sequential_102_mse: 0.0687 - sequential_103_mse: 0.0560 - sequential_104_mse: 0.0247 - sequential_105_mse: 0.0235 - sequential_106_mse: 0.0214 - sequential_107_mse: 0.0620 - sequential_108_mse: 0.0257 - sequential_109_mse: 0.0358 - sequential_110_mse: 0.0537 - sequential_111_mse: 0.0492 - sequential_96_mse: 0.0556 - sequential_97_mse: 0.0427 - sequential_98_mse: 0.0280 - sequential_99_mse: 0.0448\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n","k: 13 patience: 3\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n","Test Loss no  1 : 27.020935887602104\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.7365 - sequential_100_mse: 0.0398 - sequential_101_mse: 0.0515 - sequential_102_mse: 0.0817 - sequential_103_mse: 0.0588 - sequential_104_mse: 0.0320 - sequential_105_mse: 0.0244 - sequential_106_mse: 0.0275 - sequential_107_mse: 0.0698 - sequential_108_mse: 0.0347 - sequential_109_mse: 0.0359 - sequential_110_mse: 0.0561 - sequential_111_mse: 0.0627 - sequential_96_mse: 0.0529 - sequential_97_mse: 0.0401 - sequential_98_mse: 0.0295 - sequential_99_mse: 0.0393\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n","k: 14 patience: 4\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n","Test Loss no  1 : 27.590068405411834\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.7104 - sequential_100_mse: 0.0363 - sequential_101_mse: 0.0494 - sequential_102_mse: 0.0766 - sequential_103_mse: 0.0626 - sequential_104_mse: 0.0253 - sequential_105_mse: 0.0288 - sequential_106_mse: 0.0244 - sequential_107_mse: 0.0788 - sequential_108_mse: 0.0265 - sequential_109_mse: 0.0345 - sequential_110_mse: 0.0551 - sequential_111_mse: 0.0515 - sequential_96_mse: 0.0581 - sequential_97_mse: 0.0367 - sequential_98_mse: 0.0288 - sequential_99_mse: 0.0368\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n","k: 15 patience: 5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n","Test Loss no  1 : 27.317281139688035\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 0.5897 - sequential_100_mse: 0.0349 - sequential_101_mse: 0.0342 - sequential_102_mse: 0.0560 - sequential_103_mse: 0.0577 - sequential_104_mse: 0.0223 - sequential_105_mse: 0.0206 - sequential_106_mse: 0.0227 - sequential_107_mse: 0.0608 - sequential_108_mse: 0.0222 - sequential_109_mse: 0.0248 - sequential_110_mse: 0.0428 - sequential_111_mse: 0.0372 - sequential_96_mse: 0.0506 - sequential_97_mse: 0.0340 - sequential_98_mse: 0.0225 - sequential_99_mse: 0.0463\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n","k: 16 patience: 6\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n","Test Loss no  1 : 27.187714320811775\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 0.6744 - sequential_100_mse: 0.0337 - sequential_101_mse: 0.0373 - sequential_102_mse: 0.0670 - sequential_103_mse: 0.0624 - sequential_104_mse: 0.0243 - sequential_105_mse: 0.0292 - sequential_106_mse: 0.0256 - sequential_107_mse: 0.0745 - sequential_108_mse: 0.0245 - sequential_109_mse: 0.0336 - sequential_110_mse: 0.0514 - sequential_111_mse: 0.0515 - sequential_96_mse: 0.0516 - sequential_97_mse: 0.0377 - sequential_98_mse: 0.0280 - sequential_99_mse: 0.0420\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n","k: 17 patience: 7\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n","Test Loss no  1 : 26.78088214849275\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 0.5896 - sequential_100_mse: 0.0346 - sequential_101_mse: 0.0322 - sequential_102_mse: 0.0649 - sequential_103_mse: 0.0465 - sequential_104_mse: 0.0203 - sequential_105_mse: 0.0166 - sequential_106_mse: 0.0183 - sequential_107_mse: 0.0660 - sequential_108_mse: 0.0221 - sequential_109_mse: 0.0272 - sequential_110_mse: 0.0519 - sequential_111_mse: 0.0394 - sequential_96_mse: 0.0502 - sequential_97_mse: 0.0325 - sequential_98_mse: 0.0304 - sequential_99_mse: 0.0366\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n","k: 18 patience: 8\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n","Test Loss no  1 : 26.754113118832155\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.5503 - sequential_100_mse: 0.0307 - sequential_101_mse: 0.0387 - sequential_102_mse: 0.0565 - sequential_103_mse: 0.0382 - sequential_104_mse: 0.0209 - sequential_105_mse: 0.0204 - sequential_106_mse: 0.0183 - sequential_107_mse: 0.0563 - sequential_108_mse: 0.0200 - sequential_109_mse: 0.0257 - sequential_110_mse: 0.0443 - sequential_111_mse: 0.0367 - sequential_96_mse: 0.0434 - sequential_97_mse: 0.0308 - sequential_98_mse: 0.0254 - sequential_99_mse: 0.0441\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n","k: 19 patience: 9\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n","Test Loss no  1 : 29.230747208144205\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.5802 - sequential_100_mse: 0.0355 - sequential_101_mse: 0.0374 - sequential_102_mse: 0.0569 - sequential_103_mse: 0.0491 - sequential_104_mse: 0.0236 - sequential_105_mse: 0.0173 - sequential_106_mse: 0.0229 - sequential_107_mse: 0.0603 - sequential_108_mse: 0.0197 - sequential_109_mse: 0.0242 - sequential_110_mse: 0.0467 - sequential_111_mse: 0.0475 - sequential_96_mse: 0.0448 - sequential_97_mse: 0.0348 - sequential_98_mse: 0.0174 - sequential_99_mse: 0.0420\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n","k: 20 patience: 10\n","Iteration 3\n","global_indices [[9, 7, 1, 10, 8, 12], [6, 8, 12, 5, 9, 13], [11, 6, 4, 3, 1, 0], [8, 2, 1, 5], [15, 8, 6, 10, 1, 14]]\n","input_train : (41, 120, 47)\n","output_train : (41, 16)\n","input_val : (13, 120, 47)\n","output_val : (13, 16)\n","input_test : (14, 120, 47)\n","output_test : (14, 16)\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 96ms/step - loss: 1.1899 - sequential_112_mse: 0.1300 - sequential_113_mse: 0.0837 - sequential_114_mse: 0.0500 - sequential_115_mse: 0.1230 - sequential_116_mse: 0.0317 - sequential_117_mse: 0.0348 - sequential_118_mse: 0.1582 - sequential_119_mse: 0.0979 - sequential_120_mse: 0.0192 - sequential_121_mse: 0.0316 - sequential_122_mse: 0.0312 - sequential_123_mse: 0.0861 - sequential_124_mse: 0.0615 - sequential_125_mse: 0.0505 - sequential_126_mse: 0.1451 - sequential_127_mse: 0.0554\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 464ms/step\n","k: 0 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 478ms/step\n","Test Loss no  2 : 26.354066198003977\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 0.8670 - sequential_112_mse: 0.0808 - sequential_113_mse: 0.0625 - sequential_114_mse: 0.0329 - sequential_115_mse: 0.0894 - sequential_116_mse: 0.0238 - sequential_117_mse: 0.0380 - sequential_118_mse: 0.1015 - sequential_119_mse: 0.0830 - sequential_120_mse: 0.0252 - sequential_121_mse: 0.0288 - sequential_122_mse: 0.0321 - sequential_123_mse: 0.0567 - sequential_124_mse: 0.0402 - sequential_125_mse: 0.0494 - sequential_126_mse: 0.0850 - sequential_127_mse: 0.0376\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n","k: 1 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n","Test Loss no  2 : 25.165213202789936\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.8840 - sequential_112_mse: 0.0897 - sequential_113_mse: 0.0610 - sequential_114_mse: 0.0457 - sequential_115_mse: 0.0909 - sequential_116_mse: 0.0303 - sequential_117_mse: 0.0374 - sequential_118_mse: 0.1047 - sequential_119_mse: 0.0810 - sequential_120_mse: 0.0216 - sequential_121_mse: 0.0387 - sequential_122_mse: 0.0265 - sequential_123_mse: 0.0556 - sequential_124_mse: 0.0318 - sequential_125_mse: 0.0366 - sequential_126_mse: 0.0892 - sequential_127_mse: 0.0433\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n","k: 2 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n","Test Loss no  2 : 25.3184199131361\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.7318 - sequential_112_mse: 0.0714 - sequential_113_mse: 0.0608 - sequential_114_mse: 0.0281 - sequential_115_mse: 0.0788 - sequential_116_mse: 0.0266 - sequential_117_mse: 0.0222 - sequential_118_mse: 0.0764 - sequential_119_mse: 0.0772 - sequential_120_mse: 0.0202 - sequential_121_mse: 0.0282 - sequential_122_mse: 0.0238 - sequential_123_mse: 0.0355 - sequential_124_mse: 0.0297 - sequential_125_mse: 0.0375 - sequential_126_mse: 0.0745 - sequential_127_mse: 0.0409\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n","k: 3 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n","Test Loss no  2 : 24.894677809126087\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.7254 - sequential_112_mse: 0.0731 - sequential_113_mse: 0.0485 - sequential_114_mse: 0.0363 - sequential_115_mse: 0.0822 - sequential_116_mse: 0.0199 - sequential_117_mse: 0.0301 - sequential_118_mse: 0.0846 - sequential_119_mse: 0.0691 - sequential_120_mse: 0.0192 - sequential_121_mse: 0.0333 - sequential_122_mse: 0.0224 - sequential_123_mse: 0.0441 - sequential_124_mse: 0.0323 - sequential_125_mse: 0.0291 - sequential_126_mse: 0.0630 - sequential_127_mse: 0.0381\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n","k: 4 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n","Test Loss no  2 : 23.033633090614806\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 0.7193 - sequential_112_mse: 0.0684 - sequential_113_mse: 0.0575 - sequential_114_mse: 0.0341 - sequential_115_mse: 0.0763 - sequential_116_mse: 0.0239 - sequential_117_mse: 0.0327 - sequential_118_mse: 0.0788 - sequential_119_mse: 0.0678 - sequential_120_mse: 0.0194 - sequential_121_mse: 0.0344 - sequential_122_mse: 0.0292 - sequential_123_mse: 0.0426 - sequential_124_mse: 0.0295 - sequential_125_mse: 0.0288 - sequential_126_mse: 0.0601 - sequential_127_mse: 0.0358\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n","k: 5 patience: 1\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n","Test Loss no  2 : 22.395155630790047\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.6755 - sequential_112_mse: 0.0618 - sequential_113_mse: 0.0503 - sequential_114_mse: 0.0251 - sequential_115_mse: 0.0738 - sequential_116_mse: 0.0241 - sequential_117_mse: 0.0311 - sequential_118_mse: 0.0750 - sequential_119_mse: 0.0652 - sequential_120_mse: 0.0179 - sequential_121_mse: 0.0281 - sequential_122_mse: 0.0211 - sequential_123_mse: 0.0385 - sequential_124_mse: 0.0282 - sequential_125_mse: 0.0326 - sequential_126_mse: 0.0663 - sequential_127_mse: 0.0363\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n","k: 6 patience: 2\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n","Test Loss no  2 : 22.975688374365895\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 0.6260 - sequential_112_mse: 0.0481 - sequential_113_mse: 0.0469 - sequential_114_mse: 0.0300 - sequential_115_mse: 0.0713 - sequential_116_mse: 0.0169 - sequential_117_mse: 0.0244 - sequential_118_mse: 0.0734 - sequential_119_mse: 0.0590 - sequential_120_mse: 0.0161 - sequential_121_mse: 0.0318 - sequential_122_mse: 0.0240 - sequential_123_mse: 0.0398 - sequential_124_mse: 0.0267 - sequential_125_mse: 0.0315 - sequential_126_mse: 0.0556 - sequential_127_mse: 0.0307\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n","k: 7 patience: 3\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n","Test Loss no  2 : 22.82015211309693\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 0.5605 - sequential_112_mse: 0.0394 - sequential_113_mse: 0.0449 - sequential_114_mse: 0.0225 - sequential_115_mse: 0.0717 - sequential_116_mse: 0.0176 - sequential_117_mse: 0.0208 - sequential_118_mse: 0.0610 - sequential_119_mse: 0.0445 - sequential_120_mse: 0.0167 - sequential_121_mse: 0.0243 - sequential_122_mse: 0.0247 - sequential_123_mse: 0.0325 - sequential_124_mse: 0.0242 - sequential_125_mse: 0.0296 - sequential_126_mse: 0.0607 - sequential_127_mse: 0.0255\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n","k: 8 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n","Test Loss no  2 : 20.30583228601326\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.5859 - sequential_112_mse: 0.0539 - sequential_113_mse: 0.0429 - sequential_114_mse: 0.0316 - sequential_115_mse: 0.0662 - sequential_116_mse: 0.0166 - sequential_117_mse: 0.0291 - sequential_118_mse: 0.0600 - sequential_119_mse: 0.0560 - sequential_120_mse: 0.0142 - sequential_121_mse: 0.0249 - sequential_122_mse: 0.0157 - sequential_123_mse: 0.0304 - sequential_124_mse: 0.0264 - sequential_125_mse: 0.0281 - sequential_126_mse: 0.0596 - sequential_127_mse: 0.0303\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n","k: 9 patience: 1\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n","Test Loss no  2 : 20.188367031989955\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.5442 - sequential_112_mse: 0.0451 - sequential_113_mse: 0.0494 - sequential_114_mse: 0.0258 - sequential_115_mse: 0.0628 - sequential_116_mse: 0.0183 - sequential_117_mse: 0.0249 - sequential_118_mse: 0.0519 - sequential_119_mse: 0.0485 - sequential_120_mse: 0.0159 - sequential_121_mse: 0.0270 - sequential_122_mse: 0.0143 - sequential_123_mse: 0.0324 - sequential_124_mse: 0.0225 - sequential_125_mse: 0.0268 - sequential_126_mse: 0.0535 - sequential_127_mse: 0.0250\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n","k: 10 patience: 2\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n","Test Loss no  2 : 23.203462532641534\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.5204 - sequential_112_mse: 0.0381 - sequential_113_mse: 0.0489 - sequential_114_mse: 0.0254 - sequential_115_mse: 0.0634 - sequential_116_mse: 0.0169 - sequential_117_mse: 0.0201 - sequential_118_mse: 0.0534 - sequential_119_mse: 0.0524 - sequential_120_mse: 0.0151 - sequential_121_mse: 0.0297 - sequential_122_mse: 0.0187 - sequential_123_mse: 0.0320 - sequential_124_mse: 0.0213 - sequential_125_mse: 0.0211 - sequential_126_mse: 0.0406 - sequential_127_mse: 0.0232\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n","k: 11 patience: 3\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n","Test Loss no  2 : 23.24715616551373\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.5151 - sequential_112_mse: 0.0388 - sequential_113_mse: 0.0422 - sequential_114_mse: 0.0227 - sequential_115_mse: 0.0651 - sequential_116_mse: 0.0188 - sequential_117_mse: 0.0211 - sequential_118_mse: 0.0492 - sequential_119_mse: 0.0421 - sequential_120_mse: 0.0158 - sequential_121_mse: 0.0259 - sequential_122_mse: 0.0184 - sequential_123_mse: 0.0338 - sequential_124_mse: 0.0205 - sequential_125_mse: 0.0292 - sequential_126_mse: 0.0477 - sequential_127_mse: 0.0238\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n","k: 12 patience: 4\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n","Test Loss no  2 : 21.86498971581936\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.4738 - sequential_112_mse: 0.0291 - sequential_113_mse: 0.0365 - sequential_114_mse: 0.0220 - sequential_115_mse: 0.0438 - sequential_116_mse: 0.0147 - sequential_117_mse: 0.0210 - sequential_118_mse: 0.0443 - sequential_119_mse: 0.0489 - sequential_120_mse: 0.0136 - sequential_121_mse: 0.0279 - sequential_122_mse: 0.0227 - sequential_123_mse: 0.0291 - sequential_124_mse: 0.0197 - sequential_125_mse: 0.0269 - sequential_126_mse: 0.0505 - sequential_127_mse: 0.0230\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n","k: 13 patience: 5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n","Test Loss no  2 : 21.52172420356198\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 0.4305 - sequential_112_mse: 0.0311 - sequential_113_mse: 0.0313 - sequential_114_mse: 0.0224 - sequential_115_mse: 0.0524 - sequential_116_mse: 0.0131 - sequential_117_mse: 0.0202 - sequential_118_mse: 0.0437 - sequential_119_mse: 0.0335 - sequential_120_mse: 0.0108 - sequential_121_mse: 0.0255 - sequential_122_mse: 0.0204 - sequential_123_mse: 0.0236 - sequential_124_mse: 0.0174 - sequential_125_mse: 0.0224 - sequential_126_mse: 0.0433 - sequential_127_mse: 0.0195\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n","k: 14 patience: 6\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n","Test Loss no  2 : 21.901164156087386\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.4136 - sequential_112_mse: 0.0259 - sequential_113_mse: 0.0315 - sequential_114_mse: 0.0144 - sequential_115_mse: 0.0483 - sequential_116_mse: 0.0120 - sequential_117_mse: 0.0170 - sequential_118_mse: 0.0373 - sequential_119_mse: 0.0396 - sequential_120_mse: 0.0151 - sequential_121_mse: 0.0250 - sequential_122_mse: 0.0181 - sequential_123_mse: 0.0228 - sequential_124_mse: 0.0190 - sequential_125_mse: 0.0258 - sequential_126_mse: 0.0467 - sequential_127_mse: 0.0151\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n","k: 15 patience: 7\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n","Test Loss no  2 : 21.1338194360569\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.3576 - sequential_112_mse: 0.0214 - sequential_113_mse: 0.0273 - sequential_114_mse: 0.0161 - sequential_115_mse: 0.0463 - sequential_116_mse: 0.0162 - sequential_117_mse: 0.0095 - sequential_118_mse: 0.0306 - sequential_119_mse: 0.0306 - sequential_120_mse: 0.0121 - sequential_121_mse: 0.0230 - sequential_122_mse: 0.0134 - sequential_123_mse: 0.0216 - sequential_124_mse: 0.0132 - sequential_125_mse: 0.0218 - sequential_126_mse: 0.0345 - sequential_127_mse: 0.0201\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n","k: 16 patience: 8\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n","Test Loss no  2 : 20.290638883263785\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 0.3590 - sequential_112_mse: 0.0229 - sequential_113_mse: 0.0247 - sequential_114_mse: 0.0165 - sequential_115_mse: 0.0468 - sequential_116_mse: 0.0151 - sequential_117_mse: 0.0118 - sequential_118_mse: 0.0279 - sequential_119_mse: 0.0324 - sequential_120_mse: 0.0118 - sequential_121_mse: 0.0247 - sequential_122_mse: 0.0157 - sequential_123_mse: 0.0220 - sequential_124_mse: 0.0147 - sequential_125_mse: 0.0194 - sequential_126_mse: 0.0366 - sequential_127_mse: 0.0160\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n","k: 17 patience: 9\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n","Test Loss no  2 : 20.12971988012797\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.3452 - sequential_112_mse: 0.0211 - sequential_113_mse: 0.0283 - sequential_114_mse: 0.0199 - sequential_115_mse: 0.0406 - sequential_116_mse: 0.0156 - sequential_117_mse: 0.0105 - sequential_118_mse: 0.0323 - sequential_119_mse: 0.0242 - sequential_120_mse: 0.0142 - sequential_121_mse: 0.0238 - sequential_122_mse: 0.0129 - sequential_123_mse: 0.0188 - sequential_124_mse: 0.0117 - sequential_125_mse: 0.0212 - sequential_126_mse: 0.0388 - sequential_127_mse: 0.0112\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n","k: 18 patience: 10\n","Iteration 4\n","global_indices [[9, 7, 1, 10, 8, 12, 4, 0, 6], [6, 8, 12, 5, 9, 13, 2, 1, 3], [11, 6, 4, 3, 1, 0, 9, 12, 10], [8, 2, 1, 5, 3, 9], [15, 8, 6, 10, 1, 14, 7, 3, 11]]\n","input_train : (41, 120, 47)\n","output_train : (41, 16)\n","input_val : (14, 120, 47)\n","output_val : (14, 16)\n","input_test : (13, 120, 47)\n","output_test : (13, 16)\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 94ms/step - loss: 1.7648 - sequential_128_mse: 0.1372 - sequential_129_mse: 0.0952 - sequential_130_mse: 0.0898 - sequential_131_mse: 0.1626 - sequential_132_mse: 0.0617 - sequential_133_mse: 0.1090 - sequential_134_mse: 0.2238 - sequential_135_mse: 0.1003 - sequential_136_mse: 0.0558 - sequential_137_mse: 0.1238 - sequential_138_mse: 0.0391 - sequential_139_mse: 0.1794 - sequential_140_mse: 0.0885 - sequential_141_mse: 0.0671 - sequential_142_mse: 0.1359 - sequential_143_mse: 0.0955\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 502ms/step\n","k: 0 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 460ms/step\n","Test Loss no  3 : 32.63904746729303\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 1.1382 - sequential_128_mse: 0.0833 - sequential_129_mse: 0.0639 - sequential_130_mse: 0.0755 - sequential_131_mse: 0.0984 - sequential_132_mse: 0.0580 - sequential_133_mse: 0.0988 - sequential_134_mse: 0.1130 - sequential_135_mse: 0.0909 - sequential_136_mse: 0.0349 - sequential_137_mse: 0.0615 - sequential_138_mse: 0.0299 - sequential_139_mse: 0.0609 - sequential_140_mse: 0.0458 - sequential_141_mse: 0.0407 - sequential_142_mse: 0.1284 - sequential_143_mse: 0.0543\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n","k: 1 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n","Test Loss no  3 : 31.451293299550073\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 1.1322 - sequential_128_mse: 0.0925 - sequential_129_mse: 0.0677 - sequential_130_mse: 0.0779 - sequential_131_mse: 0.0758 - sequential_132_mse: 0.0508 - sequential_133_mse: 0.0771 - sequential_134_mse: 0.1163 - sequential_135_mse: 0.0936 - sequential_136_mse: 0.0402 - sequential_137_mse: 0.0686 - sequential_138_mse: 0.0343 - sequential_139_mse: 0.0943 - sequential_140_mse: 0.0365 - sequential_141_mse: 0.0378 - sequential_142_mse: 0.1029 - sequential_143_mse: 0.0658\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n","k: 2 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n","Test Loss no  3 : 30.622468331881123\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.9904 - sequential_128_mse: 0.0815 - sequential_129_mse: 0.0552 - sequential_130_mse: 0.0499 - sequential_131_mse: 0.0782 - sequential_132_mse: 0.0461 - sequential_133_mse: 0.0602 - sequential_134_mse: 0.0965 - sequential_135_mse: 0.0865 - sequential_136_mse: 0.0388 - sequential_137_mse: 0.0619 - sequential_138_mse: 0.0360 - sequential_139_mse: 0.0763 - sequential_140_mse: 0.0507 - sequential_141_mse: 0.0301 - sequential_142_mse: 0.0894 - sequential_143_mse: 0.0532\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n","k: 3 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n","Test Loss no  3 : 31.402905534247896\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.9694 - sequential_128_mse: 0.0728 - sequential_129_mse: 0.0642 - sequential_130_mse: 0.0613 - sequential_131_mse: 0.0802 - sequential_132_mse: 0.0507 - sequential_133_mse: 0.0623 - sequential_134_mse: 0.0918 - sequential_135_mse: 0.0833 - sequential_136_mse: 0.0394 - sequential_137_mse: 0.0571 - sequential_138_mse: 0.0316 - sequential_139_mse: 0.0662 - sequential_140_mse: 0.0446 - sequential_141_mse: 0.0374 - sequential_142_mse: 0.0827 - sequential_143_mse: 0.0437\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n","k: 4 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n","Test Loss no  3 : 30.405047483309076\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.9327 - sequential_128_mse: 0.0777 - sequential_129_mse: 0.0477 - sequential_130_mse: 0.0512 - sequential_131_mse: 0.0742 - sequential_132_mse: 0.0447 - sequential_133_mse: 0.0585 - sequential_134_mse: 0.1017 - sequential_135_mse: 0.0854 - sequential_136_mse: 0.0373 - sequential_137_mse: 0.0521 - sequential_138_mse: 0.0270 - sequential_139_mse: 0.0632 - sequential_140_mse: 0.0423 - sequential_141_mse: 0.0338 - sequential_142_mse: 0.0914 - sequential_143_mse: 0.0445\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n","k: 5 patience: 1\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n","Test Loss no  3 : 29.58315323985298\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 0.8955 - sequential_128_mse: 0.0705 - sequential_129_mse: 0.0578 - sequential_130_mse: 0.0587 - sequential_131_mse: 0.0605 - sequential_132_mse: 0.0394 - sequential_133_mse: 0.0638 - sequential_134_mse: 0.0893 - sequential_135_mse: 0.0875 - sequential_136_mse: 0.0364 - sequential_137_mse: 0.0500 - sequential_138_mse: 0.0280 - sequential_139_mse: 0.0579 - sequential_140_mse: 0.0372 - sequential_141_mse: 0.0271 - sequential_142_mse: 0.0893 - sequential_143_mse: 0.0420\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n","k: 6 patience: 2\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n","Test Loss no  3 : 29.309802261816714\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.8124 - sequential_128_mse: 0.0643 - sequential_129_mse: 0.0521 - sequential_130_mse: 0.0569 - sequential_131_mse: 0.0580 - sequential_132_mse: 0.0392 - sequential_133_mse: 0.0545 - sequential_134_mse: 0.0819 - sequential_135_mse: 0.0708 - sequential_136_mse: 0.0299 - sequential_137_mse: 0.0406 - sequential_138_mse: 0.0215 - sequential_139_mse: 0.0606 - sequential_140_mse: 0.0340 - sequential_141_mse: 0.0324 - sequential_142_mse: 0.0763 - sequential_143_mse: 0.0396\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n","k: 7 patience: 3\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n","Test Loss no  3 : 29.249915487488945\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.8083 - sequential_128_mse: 0.0675 - sequential_129_mse: 0.0509 - sequential_130_mse: 0.0550 - sequential_131_mse: 0.0513 - sequential_132_mse: 0.0348 - sequential_133_mse: 0.0595 - sequential_134_mse: 0.0837 - sequential_135_mse: 0.0806 - sequential_136_mse: 0.0314 - sequential_137_mse: 0.0390 - sequential_138_mse: 0.0222 - sequential_139_mse: 0.0589 - sequential_140_mse: 0.0357 - sequential_141_mse: 0.0266 - sequential_142_mse: 0.0708 - sequential_143_mse: 0.0405\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n","k: 8 patience: 4\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n","Test Loss no  3 : 30.617543946744323\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 0.8264 - sequential_128_mse: 0.0597 - sequential_129_mse: 0.0526 - sequential_130_mse: 0.0551 - sequential_131_mse: 0.0575 - sequential_132_mse: 0.0390 - sequential_133_mse: 0.0612 - sequential_134_mse: 0.0742 - sequential_135_mse: 0.0681 - sequential_136_mse: 0.0356 - sequential_137_mse: 0.0504 - sequential_138_mse: 0.0263 - sequential_139_mse: 0.0663 - sequential_140_mse: 0.0327 - sequential_141_mse: 0.0288 - sequential_142_mse: 0.0721 - sequential_143_mse: 0.0471\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n","k: 9 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n","Test Loss no  3 : 29.62005149337388\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.7711 - sequential_128_mse: 0.0564 - sequential_129_mse: 0.0498 - sequential_130_mse: 0.0510 - sequential_131_mse: 0.0486 - sequential_132_mse: 0.0384 - sequential_133_mse: 0.0582 - sequential_134_mse: 0.0783 - sequential_135_mse: 0.0772 - sequential_136_mse: 0.0315 - sequential_137_mse: 0.0447 - sequential_138_mse: 0.0257 - sequential_139_mse: 0.0489 - sequential_140_mse: 0.0320 - sequential_141_mse: 0.0191 - sequential_142_mse: 0.0709 - sequential_143_mse: 0.0403\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n","k: 10 patience: 1\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n","Test Loss no  3 : 27.38288404474516\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.6836 - sequential_128_mse: 0.0454 - sequential_129_mse: 0.0460 - sequential_130_mse: 0.0447 - sequential_131_mse: 0.0438 - sequential_132_mse: 0.0368 - sequential_133_mse: 0.0529 - sequential_134_mse: 0.0615 - sequential_135_mse: 0.0622 - sequential_136_mse: 0.0253 - sequential_137_mse: 0.0330 - sequential_138_mse: 0.0214 - sequential_139_mse: 0.0504 - sequential_140_mse: 0.0284 - sequential_141_mse: 0.0262 - sequential_142_mse: 0.0656 - sequential_143_mse: 0.0400\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n","k: 11 patience: 2\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n","Test Loss no  3 : 26.79625577288939\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 0.6710 - sequential_128_mse: 0.0429 - sequential_129_mse: 0.0363 - sequential_130_mse: 0.0377 - sequential_131_mse: 0.0413 - sequential_132_mse: 0.0297 - sequential_133_mse: 0.0477 - sequential_134_mse: 0.0746 - sequential_135_mse: 0.0602 - sequential_136_mse: 0.0266 - sequential_137_mse: 0.0349 - sequential_138_mse: 0.0243 - sequential_139_mse: 0.0555 - sequential_140_mse: 0.0283 - sequential_141_mse: 0.0303 - sequential_142_mse: 0.0630 - sequential_143_mse: 0.0377\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n","k: 12 patience: 3\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n","Test Loss no  3 : 26.228089057379513\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 0.5931 - sequential_128_mse: 0.0314 - sequential_129_mse: 0.0302 - sequential_130_mse: 0.0315 - sequential_131_mse: 0.0419 - sequential_132_mse: 0.0306 - sequential_133_mse: 0.0479 - sequential_134_mse: 0.0716 - sequential_135_mse: 0.0489 - sequential_136_mse: 0.0247 - sequential_137_mse: 0.0318 - sequential_138_mse: 0.0175 - sequential_139_mse: 0.0438 - sequential_140_mse: 0.0311 - sequential_141_mse: 0.0184 - sequential_142_mse: 0.0528 - sequential_143_mse: 0.0390\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n","k: 13 patience: 4\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n","Test Loss no  3 : 25.405496165716716\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.5112 - sequential_128_mse: 0.0320 - sequential_129_mse: 0.0286 - sequential_130_mse: 0.0320 - sequential_131_mse: 0.0417 - sequential_132_mse: 0.0279 - sequential_133_mse: 0.0372 - sequential_134_mse: 0.0562 - sequential_135_mse: 0.0424 - sequential_136_mse: 0.0192 - sequential_137_mse: 0.0250 - sequential_138_mse: 0.0148 - sequential_139_mse: 0.0412 - sequential_140_mse: 0.0206 - sequential_141_mse: 0.0165 - sequential_142_mse: 0.0481 - sequential_143_mse: 0.0276\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n","k: 14 patience: 5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n","Test Loss no  3 : 25.344463977628127\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.5670 - sequential_128_mse: 0.0380 - sequential_129_mse: 0.0279 - sequential_130_mse: 0.0292 - sequential_131_mse: 0.0354 - sequential_132_mse: 0.0322 - sequential_133_mse: 0.0368 - sequential_134_mse: 0.0609 - sequential_135_mse: 0.0405 - sequential_136_mse: 0.0272 - sequential_137_mse: 0.0390 - sequential_138_mse: 0.0216 - sequential_139_mse: 0.0501 - sequential_140_mse: 0.0214 - sequential_141_mse: 0.0216 - sequential_142_mse: 0.0445 - sequential_143_mse: 0.0408\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n","k: 15 patience: 6\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n","Test Loss no  3 : 26.459579036549787\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.5205 - sequential_128_mse: 0.0304 - sequential_129_mse: 0.0262 - sequential_130_mse: 0.0346 - sequential_131_mse: 0.0376 - sequential_132_mse: 0.0268 - sequential_133_mse: 0.0410 - sequential_134_mse: 0.0561 - sequential_135_mse: 0.0365 - sequential_136_mse: 0.0202 - sequential_137_mse: 0.0352 - sequential_138_mse: 0.0153 - sequential_139_mse: 0.0435 - sequential_140_mse: 0.0206 - sequential_141_mse: 0.0187 - sequential_142_mse: 0.0479 - sequential_143_mse: 0.0296\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n","k: 16 patience: 7\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n","Test Loss no  3 : 23.30258909879531\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.4682 - sequential_128_mse: 0.0291 - sequential_129_mse: 0.0211 - sequential_130_mse: 0.0219 - sequential_131_mse: 0.0360 - sequential_132_mse: 0.0244 - sequential_133_mse: 0.0368 - sequential_134_mse: 0.0507 - sequential_135_mse: 0.0322 - sequential_136_mse: 0.0188 - sequential_137_mse: 0.0290 - sequential_138_mse: 0.0203 - sequential_139_mse: 0.0375 - sequential_140_mse: 0.0196 - sequential_141_mse: 0.0214 - sequential_142_mse: 0.0375 - sequential_143_mse: 0.0319\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n","k: 17 patience: 8\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n","Test Loss no  3 : 23.996659311141983\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 0.4764 - sequential_128_mse: 0.0294 - sequential_129_mse: 0.0288 - sequential_130_mse: 0.0324 - sequential_131_mse: 0.0339 - sequential_132_mse: 0.0287 - sequential_133_mse: 0.0388 - sequential_134_mse: 0.0498 - sequential_135_mse: 0.0354 - sequential_136_mse: 0.0193 - sequential_137_mse: 0.0281 - sequential_138_mse: 0.0164 - sequential_139_mse: 0.0413 - sequential_140_mse: 0.0178 - sequential_141_mse: 0.0159 - sequential_142_mse: 0.0347 - sequential_143_mse: 0.0257\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n","k: 18 patience: 9\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n","Test Loss no  3 : 22.905977531369963\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 0.4199 - sequential_128_mse: 0.0257 - sequential_129_mse: 0.0252 - sequential_130_mse: 0.0225 - sequential_131_mse: 0.0344 - sequential_132_mse: 0.0302 - sequential_133_mse: 0.0307 - sequential_134_mse: 0.0363 - sequential_135_mse: 0.0296 - sequential_136_mse: 0.0207 - sequential_137_mse: 0.0183 - sequential_138_mse: 0.0152 - sequential_139_mse: 0.0361 - sequential_140_mse: 0.0164 - sequential_141_mse: 0.0166 - sequential_142_mse: 0.0348 - sequential_143_mse: 0.0272\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n","k: 19 patience: 10\n","Iteration 5\n","global_indices [[9, 7, 1, 10, 8, 12, 4, 0, 6, 13, 5, 3], [6, 8, 12, 5, 9, 13, 2, 1, 3, 7, 11, 4], [11, 6, 4, 3, 1, 0, 9, 12, 10, 8, 7], [8, 2, 1, 5, 3, 9, 0, 6], [15, 8, 6, 10, 1, 14, 7, 3, 11, 5, 2, 12]]\n","input_train : (41, 120, 47)\n","output_train : (41, 16)\n","input_val : (14, 120, 47)\n","output_val : (14, 16)\n","input_test : (13, 120, 47)\n","output_test : (13, 16)\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 96ms/step - loss: 1.9506 - sequential_144_mse: 0.2183 - sequential_145_mse: 0.0691 - sequential_146_mse: 0.0394 - sequential_147_mse: 0.1197 - sequential_148_mse: 0.0740 - sequential_149_mse: 0.0765 - sequential_150_mse: 0.1047 - sequential_151_mse: 0.2129 - sequential_152_mse: 0.1407 - sequential_153_mse: 0.2210 - sequential_154_mse: 0.1477 - sequential_155_mse: 0.1104 - sequential_156_mse: 0.1369 - sequential_157_mse: 0.0890 - sequential_158_mse: 0.1427 - sequential_159_mse: 0.0476\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 729ms/step\n","k: 0 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 675ms/step\n","Test Loss no  4 : 50.04769631719408\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 1.3794 - sequential_144_mse: 0.0990 - sequential_145_mse: 0.0691 - sequential_146_mse: 0.0408 - sequential_147_mse: 0.1009 - sequential_148_mse: 0.0682 - sequential_149_mse: 0.0595 - sequential_150_mse: 0.0950 - sequential_151_mse: 0.1088 - sequential_152_mse: 0.1285 - sequential_153_mse: 0.1725 - sequential_154_mse: 0.0862 - sequential_155_mse: 0.0858 - sequential_156_mse: 0.0904 - sequential_157_mse: 0.0576 - sequential_158_mse: 0.0834 - sequential_159_mse: 0.0337\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n","k: 1 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n","Test Loss no  4 : 49.1762412373028\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - loss: 1.2162 - sequential_144_mse: 0.0858 - sequential_145_mse: 0.0573 - sequential_146_mse: 0.0341 - sequential_147_mse: 0.0825 - sequential_148_mse: 0.0597 - sequential_149_mse: 0.0521 - sequential_150_mse: 0.0745 - sequential_151_mse: 0.0965 - sequential_152_mse: 0.1183 - sequential_153_mse: 0.1561 - sequential_154_mse: 0.0693 - sequential_155_mse: 0.0785 - sequential_156_mse: 0.0782 - sequential_157_mse: 0.0511 - sequential_158_mse: 0.0815 - sequential_159_mse: 0.0407\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n","k: 2 patience: 1\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n","Test Loss no  4 : 50.32217372902147\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 1.1588 - sequential_144_mse: 0.0789 - sequential_145_mse: 0.0630 - sequential_146_mse: 0.0320 - sequential_147_mse: 0.0954 - sequential_148_mse: 0.0699 - sequential_149_mse: 0.0490 - sequential_150_mse: 0.0758 - sequential_151_mse: 0.0953 - sequential_152_mse: 0.1178 - sequential_153_mse: 0.1350 - sequential_154_mse: 0.0473 - sequential_155_mse: 0.0750 - sequential_156_mse: 0.0654 - sequential_157_mse: 0.0479 - sequential_158_mse: 0.0777 - sequential_159_mse: 0.0334\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n","k: 3 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n","Test Loss no  4 : 51.80344279437064\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 1.1450 - sequential_144_mse: 0.0735 - sequential_145_mse: 0.0565 - sequential_146_mse: 0.0329 - sequential_147_mse: 0.0876 - sequential_148_mse: 0.0659 - sequential_149_mse: 0.0396 - sequential_150_mse: 0.0739 - sequential_151_mse: 0.0899 - sequential_152_mse: 0.1242 - sequential_153_mse: 0.1296 - sequential_154_mse: 0.0540 - sequential_155_mse: 0.0868 - sequential_156_mse: 0.0683 - sequential_157_mse: 0.0501 - sequential_158_mse: 0.0795 - sequential_159_mse: 0.0330\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n","k: 4 patience: 0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n","Test Loss no  4 : 51.26877848884839\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 1.1053 - sequential_144_mse: 0.0755 - sequential_145_mse: 0.0527 - sequential_146_mse: 0.0292 - sequential_147_mse: 0.0881 - sequential_148_mse: 0.0479 - sequential_149_mse: 0.0427 - sequential_150_mse: 0.0820 - sequential_151_mse: 0.0856 - sequential_152_mse: 0.1167 - sequential_153_mse: 0.1349 - sequential_154_mse: 0.0574 - sequential_155_mse: 0.0737 - sequential_156_mse: 0.0694 - sequential_157_mse: 0.0532 - sequential_158_mse: 0.0651 - sequential_159_mse: 0.0312\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n","k: 5 patience: 1\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n","Test Loss no  4 : 48.81853096370007\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 0.9766 - sequential_144_mse: 0.0533 - sequential_145_mse: 0.0373 - sequential_146_mse: 0.0173 - sequential_147_mse: 0.0722 - sequential_148_mse: 0.0605 - sequential_149_mse: 0.0290 - sequential_150_mse: 0.0830 - sequential_151_mse: 0.0684 - sequential_152_mse: 0.1101 - sequential_153_mse: 0.1184 - sequential_154_mse: 0.0513 - sequential_155_mse: 0.0798 - sequential_156_mse: 0.0619 - sequential_157_mse: 0.0468 - sequential_158_mse: 0.0590 - sequential_159_mse: 0.0281\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n","k: 6 patience: 2\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n","Test Loss no  4 : 47.12213862570503\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 0.9388 - sequential_144_mse: 0.0622 - sequential_145_mse: 0.0417 - sequential_146_mse: 0.0169 - sequential_147_mse: 0.0701 - sequential_148_mse: 0.0527 - sequential_149_mse: 0.0359 - sequential_150_mse: 0.0592 - sequential_151_mse: 0.0697 - sequential_152_mse: 0.1033 - sequential_153_mse: 0.1160 - sequential_154_mse: 0.0488 - sequential_155_mse: 0.0745 - sequential_156_mse: 0.0578 - sequential_157_mse: 0.0443 - sequential_158_mse: 0.0592 - sequential_159_mse: 0.0265\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n","k: 7 patience: 3\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n","Test Loss no  4 : 46.259645975852706\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 0.8689 - sequential_144_mse: 0.0496 - sequential_145_mse: 0.0351 - sequential_146_mse: 0.0196 - sequential_147_mse: 0.0653 - sequential_148_mse: 0.0531 - sequential_149_mse: 0.0378 - sequential_150_mse: 0.0494 - sequential_151_mse: 0.0656 - sequential_152_mse: 0.0953 - sequential_153_mse: 0.1106 - sequential_154_mse: 0.0457 - sequential_155_mse: 0.0638 - sequential_156_mse: 0.0498 - sequential_157_mse: 0.0447 - sequential_158_mse: 0.0640 - sequential_159_mse: 0.0197\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n","k: 8 patience: 4\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n","Test Loss no  4 : 46.97336585863474\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 0.8431 - sequential_144_mse: 0.0486 - sequential_145_mse: 0.0365 - sequential_146_mse: 0.0212 - sequential_147_mse: 0.0687 - sequential_148_mse: 0.0477 - sequential_149_mse: 0.0297 - sequential_150_mse: 0.0553 - sequential_151_mse: 0.0584 - sequential_152_mse: 0.0941 - sequential_153_mse: 0.1097 - sequential_154_mse: 0.0356 - sequential_155_mse: 0.0769 - sequential_156_mse: 0.0471 - sequential_157_mse: 0.0362 - sequential_158_mse: 0.0529 - sequential_159_mse: 0.0245\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n","k: 9 patience: 5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n","Test Loss no  4 : 47.45093442759571\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.7525 - sequential_144_mse: 0.0389 - sequential_145_mse: 0.0344 - sequential_146_mse: 0.0162 - sequential_147_mse: 0.0587 - sequential_148_mse: 0.0492 - sequential_149_mse: 0.0226 - sequential_150_mse: 0.0569 - sequential_151_mse: 0.0532 - sequential_152_mse: 0.0916 - sequential_153_mse: 0.0954 - sequential_154_mse: 0.0352 - sequential_155_mse: 0.0584 - sequential_156_mse: 0.0428 - sequential_157_mse: 0.0335 - sequential_158_mse: 0.0495 - sequential_159_mse: 0.0161\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n","k: 10 patience: 6\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n","Test Loss no  4 : 45.97227334378038\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 0.7440 - sequential_144_mse: 0.0381 - sequential_145_mse: 0.0286 - sequential_146_mse: 0.0217 - sequential_147_mse: 0.0610 - sequential_148_mse: 0.0379 - sequential_149_mse: 0.0283 - sequential_150_mse: 0.0514 - sequential_151_mse: 0.0572 - sequential_152_mse: 0.0823 - sequential_153_mse: 0.0889 - sequential_154_mse: 0.0340 - sequential_155_mse: 0.0645 - sequential_156_mse: 0.0491 - sequential_157_mse: 0.0318 - sequential_158_mse: 0.0471 - sequential_159_mse: 0.0220\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n","k: 11 patience: 7\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n","Test Loss no  4 : 45.11005405167863\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.7833 - sequential_144_mse: 0.0468 - sequential_145_mse: 0.0364 - sequential_146_mse: 0.0255 - sequential_147_mse: 0.0601 - sequential_148_mse: 0.0442 - sequential_149_mse: 0.0259 - sequential_150_mse: 0.0440 - sequential_151_mse: 0.0531 - sequential_152_mse: 0.0987 - sequential_153_mse: 0.1062 - sequential_154_mse: 0.0368 - sequential_155_mse: 0.0676 - sequential_156_mse: 0.0463 - sequential_157_mse: 0.0263 - sequential_158_mse: 0.0487 - sequential_159_mse: 0.0167\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n","k: 12 patience: 8\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n","Test Loss no  4 : 46.8511594066168\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 0.6930 - sequential_144_mse: 0.0368 - sequential_145_mse: 0.0293 - sequential_146_mse: 0.0141 - sequential_147_mse: 0.0501 - sequential_148_mse: 0.0383 - sequential_149_mse: 0.0204 - sequential_150_mse: 0.0370 - sequential_151_mse: 0.0471 - sequential_152_mse: 0.0955 - sequential_153_mse: 0.0931 - sequential_154_mse: 0.0396 - sequential_155_mse: 0.0572 - sequential_156_mse: 0.0379 - sequential_157_mse: 0.0327 - sequential_158_mse: 0.0436 - sequential_159_mse: 0.0204\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n","k: 13 patience: 9\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n","Test Loss no  4 : 47.829110655454656\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 0.6490 - sequential_144_mse: 0.0338 - sequential_145_mse: 0.0285 - sequential_146_mse: 0.0136 - sequential_147_mse: 0.0580 - sequential_148_mse: 0.0494 - sequential_149_mse: 0.0187 - sequential_150_mse: 0.0325 - sequential_151_mse: 0.0371 - sequential_152_mse: 0.0921 - sequential_153_mse: 0.0819 - sequential_154_mse: 0.0283 - sequential_155_mse: 0.0580 - sequential_156_mse: 0.0359 - sequential_157_mse: 0.0284 - sequential_158_mse: 0.0392 - sequential_159_mse: 0.0136\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n","k: 14 patience: 10\n","average_loss: 28.87616305712837\n"]}]}]}