{"cells":[{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":162,"status":"ok","timestamp":1724917561829,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"},"user_tz":300},"id":"fT_A9oAGAepC"},"outputs":[],"source":["from google.colab import drive\n","import pandas as pd\n","from sklearn.preprocessing import StandardScaler\n","from scipy import stats\n","import numpy as np\n","import logging\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense\n","from datetime import datetime\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import datetime\n","import matplotlib.dates as mdates\n","import os"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1350,"status":"ok","timestamp":1724917563358,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"},"user_tz":300},"id":"TishlaEBAmlN","outputId":"e7670154-a47e-43af-b315-73ce4ccd4179"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Set the base path to the desired directory on Google Drive\n","base_path = '/content/drive/MyDrive/Study_1_Data/'"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1724917563359,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"},"user_tz":300},"id":"XeSwJ8oKAt2r"},"outputs":[],"source":["def read_csv(file_path):\n","    data = pd.read_csv(file_path)\n","    return data"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1724917563360,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"},"user_tz":300},"id":"BFHyHoFvA5bX"},"outputs":[],"source":["def process_data(data, columns_to_remove):\n","    processed_data = data.drop(columns=columns_to_remove).values\n","    return processed_data"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1724917563361,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"},"user_tz":300},"id":"QeoIWcudA94b"},"outputs":[],"source":["def construct_3d_array(base_dir, participants, simulations, columns_to_remove_hr, columns_to_remove_gsr, columns_to_remove_head, columns_to_remove_eye):\n","    \"\"\"\n","    Construct 3D array from CSV files.\n","    \"\"\"\n","    num_rows = 180  # Define number of rows to keep (last 180 rows)\n","    arrays_3d = []\n","\n","    for participant in participants:\n","        participant_id = f\"{int(participant):02d}\"  # Format participant number to two digits\n","\n","        valid_simulations = []\n","\n","        for simulation in simulations:\n","            hr_file_path = os.path.join(base_dir, participant_id, simulation, f'HR{simulation.capitalize()}.csv')\n","            gsr_file_path = os.path.join(base_dir, participant_id, simulation, f'EDA{simulation.capitalize()}_downsampled.csv')\n","            head_file_path = os.path.join(base_dir, participant_id, simulation, 'head_tracking_downsampled.csv')\n","            eye_file_path = os.path.join(base_dir, participant_id, simulation, 'eye_tracking_downsampled.csv')\n","\n","            # Check if all files exist\n","            if all(os.path.exists(file) for file in [hr_file_path, gsr_file_path, head_file_path, eye_file_path]):\n","                valid_simulations.append(simulation)\n","\n","        num_valid_simulations = len(valid_simulations)\n","        if num_valid_simulations == 0:\n","            continue  # Skip this participant if no valid simulations are found\n","\n","        array_3d = np.zeros((num_valid_simulations, num_rows, 47)) # hr=1, gsr=1, head=15-3, eye=41-8 total columns after removing columns= 48\n","\n","        for s_idx, simulation in enumerate(valid_simulations):\n","            # Process hr data\n","            hr_file_path = os.path.join(base_dir, participant_id, simulation, f'HR{simulation.capitalize()}.csv')\n","            hr_data = read_csv(hr_file_path)\n","            processed_hr_data = process_data(hr_data, columns_to_remove_hr)\n","            processed_hr_data = processed_hr_data[-num_rows:]  # Keep only the last 180 rows\n","\n","            # Process gsr data\n","            gsr_file_path = os.path.join(base_dir, participant_id, simulation, f'EDA{simulation.capitalize()}_downsampled.csv')\n","            gsr_data = read_csv(gsr_file_path)\n","            processed_gsr_data = process_data(gsr_data, columns_to_remove_gsr)\n","            processed_gsr_data = processed_gsr_data[-num_rows:]  # Keep only the last 180 rows\n","\n","            # Process head data\n","            head_file_path = os.path.join(base_dir, participant_id, simulation, 'head_tracking_downsampled.csv')\n","            head_data = read_csv(head_file_path)\n","            processed_head_data = process_data(head_data, columns_to_remove_head)\n","            processed_head_data = processed_head_data[-num_rows:]  # Keep only the last 180 rows\n","\n","            # Process eye data\n","            eye_file_path = os.path.join(base_dir, participant_id, simulation, 'eye_tracking_downsampled.csv')\n","            eye_data = read_csv(eye_file_path)\n","            processed_eye_data = process_data(eye_data, columns_to_remove_eye)\n","            processed_eye_data = processed_eye_data[-num_rows:]  # Keep only the last 180 rows\n","\n","            # Combine processed data\n","            combined_data = np.concatenate((processed_hr_data, processed_gsr_data, processed_head_data, processed_eye_data), axis=1)\n","\n","            array_3d[s_idx, :, :] = combined_data\n","\n","        arrays_3d.append(array_3d)\n","\n","    return arrays_3d\n"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1724917563362,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"},"user_tz":300},"id":"fqaeUGUDBCtT"},"outputs":[],"source":["sample_size=60\n","simulations = ['flat','noise','bumps']\n","participants = [str(i) for i in range(1, 27)]  # Participants 101 to 127\n","columns_to_remove_hr = []\n","columns_to_remove_gsr = []\n","columns_to_remove_eye = ['#Frame','Time', 'Unnamed: 40','ConvergenceValid','Left_Eye_Closed','Right_Eye_Closed','LocalGazeValid','WorldGazeValid']\n","columns_to_remove_head = ['#Frame','Time', 'Unnamed: 14']"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1724917563362,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"},"user_tz":300},"id":"vPQyeAYKBYaO"},"outputs":[],"source":["def calculate_total_ssq(csv_file):\n","    # Read the CSV file into a DataFrame\n","    df = pd.read_csv(csv_file)\n","    n_columns = [0, 5, 6, 7, 8, 14, 15]\n","    o_columns = [0, 1, 2, 3, 4, 8, 10]\n","    d_columns = [4, 7, 9, 10, 11, 12, 13]\n","\n","    # Calculate sum for each specified set of columns\n","    n_val = df.iloc[0, n_columns].sum()\n","    o_val = df.iloc[0, o_columns].sum()\n","    d_val = df.iloc[0, d_columns].sum()\n","\n","    return n_val, o_val, d_val"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1724917563363,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"},"user_tz":300},"id":"Xpn0lDt0BfvE"},"outputs":[],"source":["def merge_ssq_column(conditions,participants):\n","  directories = []\n","  total_ssq_values = []\n","  for participant in participants:\n","      participant = f\"{int(participant):02d}\"\n","      for condition in conditions:\n","          directory = os.path.join(base_path, participant, condition)\n","          directories.append(directory)\n","\n","  # Loop through each directory\n","  for directory in directories:\n","      # Check if the directory exists\n","      if not os.path.exists(directory):\n","          continue\n","\n","      # Get all CSV files in the directory that are named 'ssq.csv'\n","      csv_files = [file for file in os.listdir(directory) if file == 'ssq.csv']\n","\n","      # Loop through each CSV file\n","      for csv_file in csv_files:\n","          csv_path = os.path.join(directory, csv_file)\n","          df = pd.read_csv(csv_path)\n","          # n_val,o_val,d_val = calculate_total_ssq(csv_path)\n","          # total_ssq_values.append([n_val, o_val, d_val])\n","          ssq_values_participant = df.iloc[:, 0:17].values.flatten()   # Assuming SSQ values are in columns 1 to 16\n","          total_ssq_values.append(ssq_values_participant)\n","  ssq_array = np.array(total_ssq_values)\n","  return ssq_array\n","\n","def merge_total_ssq(conditions,participants):\n","  directories = []\n","  total_ssq_values = []\n","  for participant in participants:\n","      participant = f\"{int(participant):02d}\"\n","      for condition in conditions:\n","          directory = os.path.join(base_path, participant, condition)\n","          directories.append(directory)\n","\n","  # Loop through each directory\n","  for directory in directories:\n","      # Check if the directory exists\n","      if not os.path.exists(directory):\n","          continue\n","\n","      # Get all CSV files in the directory that are named 'ssq.csv'\n","      csv_files = [file for file in os.listdir(directory) if file == 'ssq.csv']\n","\n","      # Loop through each CSV file\n","      for csv_file in csv_files:\n","          csv_path = os.path.join(directory, csv_file)\n","          n_val,o_val,d_val = calculate_total_ssq(csv_path)\n","          total_ssq = (n_val+o_val+d_val) * 3.74\n","          df = pd.read_csv(csv_path)\n","          df[\"total-ssq\"] = total_ssq\n","          #print(\"csv_path: \",csv_path,\"   \",total_ssq)\n","          total_ssq_values.append(total_ssq)\n","  # Create a DataFrame from the list of total SSQ values\n","  df_total_ssq = pd.DataFrame(total_ssq_values, columns=[\"total-ssq\"])\n","  # Convert the list of total SSQ values to a NumPy array\n","  total_ssq_array = np.array(total_ssq_values)\n","  return total_ssq_array\n","\n"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"7k17K0HrCr6-","executionInfo":{"status":"ok","timestamp":1724917565757,"user_tz":300,"elapsed":2404,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"outputs":[],"source":["participants_group_1 = [1,3,4,11,25]\n","participants_group_2 = [2,7,8,9,17]\n","participants_group_3 = [10,12,13,22,23]\n","participants_group_4 = [5,14,18,20,21]\n","participants_group_5 = [6,15,16,19,24,26]\n","\n","arrays_group_1 = construct_3d_array(base_path, participants_group_1, simulations, columns_to_remove_hr, columns_to_remove_gsr, columns_to_remove_head, columns_to_remove_eye)\n","arrays_group_2 = construct_3d_array(base_path, participants_group_2, simulations, columns_to_remove_hr, columns_to_remove_gsr, columns_to_remove_head, columns_to_remove_eye)\n","arrays_group_3 = construct_3d_array(base_path, participants_group_3, simulations, columns_to_remove_hr, columns_to_remove_gsr, columns_to_remove_head, columns_to_remove_eye)\n","arrays_group_4 = construct_3d_array(base_path, participants_group_4, simulations, columns_to_remove_hr, columns_to_remove_gsr, columns_to_remove_head, columns_to_remove_eye)\n","arrays_group_5 = construct_3d_array(base_path, participants_group_5, simulations, columns_to_remove_hr, columns_to_remove_gsr, columns_to_remove_head, columns_to_remove_eye)"]},{"cell_type":"code","source":["# Concatenate arrays along the first axis\n","input_group_1 = np.concatenate(arrays_group_1, axis=0)\n","input_group_2 = np.concatenate(arrays_group_2, axis=0)\n","input_group_3 = np.concatenate(arrays_group_3, axis=0)\n","input_group_4 = np.concatenate(arrays_group_4, axis=0)\n","input_group_5 = np.concatenate(arrays_group_5, axis=0)\n"],"metadata":{"id":"w-JtDyHLsF9Y","executionInfo":{"status":"ok","timestamp":1724917565760,"user_tz":300,"elapsed":15,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["output_group_1=merge_ssq_column(simulations,participants_group_1)\n","output_group_2=merge_ssq_column(simulations,participants_group_2)\n","output_group_3=merge_ssq_column(simulations,participants_group_3)\n","output_group_4=merge_ssq_column(simulations,participants_group_4)\n","output_group_5=merge_ssq_column(simulations,participants_group_5)\n","\n","output_group_1 = np.squeeze(output_group_1)\n","output_group_2 = np.squeeze(output_group_2)\n","output_group_3 = np.squeeze(output_group_3)\n","output_group_4 = np.squeeze(output_group_4)\n","output_group_5 = np.squeeze(output_group_5)\n","\n","\n","output_total_ssq_group_1=merge_total_ssq(simulations,participants_group_1)\n","output_total_ssq_group_2=merge_total_ssq(simulations,participants_group_2)\n","output_total_ssq_group_3=merge_total_ssq(simulations,participants_group_3)\n","output_total_ssq_group_4=merge_total_ssq(simulations,participants_group_4)\n","output_total_ssq_group_5=merge_total_ssq(simulations,participants_group_5)\n","\n","output_total_ssq_group_1=output_total_ssq_group_1.reshape(-1, 1)\n","output_total_ssq_group_2=output_total_ssq_group_2.reshape(-1, 1)\n","output_total_ssq_group_3=output_total_ssq_group_3.reshape(-1, 1)\n","output_total_ssq_group_4=output_total_ssq_group_4.reshape(-1, 1)\n","output_total_ssq_group_5=output_total_ssq_group_5.reshape(-1, 1)\n","\n"],"metadata":{"id":"DbS4TcoLjrFf","executionInfo":{"status":"ok","timestamp":1724917567271,"user_tz":300,"elapsed":1521,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","execution_count":25,"metadata":{"id":"26ADF-kiC1EZ","executionInfo":{"status":"ok","timestamp":1724917567272,"user_tz":300,"elapsed":11,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"outputs":[],"source":["def scale_input_data(input_train, input_test):\n","    # Get the shape of the input data\n","    num_samples_train, time_steps_train, num_features = input_train.shape\n","    num_samples_test, time_steps_test, _ = input_test.shape\n","\n","    # Reshape the input data into 2D arrays\n","    flattened_train_data = input_train.reshape(-1, num_features)\n","    flattened_test_data = input_test.reshape(-1, num_features)\n","\n","    # Initialize a MinMaxScaler object\n","    scaler = MinMaxScaler()\n","\n","    # Fit the scaler on the training data and transform both train and test data\n","    scaled_train_data = scaler.fit_transform(flattened_train_data)\n","    scaled_test_data = scaler.transform(flattened_test_data)\n","\n","    # Reshape the scaled data back to its original shape\n","    scaled_train_data = scaled_train_data.reshape(num_samples_train, time_steps_train, num_features)\n","    scaled_test_data = scaled_test_data.reshape(num_samples_test, time_steps_test, num_features)\n","\n","    return scaled_train_data, scaled_test_data\n","\n","def scale_target_var(target_data):\n","    min_val, max_val = np.min(target_data, axis=0), np.max(target_data, axis=0)\n","    target_data = (target_data-min_val)/(max_val-min_val)\n","\n","    return target_data, min_val, max_val"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":212559,"status":"ok","timestamp":1724917779821,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"},"user_tz":300},"id":"E6ssyYUeDJwI","outputId":"c167961c-ea3e-4a9e-bdc2-9771f7b01e42"},"outputs":[{"output_type":"stream","name":"stdout","text":["input_train : (38, 120, 47)\n","output_train : (38, 16)\n","input_val : (14, 120, 47)\n","output_val : (14, 16)\n","input_test : (16, 120, 47)\n","output_test : (16, 16)\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 78ms/step - loss: 2.6892 - output_0_mse: 0.1902 - output_10_mse: 0.0694 - output_11_mse: 0.1285 - output_12_mse: 0.0938 - output_13_mse: 0.1814 - output_14_mse: 0.1033 - output_15_mse: 0.1924 - output_1_mse: 0.2947 - output_2_mse: 0.1895 - output_3_mse: 0.1127 - output_4_mse: 0.0979 - output_5_mse: 0.5489 - output_6_mse: 0.1779 - output_7_mse: 0.1183 - output_8_mse: 0.1008 - output_9_mse: 0.0896\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 899ms/step\n","k: 0 patience: 0\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 123ms/step - loss: 1.9446 - output_0_mse: 0.3118 - output_10_mse: 0.0720 - output_11_mse: 0.1096 - output_12_mse: 0.0608 - output_13_mse: 0.0839 - output_14_mse: 0.0692 - output_15_mse: 0.1156 - output_1_mse: 0.0846 - output_2_mse: 0.0554 - output_3_mse: 0.2792 - output_4_mse: 0.0704 - output_5_mse: 0.0718 - output_6_mse: 0.1259 - output_7_mse: 0.2087 - output_8_mse: 0.1283 - output_9_mse: 0.0976\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n","k: 1 patience: 0\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 126ms/step - loss: 1.5222 - output_0_mse: 0.1585 - output_10_mse: 0.0680 - output_11_mse: 0.0949 - output_12_mse: 0.0638 - output_13_mse: 0.1218 - output_14_mse: 0.0945 - output_15_mse: 0.0917 - output_1_mse: 0.1284 - output_2_mse: 0.0372 - output_3_mse: 0.1302 - output_4_mse: 0.0783 - output_5_mse: 0.0960 - output_6_mse: 0.1049 - output_7_mse: 0.0953 - output_8_mse: 0.0551 - output_9_mse: 0.1036\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n","k: 2 patience: 1\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 130ms/step - loss: 1.3556 - output_0_mse: 0.1331 - output_10_mse: 0.0548 - output_11_mse: 0.0838 - output_12_mse: 0.0548 - output_13_mse: 0.0932 - output_14_mse: 0.0728 - output_15_mse: 0.1075 - output_1_mse: 0.0827 - output_2_mse: 0.0524 - output_3_mse: 0.1113 - output_4_mse: 0.0362 - output_5_mse: 0.0811 - output_6_mse: 0.1085 - output_7_mse: 0.1489 - output_8_mse: 0.0493 - output_9_mse: 0.0853\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n","k: 3 patience: 2\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 1.1658 - output_0_mse: 0.0756 - output_10_mse: 0.0496 - output_11_mse: 0.0818 - output_12_mse: 0.0579 - output_13_mse: 0.0698 - output_14_mse: 0.0883 - output_15_mse: 0.0972 - output_1_mse: 0.0564 - output_2_mse: 0.0505 - output_3_mse: 0.0939 - output_4_mse: 0.0477 - output_5_mse: 0.0545 - output_6_mse: 0.1062 - output_7_mse: 0.1299 - output_8_mse: 0.0545 - output_9_mse: 0.0521\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n","k: 4 patience: 3\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 1.1404 - output_0_mse: 0.0880 - output_10_mse: 0.0596 - output_11_mse: 0.0864 - output_12_mse: 0.0666 - output_13_mse: 0.0635 - output_14_mse: 0.0677 - output_15_mse: 0.0912 - output_1_mse: 0.0571 - output_2_mse: 0.0387 - output_3_mse: 0.0942 - output_4_mse: 0.0528 - output_5_mse: 0.0679 - output_6_mse: 0.0860 - output_7_mse: 0.0886 - output_8_mse: 0.0479 - output_9_mse: 0.0841\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n","k: 5 patience: 4\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 1.1138 - output_0_mse: 0.0612 - output_10_mse: 0.0502 - output_11_mse: 0.0987 - output_12_mse: 0.0421 - output_13_mse: 0.0547 - output_14_mse: 0.0942 - output_15_mse: 0.0926 - output_1_mse: 0.0773 - output_2_mse: 0.0416 - output_3_mse: 0.1030 - output_4_mse: 0.0546 - output_5_mse: 0.0553 - output_6_mse: 0.0794 - output_7_mse: 0.1018 - output_8_mse: 0.0463 - output_9_mse: 0.0608\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n","k: 6 patience: 5\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 1.0965 - output_0_mse: 0.0706 - output_10_mse: 0.0541 - output_11_mse: 0.0768 - output_12_mse: 0.0633 - output_13_mse: 0.0447 - output_14_mse: 0.0723 - output_15_mse: 0.0970 - output_1_mse: 0.0600 - output_2_mse: 0.0352 - output_3_mse: 0.1153 - output_4_mse: 0.0566 - output_5_mse: 0.0591 - output_6_mse: 0.0884 - output_7_mse: 0.1066 - output_8_mse: 0.0479 - output_9_mse: 0.0486\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n","k: 7 patience: 6\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 0.9522 - output_0_mse: 0.0677 - output_10_mse: 0.0367 - output_11_mse: 0.0710 - output_12_mse: 0.0430 - output_13_mse: 0.0459 - output_14_mse: 0.0588 - output_15_mse: 0.0756 - output_1_mse: 0.0578 - output_2_mse: 0.0316 - output_3_mse: 0.0855 - output_4_mse: 0.0617 - output_5_mse: 0.0597 - output_6_mse: 0.0775 - output_7_mse: 0.0828 - output_8_mse: 0.0369 - output_9_mse: 0.0600\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n","k: 8 patience: 7\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.8753 - output_0_mse: 0.0550 - output_10_mse: 0.0367 - output_11_mse: 0.0741 - output_12_mse: 0.0395 - output_13_mse: 0.0444 - output_14_mse: 0.0618 - output_15_mse: 0.0674 - output_1_mse: 0.0524 - output_2_mse: 0.0367 - output_3_mse: 0.0837 - output_4_mse: 0.0469 - output_5_mse: 0.0545 - output_6_mse: 0.0670 - output_7_mse: 0.0662 - output_8_mse: 0.0354 - output_9_mse: 0.0536\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n","k: 9 patience: 8\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 0.8807 - output_0_mse: 0.0492 - output_10_mse: 0.0516 - output_11_mse: 0.0955 - output_12_mse: 0.0496 - output_13_mse: 0.0568 - output_14_mse: 0.0504 - output_15_mse: 0.0442 - output_1_mse: 0.0447 - output_2_mse: 0.0411 - output_3_mse: 0.0827 - output_4_mse: 0.0458 - output_5_mse: 0.0579 - output_6_mse: 0.0507 - output_7_mse: 0.0542 - output_8_mse: 0.0488 - output_9_mse: 0.0575\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n","k: 10 patience: 9\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.8573 - output_0_mse: 0.0565 - output_10_mse: 0.0477 - output_11_mse: 0.0699 - output_12_mse: 0.0447 - output_13_mse: 0.0468 - output_14_mse: 0.0499 - output_15_mse: 0.0475 - output_1_mse: 0.0538 - output_2_mse: 0.0367 - output_3_mse: 0.0842 - output_4_mse: 0.0404 - output_5_mse: 0.0540 - output_6_mse: 0.0590 - output_7_mse: 0.0707 - output_8_mse: 0.0340 - output_9_mse: 0.0614\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n","k: 11 patience: 10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 616ms/step\n","Test Loss: 20.021424226827648\n","input_train : (38, 120, 47)\n","output_train : (38, 16)\n","input_val : (14, 120, 47)\n","output_val : (14, 16)\n","input_test : (16, 120, 47)\n","output_test : (16, 16)\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 118ms/step - loss: 1.9587 - output_0_mse: 0.1074 - output_10_mse: 0.1056 - output_11_mse: 0.0804 - output_12_mse: 0.0635 - output_13_mse: 0.0731 - output_14_mse: 0.2469 - output_15_mse: 0.2448 - output_1_mse: 0.1552 - output_2_mse: 0.0850 - output_3_mse: 0.0928 - output_4_mse: 0.0819 - output_5_mse: 0.0826 - output_6_mse: 0.1558 - output_7_mse: 0.1108 - output_8_mse: 0.2311 - output_9_mse: 0.0417\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 634ms/step\n","k: 0 patience: 0\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 1.2642 - output_0_mse: 0.1129 - output_10_mse: 0.0556 - output_11_mse: 0.0715 - output_12_mse: 0.0929 - output_13_mse: 0.0748 - output_14_mse: 0.0905 - output_15_mse: 0.0824 - output_1_mse: 0.0771 - output_2_mse: 0.0438 - output_3_mse: 0.0832 - output_4_mse: 0.0381 - output_5_mse: 0.0597 - output_6_mse: 0.2067 - output_7_mse: 0.0974 - output_8_mse: 0.0254 - output_9_mse: 0.0522\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n","k: 1 patience: 0\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 1.1020 - output_0_mse: 0.0989 - output_10_mse: 0.0223 - output_11_mse: 0.0527 - output_12_mse: 0.0425 - output_13_mse: 0.0610 - output_14_mse: 0.0851 - output_15_mse: 0.1269 - output_1_mse: 0.0996 - output_2_mse: 0.0454 - output_3_mse: 0.0878 - output_4_mse: 0.0486 - output_5_mse: 0.0493 - output_6_mse: 0.1197 - output_7_mse: 0.0927 - output_8_mse: 0.0366 - output_9_mse: 0.0329\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n","k: 2 patience: 0\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 1.0313 - output_0_mse: 0.0958 - output_10_mse: 0.0301 - output_11_mse: 0.0779 - output_12_mse: 0.0623 - output_13_mse: 0.0394 - output_14_mse: 0.1149 - output_15_mse: 0.0985 - output_1_mse: 0.0584 - output_2_mse: 0.0428 - output_3_mse: 0.0931 - output_4_mse: 0.0311 - output_5_mse: 0.0579 - output_6_mse: 0.0892 - output_7_mse: 0.0781 - output_8_mse: 0.0320 - output_9_mse: 0.0297\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n","k: 3 patience: 1\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 1.0035 - output_0_mse: 0.0873 - output_10_mse: 0.0314 - output_11_mse: 0.0533 - output_12_mse: 0.0483 - output_13_mse: 0.0560 - output_14_mse: 0.1030 - output_15_mse: 0.0926 - output_1_mse: 0.0535 - output_2_mse: 0.0334 - output_3_mse: 0.0689 - output_4_mse: 0.0367 - output_5_mse: 0.0691 - output_6_mse: 0.1131 - output_7_mse: 0.0863 - output_8_mse: 0.0420 - output_9_mse: 0.0286\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n","k: 4 patience: 0\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.9423 - output_0_mse: 0.0708 - output_10_mse: 0.0229 - output_11_mse: 0.0601 - output_12_mse: 0.0300 - output_13_mse: 0.0321 - output_14_mse: 0.0807 - output_15_mse: 0.0967 - output_1_mse: 0.0571 - output_2_mse: 0.0346 - output_3_mse: 0.1068 - output_4_mse: 0.0355 - output_5_mse: 0.0683 - output_6_mse: 0.0987 - output_7_mse: 0.0877 - output_8_mse: 0.0320 - output_9_mse: 0.0283\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n","k: 5 patience: 1\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.8024 - output_0_mse: 0.0586 - output_10_mse: 0.0201 - output_11_mse: 0.0627 - output_12_mse: 0.0360 - output_13_mse: 0.0282 - output_14_mse: 0.0602 - output_15_mse: 0.0681 - output_1_mse: 0.0610 - output_2_mse: 0.0359 - output_3_mse: 0.0756 - output_4_mse: 0.0398 - output_5_mse: 0.0433 - output_6_mse: 0.0852 - output_7_mse: 0.0713 - output_8_mse: 0.0261 - output_9_mse: 0.0302\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n","k: 6 patience: 2\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.7346 - output_0_mse: 0.0670 - output_10_mse: 0.0178 - output_11_mse: 0.0536 - output_12_mse: 0.0281 - output_13_mse: 0.0314 - output_14_mse: 0.0522 - output_15_mse: 0.0828 - output_1_mse: 0.0529 - output_2_mse: 0.0319 - output_3_mse: 0.0502 - output_4_mse: 0.0295 - output_5_mse: 0.0321 - output_6_mse: 0.0878 - output_7_mse: 0.0677 - output_8_mse: 0.0258 - output_9_mse: 0.0239\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n","k: 7 patience: 3\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.8328 - output_0_mse: 0.0871 - output_10_mse: 0.0305 - output_11_mse: 0.0662 - output_12_mse: 0.0383 - output_13_mse: 0.0253 - output_14_mse: 0.0755 - output_15_mse: 0.0636 - output_1_mse: 0.0544 - output_2_mse: 0.0398 - output_3_mse: 0.0606 - output_4_mse: 0.0329 - output_5_mse: 0.0557 - output_6_mse: 0.0823 - output_7_mse: 0.0655 - output_8_mse: 0.0280 - output_9_mse: 0.0271\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n","k: 8 patience: 4\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.6528 - output_0_mse: 0.0500 - output_10_mse: 0.0186 - output_11_mse: 0.0480 - output_12_mse: 0.0310 - output_13_mse: 0.0221 - output_14_mse: 0.0654 - output_15_mse: 0.0682 - output_1_mse: 0.0511 - output_2_mse: 0.0318 - output_3_mse: 0.0386 - output_4_mse: 0.0356 - output_5_mse: 0.0295 - output_6_mse: 0.0655 - output_7_mse: 0.0518 - output_8_mse: 0.0182 - output_9_mse: 0.0275\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n","k: 9 patience: 5\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 0.6361 - output_0_mse: 0.0405 - output_10_mse: 0.0162 - output_11_mse: 0.0565 - output_12_mse: 0.0216 - output_13_mse: 0.0222 - output_14_mse: 0.0539 - output_15_mse: 0.0648 - output_1_mse: 0.0523 - output_2_mse: 0.0255 - output_3_mse: 0.0500 - output_4_mse: 0.0427 - output_5_mse: 0.0259 - output_6_mse: 0.0575 - output_7_mse: 0.0522 - output_8_mse: 0.0175 - output_9_mse: 0.0367\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n","k: 10 patience: 0\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.6517 - output_0_mse: 0.0472 - output_10_mse: 0.0163 - output_11_mse: 0.0547 - output_12_mse: 0.0332 - output_13_mse: 0.0252 - output_14_mse: 0.0644 - output_15_mse: 0.0759 - output_1_mse: 0.0563 - output_2_mse: 0.0202 - output_3_mse: 0.0448 - output_4_mse: 0.0336 - output_5_mse: 0.0442 - output_6_mse: 0.0565 - output_7_mse: 0.0434 - output_8_mse: 0.0175 - output_9_mse: 0.0183\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n","k: 11 patience: 1\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 140ms/step - loss: 0.5711 - output_0_mse: 0.0559 - output_10_mse: 0.0185 - output_11_mse: 0.0389 - output_12_mse: 0.0239 - output_13_mse: 0.0230 - output_14_mse: 0.0488 - output_15_mse: 0.0435 - output_1_mse: 0.0525 - output_2_mse: 0.0243 - output_3_mse: 0.0440 - output_4_mse: 0.0250 - output_5_mse: 0.0357 - output_6_mse: 0.0582 - output_7_mse: 0.0398 - output_8_mse: 0.0167 - output_9_mse: 0.0227\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n","k: 12 patience: 0\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 136ms/step - loss: 0.5282 - output_0_mse: 0.0414 - output_10_mse: 0.0175 - output_11_mse: 0.0468 - output_12_mse: 0.0293 - output_13_mse: 0.0268 - output_14_mse: 0.0385 - output_15_mse: 0.0622 - output_1_mse: 0.0383 - output_2_mse: 0.0181 - output_3_mse: 0.0424 - output_4_mse: 0.0271 - output_5_mse: 0.0250 - output_6_mse: 0.0496 - output_7_mse: 0.0320 - output_8_mse: 0.0189 - output_9_mse: 0.0143\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n","k: 13 patience: 1\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 143ms/step - loss: 0.5391 - output_0_mse: 0.0389 - output_10_mse: 0.0209 - output_11_mse: 0.0405 - output_12_mse: 0.0270 - output_13_mse: 0.0242 - output_14_mse: 0.0448 - output_15_mse: 0.0393 - output_1_mse: 0.0444 - output_2_mse: 0.0319 - output_3_mse: 0.0514 - output_4_mse: 0.0213 - output_5_mse: 0.0298 - output_6_mse: 0.0551 - output_7_mse: 0.0323 - output_8_mse: 0.0136 - output_9_mse: 0.0239\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n","k: 14 patience: 2\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 166ms/step - loss: 0.4818 - output_0_mse: 0.0323 - output_10_mse: 0.0251 - output_11_mse: 0.0291 - output_12_mse: 0.0285 - output_13_mse: 0.0208 - output_14_mse: 0.0431 - output_15_mse: 0.0476 - output_1_mse: 0.0366 - output_2_mse: 0.0195 - output_3_mse: 0.0402 - output_4_mse: 0.0246 - output_5_mse: 0.0230 - output_6_mse: 0.0428 - output_7_mse: 0.0355 - output_8_mse: 0.0151 - output_9_mse: 0.0181\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n","k: 15 patience: 0\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.5006 - output_0_mse: 0.0320 - output_10_mse: 0.0117 - output_11_mse: 0.0371 - output_12_mse: 0.0236 - output_13_mse: 0.0186 - output_14_mse: 0.0449 - output_15_mse: 0.0458 - output_1_mse: 0.0510 - output_2_mse: 0.0137 - output_3_mse: 0.0446 - output_4_mse: 0.0238 - output_5_mse: 0.0362 - output_6_mse: 0.0453 - output_7_mse: 0.0273 - output_8_mse: 0.0199 - output_9_mse: 0.0251\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n","k: 16 patience: 1\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.4838 - output_0_mse: 0.0484 - output_10_mse: 0.0112 - output_11_mse: 0.0558 - output_12_mse: 0.0245 - output_13_mse: 0.0186 - output_14_mse: 0.0343 - output_15_mse: 0.0535 - output_1_mse: 0.0279 - output_2_mse: 0.0082 - output_3_mse: 0.0439 - output_4_mse: 0.0221 - output_5_mse: 0.0219 - output_6_mse: 0.0457 - output_7_mse: 0.0226 - output_8_mse: 0.0250 - output_9_mse: 0.0203\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n","k: 17 patience: 2\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.4516 - output_0_mse: 0.0353 - output_10_mse: 0.0163 - output_11_mse: 0.0273 - output_12_mse: 0.0304 - output_13_mse: 0.0212 - output_14_mse: 0.0376 - output_15_mse: 0.0358 - output_1_mse: 0.0428 - output_2_mse: 0.0159 - output_3_mse: 0.0412 - output_4_mse: 0.0318 - output_5_mse: 0.0188 - output_6_mse: 0.0415 - output_7_mse: 0.0247 - output_8_mse: 0.0131 - output_9_mse: 0.0179\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n","k: 18 patience: 3\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.4412 - output_0_mse: 0.0374 - output_10_mse: 0.0106 - output_11_mse: 0.0439 - output_12_mse: 0.0140 - output_13_mse: 0.0171 - output_14_mse: 0.0400 - output_15_mse: 0.0333 - output_1_mse: 0.0471 - output_2_mse: 0.0069 - output_3_mse: 0.0321 - output_4_mse: 0.0222 - output_5_mse: 0.0227 - output_6_mse: 0.0513 - output_7_mse: 0.0341 - output_8_mse: 0.0164 - output_9_mse: 0.0120\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n","k: 19 patience: 4\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 0.4228 - output_0_mse: 0.0290 - output_10_mse: 0.0116 - output_11_mse: 0.0409 - output_12_mse: 0.0184 - output_13_mse: 0.0180 - output_14_mse: 0.0257 - output_15_mse: 0.0344 - output_1_mse: 0.0367 - output_2_mse: 0.0094 - output_3_mse: 0.0271 - output_4_mse: 0.0296 - output_5_mse: 0.0185 - output_6_mse: 0.0510 - output_7_mse: 0.0296 - output_8_mse: 0.0174 - output_9_mse: 0.0255\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n","k: 20 patience: 5\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.3930 - output_0_mse: 0.0312 - output_10_mse: 0.0118 - output_11_mse: 0.0320 - output_12_mse: 0.0176 - output_13_mse: 0.0182 - output_14_mse: 0.0361 - output_15_mse: 0.0362 - output_1_mse: 0.0363 - output_2_mse: 0.0103 - output_3_mse: 0.0296 - output_4_mse: 0.0179 - output_5_mse: 0.0159 - output_6_mse: 0.0460 - output_7_mse: 0.0220 - output_8_mse: 0.0170 - output_9_mse: 0.0151\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n","k: 21 patience: 6\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.3862 - output_0_mse: 0.0328 - output_10_mse: 0.0142 - output_11_mse: 0.0363 - output_12_mse: 0.0230 - output_13_mse: 0.0188 - output_14_mse: 0.0320 - output_15_mse: 0.0313 - output_1_mse: 0.0350 - output_2_mse: 0.0076 - output_3_mse: 0.0275 - output_4_mse: 0.0208 - output_5_mse: 0.0143 - output_6_mse: 0.0341 - output_7_mse: 0.0262 - output_8_mse: 0.0149 - output_9_mse: 0.0174\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n","k: 22 patience: 7\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.3840 - output_0_mse: 0.0207 - output_10_mse: 0.0134 - output_11_mse: 0.0339 - output_12_mse: 0.0173 - output_13_mse: 0.0167 - output_14_mse: 0.0372 - output_15_mse: 0.0296 - output_1_mse: 0.0322 - output_2_mse: 0.0103 - output_3_mse: 0.0246 - output_4_mse: 0.0328 - output_5_mse: 0.0313 - output_6_mse: 0.0291 - output_7_mse: 0.0193 - output_8_mse: 0.0200 - output_9_mse: 0.0157\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n","k: 23 patience: 8\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.4143 - output_0_mse: 0.0214 - output_10_mse: 0.0122 - output_11_mse: 0.0333 - output_12_mse: 0.0173 - output_13_mse: 0.0145 - output_14_mse: 0.0379 - output_15_mse: 0.0362 - output_1_mse: 0.0379 - output_2_mse: 0.0217 - output_3_mse: 0.0235 - output_4_mse: 0.0252 - output_5_mse: 0.0253 - output_6_mse: 0.0419 - output_7_mse: 0.0341 - output_8_mse: 0.0167 - output_9_mse: 0.0152\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n","k: 24 patience: 9\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.4183 - output_0_mse: 0.0261 - output_10_mse: 0.0068 - output_11_mse: 0.0252 - output_12_mse: 0.0337 - output_13_mse: 0.0186 - output_14_mse: 0.0305 - output_15_mse: 0.0369 - output_1_mse: 0.0287 - output_2_mse: 0.0154 - output_3_mse: 0.0641 - output_4_mse: 0.0271 - output_5_mse: 0.0102 - output_6_mse: 0.0380 - output_7_mse: 0.0234 - output_8_mse: 0.0167 - output_9_mse: 0.0173\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n","k: 25 patience: 10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 654ms/step\n","Test Loss: 27.13361180703054\n","input_train : (38, 120, 47)\n","output_train : (38, 16)\n","input_val : (14, 120, 47)\n","output_val : (14, 16)\n","input_test : (16, 120, 47)\n","output_test : (16, 16)\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 81ms/step - loss: 3.4558 - output_0_mse: 0.3948 - output_10_mse: 0.0779 - output_11_mse: 0.1159 - output_12_mse: 0.1340 - output_13_mse: 0.1523 - output_14_mse: 0.1828 - output_15_mse: 0.1065 - output_1_mse: 0.1376 - output_2_mse: 0.2611 - output_3_mse: 0.1274 - output_4_mse: 0.8040 - output_5_mse: 0.1739 - output_6_mse: 0.2701 - output_7_mse: 0.3997 - output_8_mse: 0.0666 - output_9_mse: 0.0515\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 612ms/step\n","k: 0 patience: 0\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 1.9243 - output_0_mse: 0.1665 - output_10_mse: 0.0861 - output_11_mse: 0.0999 - output_12_mse: 0.1186 - output_13_mse: 0.1014 - output_14_mse: 0.2706 - output_15_mse: 0.0971 - output_1_mse: 0.1093 - output_2_mse: 0.0751 - output_3_mse: 0.0998 - output_4_mse: 0.0792 - output_5_mse: 0.0720 - output_6_mse: 0.1360 - output_7_mse: 0.2437 - output_8_mse: 0.0868 - output_9_mse: 0.0821\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n","k: 1 patience: 0\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 1.5377 - output_0_mse: 0.1528 - output_10_mse: 0.0640 - output_11_mse: 0.1255 - output_12_mse: 0.0635 - output_13_mse: 0.0887 - output_14_mse: 0.1358 - output_15_mse: 0.1063 - output_1_mse: 0.0655 - output_2_mse: 0.0514 - output_3_mse: 0.1576 - output_4_mse: 0.0600 - output_5_mse: 0.0505 - output_6_mse: 0.1397 - output_7_mse: 0.1078 - output_8_mse: 0.0845 - output_9_mse: 0.0841\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n","k: 2 patience: 1\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 1.3804 - output_0_mse: 0.1414 - output_10_mse: 0.0744 - output_11_mse: 0.1303 - output_12_mse: 0.0556 - output_13_mse: 0.0761 - output_14_mse: 0.0927 - output_15_mse: 0.1004 - output_1_mse: 0.0902 - output_2_mse: 0.0328 - output_3_mse: 0.0734 - output_4_mse: 0.0629 - output_5_mse: 0.0680 - output_6_mse: 0.1365 - output_7_mse: 0.1093 - output_8_mse: 0.0794 - output_9_mse: 0.0571\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n","k: 3 patience: 2\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 1.3632 - output_0_mse: 0.1161 - output_10_mse: 0.0555 - output_11_mse: 0.1299 - output_12_mse: 0.0573 - output_13_mse: 0.0748 - output_14_mse: 0.0992 - output_15_mse: 0.1166 - output_1_mse: 0.0929 - output_2_mse: 0.0248 - output_3_mse: 0.0943 - output_4_mse: 0.0659 - output_5_mse: 0.0636 - output_6_mse: 0.1429 - output_7_mse: 0.1102 - output_8_mse: 0.0556 - output_9_mse: 0.0637\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n","k: 4 patience: 3\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 1.1418 - output_0_mse: 0.1004 - output_10_mse: 0.0563 - output_11_mse: 0.1246 - output_12_mse: 0.0675 - output_13_mse: 0.0651 - output_14_mse: 0.0811 - output_15_mse: 0.0706 - output_1_mse: 0.0737 - output_2_mse: 0.0286 - output_3_mse: 0.0784 - output_4_mse: 0.0490 - output_5_mse: 0.0545 - output_6_mse: 0.1030 - output_7_mse: 0.0954 - output_8_mse: 0.0426 - output_9_mse: 0.0511\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n","k: 5 patience: 4\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 1.0540 - output_0_mse: 0.0949 - output_10_mse: 0.0348 - output_11_mse: 0.0853 - output_12_mse: 0.0504 - output_13_mse: 0.0682 - output_14_mse: 0.0865 - output_15_mse: 0.0667 - output_1_mse: 0.0661 - output_2_mse: 0.0432 - output_3_mse: 0.0689 - output_4_mse: 0.0453 - output_5_mse: 0.0483 - output_6_mse: 0.1057 - output_7_mse: 0.1013 - output_8_mse: 0.0371 - output_9_mse: 0.0511\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n","k: 6 patience: 5\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 134ms/step - loss: 1.1120 - output_0_mse: 0.0795 - output_10_mse: 0.0545 - output_11_mse: 0.0762 - output_12_mse: 0.0511 - output_13_mse: 0.0619 - output_14_mse: 0.0841 - output_15_mse: 0.0726 - output_1_mse: 0.0831 - output_2_mse: 0.0264 - output_3_mse: 0.1035 - output_4_mse: 0.0603 - output_5_mse: 0.0650 - output_6_mse: 0.0991 - output_7_mse: 0.0941 - output_8_mse: 0.0524 - output_9_mse: 0.0482\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n","k: 7 patience: 6\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 147ms/step - loss: 1.0803 - output_0_mse: 0.0875 - output_10_mse: 0.0460 - output_11_mse: 0.1182 - output_12_mse: 0.0511 - output_13_mse: 0.0711 - output_14_mse: 0.0913 - output_15_mse: 0.0577 - output_1_mse: 0.0458 - output_2_mse: 0.0380 - output_3_mse: 0.0629 - output_4_mse: 0.0531 - output_5_mse: 0.0631 - output_6_mse: 0.1019 - output_7_mse: 0.1005 - output_8_mse: 0.0471 - output_9_mse: 0.0451\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n","k: 8 patience: 0\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 134ms/step - loss: 1.0578 - output_0_mse: 0.0764 - output_10_mse: 0.0444 - output_11_mse: 0.1297 - output_12_mse: 0.0470 - output_13_mse: 0.0505 - output_14_mse: 0.0889 - output_15_mse: 0.0669 - output_1_mse: 0.0784 - output_2_mse: 0.0273 - output_3_mse: 0.0722 - output_4_mse: 0.0518 - output_5_mse: 0.0570 - output_6_mse: 0.0959 - output_7_mse: 0.0955 - output_8_mse: 0.0326 - output_9_mse: 0.0432\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n","k: 9 patience: 1\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 164ms/step - loss: 0.8427 - output_0_mse: 0.0543 - output_10_mse: 0.0373 - output_11_mse: 0.0841 - output_12_mse: 0.0348 - output_13_mse: 0.0548 - output_14_mse: 0.0714 - output_15_mse: 0.0529 - output_1_mse: 0.0470 - output_2_mse: 0.0202 - output_3_mse: 0.0557 - output_4_mse: 0.0300 - output_5_mse: 0.0429 - output_6_mse: 0.0923 - output_7_mse: 0.0771 - output_8_mse: 0.0448 - output_9_mse: 0.0433\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n","k: 10 patience: 2\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.9524 - output_0_mse: 0.0692 - output_10_mse: 0.0467 - output_11_mse: 0.0899 - output_12_mse: 0.0775 - output_13_mse: 0.0430 - output_14_mse: 0.0913 - output_15_mse: 0.0425 - output_1_mse: 0.0609 - output_2_mse: 0.0242 - output_3_mse: 0.0719 - output_4_mse: 0.0512 - output_5_mse: 0.0434 - output_6_mse: 0.0942 - output_7_mse: 0.0700 - output_8_mse: 0.0400 - output_9_mse: 0.0366\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n","k: 11 patience: 3\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.8779 - output_0_mse: 0.0623 - output_10_mse: 0.0408 - output_11_mse: 0.0771 - output_12_mse: 0.0368 - output_13_mse: 0.0515 - output_14_mse: 0.0833 - output_15_mse: 0.0420 - output_1_mse: 0.0454 - output_2_mse: 0.0139 - output_3_mse: 0.0844 - output_4_mse: 0.0431 - output_5_mse: 0.0540 - output_6_mse: 0.0883 - output_7_mse: 0.0677 - output_8_mse: 0.0373 - output_9_mse: 0.0501\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n","k: 12 patience: 4\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.9070 - output_0_mse: 0.0519 - output_10_mse: 0.0349 - output_11_mse: 0.0924 - output_12_mse: 0.0579 - output_13_mse: 0.0751 - output_14_mse: 0.0845 - output_15_mse: 0.0364 - output_1_mse: 0.0565 - output_2_mse: 0.0253 - output_3_mse: 0.0810 - output_4_mse: 0.0456 - output_5_mse: 0.0429 - output_6_mse: 0.0643 - output_7_mse: 0.0652 - output_8_mse: 0.0334 - output_9_mse: 0.0597\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n","k: 13 patience: 5\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.7884 - output_0_mse: 0.0435 - output_10_mse: 0.0459 - output_11_mse: 0.0841 - output_12_mse: 0.0311 - output_13_mse: 0.0493 - output_14_mse: 0.0622 - output_15_mse: 0.0311 - output_1_mse: 0.0649 - output_2_mse: 0.0283 - output_3_mse: 0.0758 - output_4_mse: 0.0457 - output_5_mse: 0.0387 - output_6_mse: 0.0595 - output_7_mse: 0.0519 - output_8_mse: 0.0413 - output_9_mse: 0.0351\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n","k: 14 patience: 6\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.9438 - output_0_mse: 0.0714 - output_10_mse: 0.0385 - output_11_mse: 0.1123 - output_12_mse: 0.0376 - output_13_mse: 0.0625 - output_14_mse: 0.0663 - output_15_mse: 0.0482 - output_1_mse: 0.0503 - output_2_mse: 0.0308 - output_3_mse: 0.0683 - output_4_mse: 0.0552 - output_5_mse: 0.0484 - output_6_mse: 0.0778 - output_7_mse: 0.0852 - output_8_mse: 0.0534 - output_9_mse: 0.0375\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n","k: 15 patience: 7\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.7669 - output_0_mse: 0.0516 - output_10_mse: 0.0285 - output_11_mse: 0.0879 - output_12_mse: 0.0282 - output_13_mse: 0.0558 - output_14_mse: 0.0683 - output_15_mse: 0.0378 - output_1_mse: 0.0418 - output_2_mse: 0.0206 - output_3_mse: 0.0663 - output_4_mse: 0.0414 - output_5_mse: 0.0354 - output_6_mse: 0.0691 - output_7_mse: 0.0587 - output_8_mse: 0.0361 - output_9_mse: 0.0394\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n","k: 16 patience: 8\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.7949 - output_0_mse: 0.0432 - output_10_mse: 0.0383 - output_11_mse: 0.0798 - output_12_mse: 0.0324 - output_13_mse: 0.0575 - output_14_mse: 0.0655 - output_15_mse: 0.0428 - output_1_mse: 0.0440 - output_2_mse: 0.0278 - output_3_mse: 0.0669 - output_4_mse: 0.0541 - output_5_mse: 0.0363 - output_6_mse: 0.0597 - output_7_mse: 0.0527 - output_8_mse: 0.0378 - output_9_mse: 0.0561\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n","k: 17 patience: 0\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.6892 - output_0_mse: 0.0453 - output_10_mse: 0.0304 - output_11_mse: 0.0808 - output_12_mse: 0.0315 - output_13_mse: 0.0447 - output_14_mse: 0.0547 - output_15_mse: 0.0433 - output_1_mse: 0.0469 - output_2_mse: 0.0184 - output_3_mse: 0.0609 - output_4_mse: 0.0392 - output_5_mse: 0.0352 - output_6_mse: 0.0598 - output_7_mse: 0.0429 - output_8_mse: 0.0241 - output_9_mse: 0.0314\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n","k: 18 patience: 1\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.6966 - output_0_mse: 0.0368 - output_10_mse: 0.0299 - output_11_mse: 0.0758 - output_12_mse: 0.0233 - output_13_mse: 0.0461 - output_14_mse: 0.0737 - output_15_mse: 0.0507 - output_1_mse: 0.0481 - output_2_mse: 0.0172 - output_3_mse: 0.0537 - output_4_mse: 0.0422 - output_5_mse: 0.0425 - output_6_mse: 0.0585 - output_7_mse: 0.0463 - output_8_mse: 0.0248 - output_9_mse: 0.0272\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n","k: 19 patience: 2\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.6663 - output_0_mse: 0.0512 - output_10_mse: 0.0266 - output_11_mse: 0.0656 - output_12_mse: 0.0243 - output_13_mse: 0.0416 - output_14_mse: 0.0605 - output_15_mse: 0.0307 - output_1_mse: 0.0334 - output_2_mse: 0.0213 - output_3_mse: 0.0553 - output_4_mse: 0.0421 - output_5_mse: 0.0297 - output_6_mse: 0.0651 - output_7_mse: 0.0452 - output_8_mse: 0.0285 - output_9_mse: 0.0451\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n","k: 20 patience: 3\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.6389 - output_0_mse: 0.0491 - output_10_mse: 0.0238 - output_11_mse: 0.0809 - output_12_mse: 0.0148 - output_13_mse: 0.0384 - output_14_mse: 0.0574 - output_15_mse: 0.0387 - output_1_mse: 0.0335 - output_2_mse: 0.0186 - output_3_mse: 0.0449 - output_4_mse: 0.0447 - output_5_mse: 0.0211 - output_6_mse: 0.0573 - output_7_mse: 0.0603 - output_8_mse: 0.0271 - output_9_mse: 0.0285\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n","k: 21 patience: 4\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.5753 - output_0_mse: 0.0393 - output_10_mse: 0.0183 - output_11_mse: 0.0751 - output_12_mse: 0.0197 - output_13_mse: 0.0261 - output_14_mse: 0.0413 - output_15_mse: 0.0292 - output_1_mse: 0.0283 - output_2_mse: 0.0198 - output_3_mse: 0.0588 - output_4_mse: 0.0348 - output_5_mse: 0.0310 - output_6_mse: 0.0563 - output_7_mse: 0.0425 - output_8_mse: 0.0259 - output_9_mse: 0.0288\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n","k: 22 patience: 5\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.5441 - output_0_mse: 0.0381 - output_10_mse: 0.0123 - output_11_mse: 0.0714 - output_12_mse: 0.0171 - output_13_mse: 0.0329 - output_14_mse: 0.0391 - output_15_mse: 0.0389 - output_1_mse: 0.0318 - output_2_mse: 0.0239 - output_3_mse: 0.0576 - output_4_mse: 0.0297 - output_5_mse: 0.0193 - output_6_mse: 0.0477 - output_7_mse: 0.0343 - output_8_mse: 0.0199 - output_9_mse: 0.0300\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n","k: 23 patience: 6\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.4713 - output_0_mse: 0.0333 - output_10_mse: 0.0150 - output_11_mse: 0.0609 - output_12_mse: 0.0116 - output_13_mse: 0.0340 - output_14_mse: 0.0387 - output_15_mse: 0.0235 - output_1_mse: 0.0296 - output_2_mse: 0.0144 - output_3_mse: 0.0444 - output_4_mse: 0.0327 - output_5_mse: 0.0188 - output_6_mse: 0.0450 - output_7_mse: 0.0312 - output_8_mse: 0.0139 - output_9_mse: 0.0243\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n","k: 24 patience: 7\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 0.4829 - output_0_mse: 0.0361 - output_10_mse: 0.0137 - output_11_mse: 0.0608 - output_12_mse: 0.0106 - output_13_mse: 0.0224 - output_14_mse: 0.0373 - output_15_mse: 0.0178 - output_1_mse: 0.0250 - output_2_mse: 0.0216 - output_3_mse: 0.0530 - output_4_mse: 0.0369 - output_5_mse: 0.0341 - output_6_mse: 0.0410 - output_7_mse: 0.0298 - output_8_mse: 0.0127 - output_9_mse: 0.0302\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n","k: 25 patience: 8\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 130ms/step - loss: 0.5077 - output_0_mse: 0.0410 - output_10_mse: 0.0155 - output_11_mse: 0.0675 - output_12_mse: 0.0121 - output_13_mse: 0.0286 - output_14_mse: 0.0404 - output_15_mse: 0.0297 - output_1_mse: 0.0271 - output_2_mse: 0.0320 - output_3_mse: 0.0547 - output_4_mse: 0.0268 - output_5_mse: 0.0153 - output_6_mse: 0.0516 - output_7_mse: 0.0240 - output_8_mse: 0.0233 - output_9_mse: 0.0179\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n","k: 26 patience: 9\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 126ms/step - loss: 0.4936 - output_0_mse: 0.0383 - output_10_mse: 0.0129 - output_11_mse: 0.0523 - output_12_mse: 0.0208 - output_13_mse: 0.0206 - output_14_mse: 0.0335 - output_15_mse: 0.0210 - output_1_mse: 0.0303 - output_2_mse: 0.0202 - output_3_mse: 0.0708 - output_4_mse: 0.0283 - output_5_mse: 0.0215 - output_6_mse: 0.0346 - output_7_mse: 0.0324 - output_8_mse: 0.0242 - output_9_mse: 0.0321\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n","k: 27 patience: 10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 982ms/step\n","Test Loss: 41.843066902123276\n","input_train : (38, 120, 47)\n","output_train : (38, 16)\n","input_val : (14, 120, 47)\n","output_val : (14, 16)\n","input_test : (16, 120, 47)\n","output_test : (16, 16)\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 87ms/step - loss: 3.5164 - output_0_mse: 0.2800 - output_10_mse: 0.1168 - output_11_mse: 0.2158 - output_12_mse: 0.5604 - output_13_mse: 0.0642 - output_14_mse: 0.3111 - output_15_mse: 0.0569 - output_1_mse: 0.1122 - output_2_mse: 0.0852 - output_3_mse: 0.2492 - output_4_mse: 0.0466 - output_5_mse: 0.2995 - output_6_mse: 0.3135 - output_7_mse: 0.6716 - output_8_mse: 0.0534 - output_9_mse: 0.0801\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 675ms/step\n","k: 0 patience: 0\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 155ms/step - loss: 1.3833 - output_0_mse: 0.1191 - output_10_mse: 0.0151 - output_11_mse: 0.0904 - output_12_mse: 0.0587 - output_13_mse: 0.0472 - output_14_mse: 0.1223 - output_15_mse: 0.1036 - output_1_mse: 0.1519 - output_2_mse: 0.1095 - output_3_mse: 0.1314 - output_4_mse: 0.0509 - output_5_mse: 0.0409 - output_6_mse: 0.1114 - output_7_mse: 0.1698 - output_8_mse: 0.0243 - output_9_mse: 0.0366\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n","k: 1 patience: 0\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 138ms/step - loss: 1.1570 - output_0_mse: 0.1174 - output_10_mse: 0.0122 - output_11_mse: 0.0798 - output_12_mse: 0.0512 - output_13_mse: 0.0221 - output_14_mse: 0.0981 - output_15_mse: 0.0922 - output_1_mse: 0.0822 - output_2_mse: 0.0538 - output_3_mse: 0.1096 - output_4_mse: 0.0587 - output_5_mse: 0.0334 - output_6_mse: 0.1343 - output_7_mse: 0.1378 - output_8_mse: 0.0285 - output_9_mse: 0.0455\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n","k: 2 patience: 1\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 148ms/step - loss: 1.0626 - output_0_mse: 0.1153 - output_10_mse: 0.0132 - output_11_mse: 0.0812 - output_12_mse: 0.0458 - output_13_mse: 0.0101 - output_14_mse: 0.0721 - output_15_mse: 0.0502 - output_1_mse: 0.0702 - output_2_mse: 0.0926 - output_3_mse: 0.1088 - output_4_mse: 0.0332 - output_5_mse: 0.0390 - output_6_mse: 0.1197 - output_7_mse: 0.1223 - output_8_mse: 0.0372 - output_9_mse: 0.0518\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n","k: 3 patience: 0\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 153ms/step - loss: 0.9003 - output_0_mse: 0.1014 - output_10_mse: 0.0102 - output_11_mse: 0.0633 - output_12_mse: 0.0292 - output_13_mse: 0.0146 - output_14_mse: 0.0597 - output_15_mse: 0.0540 - output_1_mse: 0.0463 - output_2_mse: 0.0593 - output_3_mse: 0.0881 - output_4_mse: 0.0450 - output_5_mse: 0.0261 - output_6_mse: 0.1238 - output_7_mse: 0.1214 - output_8_mse: 0.0246 - output_9_mse: 0.0333\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n","k: 4 patience: 1\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.9398 - output_0_mse: 0.1019 - output_10_mse: 0.0133 - output_11_mse: 0.0897 - output_12_mse: 0.0337 - output_13_mse: 0.0151 - output_14_mse: 0.0929 - output_15_mse: 0.0491 - output_1_mse: 0.0628 - output_2_mse: 0.0632 - output_3_mse: 0.0744 - output_4_mse: 0.0361 - output_5_mse: 0.0241 - output_6_mse: 0.1126 - output_7_mse: 0.1083 - output_8_mse: 0.0182 - output_9_mse: 0.0446\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n","k: 5 patience: 2\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.8784 - output_0_mse: 0.0814 - output_10_mse: 0.0088 - output_11_mse: 0.0701 - output_12_mse: 0.0352 - output_13_mse: 0.0102 - output_14_mse: 0.0811 - output_15_mse: 0.0614 - output_1_mse: 0.0599 - output_2_mse: 0.0706 - output_3_mse: 0.0772 - output_4_mse: 0.0377 - output_5_mse: 0.0266 - output_6_mse: 0.0901 - output_7_mse: 0.1108 - output_8_mse: 0.0187 - output_9_mse: 0.0384\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n","k: 6 patience: 3\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.8949 - output_0_mse: 0.0789 - output_10_mse: 0.0093 - output_11_mse: 0.0943 - output_12_mse: 0.0338 - output_13_mse: 0.0052 - output_14_mse: 0.0947 - output_15_mse: 0.0453 - output_1_mse: 0.0702 - output_2_mse: 0.0682 - output_3_mse: 0.0739 - output_4_mse: 0.0331 - output_5_mse: 0.0210 - output_6_mse: 0.1016 - output_7_mse: 0.1001 - output_8_mse: 0.0180 - output_9_mse: 0.0472\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n","k: 7 patience: 0\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 0.9172 - output_0_mse: 0.1070 - output_10_mse: 0.0049 - output_11_mse: 0.0781 - output_12_mse: 0.0320 - output_13_mse: 0.0090 - output_14_mse: 0.1000 - output_15_mse: 0.0505 - output_1_mse: 0.0634 - output_2_mse: 0.0597 - output_3_mse: 0.0619 - output_4_mse: 0.0340 - output_5_mse: 0.0258 - output_6_mse: 0.1036 - output_7_mse: 0.1183 - output_8_mse: 0.0148 - output_9_mse: 0.0542\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n","k: 8 patience: 0\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.8121 - output_0_mse: 0.0773 - output_10_mse: 0.0128 - output_11_mse: 0.0752 - output_12_mse: 0.0236 - output_13_mse: 0.0084 - output_14_mse: 0.0932 - output_15_mse: 0.0452 - output_1_mse: 0.0415 - output_2_mse: 0.0743 - output_3_mse: 0.0678 - output_4_mse: 0.0282 - output_5_mse: 0.0314 - output_6_mse: 0.0767 - output_7_mse: 0.0982 - output_8_mse: 0.0202 - output_9_mse: 0.0381\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n","k: 9 patience: 1\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.7963 - output_0_mse: 0.0872 - output_10_mse: 0.0102 - output_11_mse: 0.0653 - output_12_mse: 0.0226 - output_13_mse: 0.0052 - output_14_mse: 0.0654 - output_15_mse: 0.0572 - output_1_mse: 0.0511 - output_2_mse: 0.0558 - output_3_mse: 0.0704 - output_4_mse: 0.0280 - output_5_mse: 0.0206 - output_6_mse: 0.1011 - output_7_mse: 0.1047 - output_8_mse: 0.0182 - output_9_mse: 0.0333\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n","k: 10 patience: 2\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.8008 - output_0_mse: 0.0807 - output_10_mse: 0.0064 - output_11_mse: 0.0773 - output_12_mse: 0.0296 - output_13_mse: 0.0090 - output_14_mse: 0.0693 - output_15_mse: 0.0727 - output_1_mse: 0.0526 - output_2_mse: 0.0659 - output_3_mse: 0.0661 - output_4_mse: 0.0321 - output_5_mse: 0.0186 - output_6_mse: 0.0782 - output_7_mse: 0.0927 - output_8_mse: 0.0140 - output_9_mse: 0.0355\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n","k: 11 patience: 3\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 0.7520 - output_0_mse: 0.0682 - output_10_mse: 0.0056 - output_11_mse: 0.0813 - output_12_mse: 0.0254 - output_13_mse: 0.0046 - output_14_mse: 0.0578 - output_15_mse: 0.0469 - output_1_mse: 0.0381 - output_2_mse: 0.0585 - output_3_mse: 0.0807 - output_4_mse: 0.0346 - output_5_mse: 0.0243 - output_6_mse: 0.0867 - output_7_mse: 0.0882 - output_8_mse: 0.0193 - output_9_mse: 0.0318\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n","k: 12 patience: 4\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 0.6859 - output_0_mse: 0.0588 - output_10_mse: 0.0064 - output_11_mse: 0.0733 - output_12_mse: 0.0322 - output_13_mse: 0.0047 - output_14_mse: 0.0533 - output_15_mse: 0.0429 - output_1_mse: 0.0483 - output_2_mse: 0.0480 - output_3_mse: 0.0744 - output_4_mse: 0.0367 - output_5_mse: 0.0212 - output_6_mse: 0.0605 - output_7_mse: 0.0686 - output_8_mse: 0.0246 - output_9_mse: 0.0320\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n","k: 13 patience: 5\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.6236 - output_0_mse: 0.0463 - output_10_mse: 0.0064 - output_11_mse: 0.0555 - output_12_mse: 0.0202 - output_13_mse: 0.0093 - output_14_mse: 0.0498 - output_15_mse: 0.0379 - output_1_mse: 0.0423 - output_2_mse: 0.0420 - output_3_mse: 0.0671 - output_4_mse: 0.0302 - output_5_mse: 0.0240 - output_6_mse: 0.0669 - output_7_mse: 0.0776 - output_8_mse: 0.0197 - output_9_mse: 0.0285\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n","k: 14 patience: 6\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.5579 - output_0_mse: 0.0550 - output_10_mse: 0.0056 - output_11_mse: 0.0532 - output_12_mse: 0.0213 - output_13_mse: 0.0066 - output_14_mse: 0.0416 - output_15_mse: 0.0365 - output_1_mse: 0.0386 - output_2_mse: 0.0354 - output_3_mse: 0.0584 - output_4_mse: 0.0263 - output_5_mse: 0.0148 - output_6_mse: 0.0613 - output_7_mse: 0.0501 - output_8_mse: 0.0206 - output_9_mse: 0.0326\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n","k: 15 patience: 7\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 0.5510 - output_0_mse: 0.0354 - output_10_mse: 0.0063 - output_11_mse: 0.0661 - output_12_mse: 0.0264 - output_13_mse: 0.0057 - output_14_mse: 0.0462 - output_15_mse: 0.0302 - output_1_mse: 0.0320 - output_2_mse: 0.0489 - output_3_mse: 0.0670 - output_4_mse: 0.0256 - output_5_mse: 0.0163 - output_6_mse: 0.0490 - output_7_mse: 0.0524 - output_8_mse: 0.0145 - output_9_mse: 0.0291\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n","k: 16 patience: 8\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.6284 - output_0_mse: 0.0459 - output_10_mse: 0.0098 - output_11_mse: 0.0904 - output_12_mse: 0.0287 - output_13_mse: 0.0104 - output_14_mse: 0.0425 - output_15_mse: 0.0409 - output_1_mse: 0.0382 - output_2_mse: 0.0521 - output_3_mse: 0.0486 - output_4_mse: 0.0323 - output_5_mse: 0.0195 - output_6_mse: 0.0615 - output_7_mse: 0.0618 - output_8_mse: 0.0126 - output_9_mse: 0.0332\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n","k: 17 patience: 9\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 168ms/step - loss: 0.5605 - output_0_mse: 0.0322 - output_10_mse: 0.0071 - output_11_mse: 0.0642 - output_12_mse: 0.0231 - output_13_mse: 0.0070 - output_14_mse: 0.0335 - output_15_mse: 0.0496 - output_1_mse: 0.0327 - output_2_mse: 0.0444 - output_3_mse: 0.0570 - output_4_mse: 0.0284 - output_5_mse: 0.0240 - output_6_mse: 0.0554 - output_7_mse: 0.0450 - output_8_mse: 0.0165 - output_9_mse: 0.0403\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n","k: 18 patience: 10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 956ms/step\n","Test Loss: 33.22755816454888\n","input_train : (38, 120, 47)\n","output_train : (38, 16)\n","input_val : (14, 120, 47)\n","output_val : (14, 16)\n","input_test : (16, 120, 47)\n","output_test : (16, 16)\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 146ms/step - loss: 2.0678 - output_0_mse: 0.1730 - output_10_mse: 0.0590 - output_11_mse: 0.1638 - output_12_mse: 0.1824 - output_13_mse: 0.0396 - output_14_mse: 0.1203 - output_15_mse: 0.1239 - output_1_mse: 0.1038 - output_2_mse: 0.0673 - output_3_mse: 0.1402 - output_4_mse: 0.0510 - output_5_mse: 0.1969 - output_6_mse: 0.1616 - output_7_mse: 0.3132 - output_8_mse: 0.1129 - output_9_mse: 0.0589\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 941ms/step\n","k: 0 patience: 0\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 130ms/step - loss: 2.0853 - output_0_mse: 0.1329 - output_10_mse: 0.0702 - output_11_mse: 0.0961 - output_12_mse: 0.0798 - output_13_mse: 0.0887 - output_14_mse: 0.1715 - output_15_mse: 0.3201 - output_1_mse: 0.1334 - output_2_mse: 0.1552 - output_3_mse: 0.1588 - output_4_mse: 0.0688 - output_5_mse: 0.0951 - output_6_mse: 0.1585 - output_7_mse: 0.1870 - output_8_mse: 0.0868 - output_9_mse: 0.0826\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n","k: 1 patience: 0\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 159ms/step - loss: 1.8304 - output_0_mse: 0.1046 - output_10_mse: 0.0735 - output_11_mse: 0.1121 - output_12_mse: 0.0741 - output_13_mse: 0.0955 - output_14_mse: 0.2271 - output_15_mse: 0.1492 - output_1_mse: 0.1474 - output_2_mse: 0.0850 - output_3_mse: 0.1785 - output_4_mse: 0.0515 - output_5_mse: 0.1025 - output_6_mse: 0.1450 - output_7_mse: 0.1338 - output_8_mse: 0.0653 - output_9_mse: 0.0852\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step\n","k: 2 patience: 0\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 1.4775 - output_0_mse: 0.1316 - output_10_mse: 0.0662 - output_11_mse: 0.0848 - output_12_mse: 0.0554 - output_13_mse: 0.0486 - output_14_mse: 0.0985 - output_15_mse: 0.0781 - output_1_mse: 0.0829 - output_2_mse: 0.0835 - output_3_mse: 0.0995 - output_4_mse: 0.0643 - output_5_mse: 0.1328 - output_6_mse: 0.1200 - output_7_mse: 0.1443 - output_8_mse: 0.0749 - output_9_mse: 0.1120\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n","k: 3 patience: 1\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 1.4125 - output_0_mse: 0.0964 - output_10_mse: 0.0305 - output_11_mse: 0.0539 - output_12_mse: 0.0552 - output_13_mse: 0.0663 - output_14_mse: 0.1379 - output_15_mse: 0.1138 - output_1_mse: 0.0763 - output_2_mse: 0.0969 - output_3_mse: 0.1051 - output_4_mse: 0.0542 - output_5_mse: 0.0717 - output_6_mse: 0.1560 - output_7_mse: 0.1390 - output_8_mse: 0.0854 - output_9_mse: 0.0740\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n","k: 4 patience: 2\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 1.4073 - output_0_mse: 0.1720 - output_10_mse: 0.0432 - output_11_mse: 0.0987 - output_12_mse: 0.0666 - output_13_mse: 0.0484 - output_14_mse: 0.0956 - output_15_mse: 0.0903 - output_1_mse: 0.0610 - output_2_mse: 0.0648 - output_3_mse: 0.1350 - output_4_mse: 0.0622 - output_5_mse: 0.0911 - output_6_mse: 0.1424 - output_7_mse: 0.1103 - output_8_mse: 0.0515 - output_9_mse: 0.0744\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n","k: 5 patience: 0\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 1.2134 - output_0_mse: 0.1092 - output_10_mse: 0.0337 - output_11_mse: 0.0982 - output_12_mse: 0.0622 - output_13_mse: 0.0526 - output_14_mse: 0.1068 - output_15_mse: 0.0730 - output_1_mse: 0.0836 - output_2_mse: 0.0443 - output_3_mse: 0.0927 - output_4_mse: 0.0557 - output_5_mse: 0.0700 - output_6_mse: 0.1024 - output_7_mse: 0.1025 - output_8_mse: 0.0597 - output_9_mse: 0.0670\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n","k: 6 patience: 1\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 1.1127 - output_0_mse: 0.0759 - output_10_mse: 0.0399 - output_11_mse: 0.0791 - output_12_mse: 0.0665 - output_13_mse: 0.0377 - output_14_mse: 0.0803 - output_15_mse: 0.0977 - output_1_mse: 0.0655 - output_2_mse: 0.0461 - output_3_mse: 0.0987 - output_4_mse: 0.0538 - output_5_mse: 0.0744 - output_6_mse: 0.1109 - output_7_mse: 0.0777 - output_8_mse: 0.0586 - output_9_mse: 0.0500\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n","k: 7 patience: 2\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.9919 - output_0_mse: 0.0694 - output_10_mse: 0.0465 - output_11_mse: 0.0750 - output_12_mse: 0.0655 - output_13_mse: 0.0374 - output_14_mse: 0.0923 - output_15_mse: 0.0618 - output_1_mse: 0.0676 - output_2_mse: 0.0442 - output_3_mse: 0.0697 - output_4_mse: 0.0470 - output_5_mse: 0.0555 - output_6_mse: 0.0971 - output_7_mse: 0.0727 - output_8_mse: 0.0453 - output_9_mse: 0.0448\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n","k: 8 patience: 3\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 1.0372 - output_0_mse: 0.0801 - output_10_mse: 0.0436 - output_11_mse: 0.0623 - output_12_mse: 0.0385 - output_13_mse: 0.0459 - output_14_mse: 0.0600 - output_15_mse: 0.0673 - output_1_mse: 0.0583 - output_2_mse: 0.0387 - output_3_mse: 0.1051 - output_4_mse: 0.0434 - output_5_mse: 0.0728 - output_6_mse: 0.1126 - output_7_mse: 0.1056 - output_8_mse: 0.0390 - output_9_mse: 0.0640\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n","k: 9 patience: 4\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.9869 - output_0_mse: 0.0652 - output_10_mse: 0.0550 - output_11_mse: 0.0470 - output_12_mse: 0.0455 - output_13_mse: 0.0385 - output_14_mse: 0.0845 - output_15_mse: 0.0609 - output_1_mse: 0.0706 - output_2_mse: 0.0366 - output_3_mse: 0.0959 - output_4_mse: 0.0527 - output_5_mse: 0.0706 - output_6_mse: 0.0792 - output_7_mse: 0.0793 - output_8_mse: 0.0507 - output_9_mse: 0.0546\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n","k: 10 patience: 5\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.8385 - output_0_mse: 0.0740 - output_10_mse: 0.0322 - output_11_mse: 0.0510 - output_12_mse: 0.0386 - output_13_mse: 0.0285 - output_14_mse: 0.0598 - output_15_mse: 0.0574 - output_1_mse: 0.0631 - output_2_mse: 0.0359 - output_3_mse: 0.0782 - output_4_mse: 0.0314 - output_5_mse: 0.0540 - output_6_mse: 0.0763 - output_7_mse: 0.0604 - output_8_mse: 0.0380 - output_9_mse: 0.0598\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n","k: 11 patience: 6\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.8646 - output_0_mse: 0.0626 - output_10_mse: 0.0372 - output_11_mse: 0.0543 - output_12_mse: 0.0319 - output_13_mse: 0.0268 - output_14_mse: 0.0699 - output_15_mse: 0.0546 - output_1_mse: 0.0570 - output_2_mse: 0.0407 - output_3_mse: 0.0760 - output_4_mse: 0.0436 - output_5_mse: 0.0513 - output_6_mse: 0.0735 - output_7_mse: 0.0585 - output_8_mse: 0.0511 - output_9_mse: 0.0756\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n","k: 12 patience: 7\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 0.8130 - output_0_mse: 0.0551 - output_10_mse: 0.0319 - output_11_mse: 0.0677 - output_12_mse: 0.0248 - output_13_mse: 0.0269 - output_14_mse: 0.0838 - output_15_mse: 0.0434 - output_1_mse: 0.0537 - output_2_mse: 0.0267 - output_3_mse: 0.0765 - output_4_mse: 0.0273 - output_5_mse: 0.0490 - output_6_mse: 0.0852 - output_7_mse: 0.0585 - output_8_mse: 0.0453 - output_9_mse: 0.0571\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n","k: 13 patience: 8\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.7867 - output_0_mse: 0.0516 - output_10_mse: 0.0286 - output_11_mse: 0.0492 - output_12_mse: 0.0302 - output_13_mse: 0.0251 - output_14_mse: 0.0583 - output_15_mse: 0.0529 - output_1_mse: 0.0590 - output_2_mse: 0.0335 - output_3_mse: 0.0785 - output_4_mse: 0.0371 - output_5_mse: 0.0378 - output_6_mse: 0.0879 - output_7_mse: 0.0549 - output_8_mse: 0.0485 - output_9_mse: 0.0537\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n","k: 14 patience: 9\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 0.6705 - output_0_mse: 0.0466 - output_10_mse: 0.0161 - output_11_mse: 0.0502 - output_12_mse: 0.0336 - output_13_mse: 0.0283 - output_14_mse: 0.0403 - output_15_mse: 0.0355 - output_1_mse: 0.0336 - output_2_mse: 0.0344 - output_3_mse: 0.0767 - output_4_mse: 0.0305 - output_5_mse: 0.0392 - output_6_mse: 0.0622 - output_7_mse: 0.0474 - output_8_mse: 0.0439 - output_9_mse: 0.0521\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n","k: 15 patience: 10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n","Test Loss: 21.40424706383349\n","average_loss: 28.725981632872767\n"]}],"source":["import tensorflow as tf\n","from keras.models import Model\n","from keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D, LSTM\n","from keras.optimizers import Adam\n","import numpy as np\n","import sklearn\n","\n","total_losses = []\n","\n","# Transformer Encoder\n","def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n","    x = LayerNormalization(epsilon=1e-6)(inputs)\n","    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n","    x = Dropout(dropout)(x)\n","    res = x + inputs\n","\n","    x = LayerNormalization(epsilon=1e-6)(res)\n","    x = Dense(ff_dim, activation=\"relu\")(x)\n","    x = Dropout(dropout)(x)\n","    x = Dense(inputs.shape[-1])(x)\n","    return x + res\n","\n","# Transformer Decoder\n","def transformer_decoder(inputs, enc_outputs, head_size, num_heads, ff_dim, dropout=0):\n","    # Self attention\n","    x = LayerNormalization(epsilon=1e-6)(inputs)\n","    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n","    x = Dropout(dropout)(x)\n","    res = x + inputs\n","\n","    # Cross attention\n","    x = LayerNormalization(epsilon=1e-6)(res)\n","    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, enc_outputs)\n","    x = Dropout(dropout)(x)\n","    res = x + res\n","\n","    # Feed forward\n","    x = LayerNormalization(epsilon=1e-6)(res)\n","    x = Dense(ff_dim, activation=\"relu\")(x)\n","    x = Dropout(dropout)(x)\n","    x = Dense(inputs.shape[-1])(x)\n","    return x + res\n","\n","# Soft Parameter Sharing Regularizer\n","def soft_parameter_sharing_regularizer(model, alpha=0.01):\n","    lstm_layers = [layer for layer in model.layers if isinstance(layer, LSTM)]\n","    if not lstm_layers:\n","        return 0.0\n","\n","    weights = [tf.convert_to_tensor(layer.weights[0]) for layer in lstm_layers]\n","\n","    shared_weights = tf.reduce_mean(tf.stack(weights), axis=0)\n","    return alpha * tf.reduce_sum([tf.reduce_sum(tf.square(w - shared_weights)) for w in weights])\n","\n","# Custom Loss Function\n","def custom_loss(y_true, y_pred, model):\n","    mse = tf.keras.losses.mse(y_true, y_pred)\n","    reg_loss = soft_parameter_sharing_regularizer(model)\n","    return mse + reg_loss\n","\n","# Model Definition\n","def get_soft_shared_model(input_shape1, input_shape2, output_shape):\n","    # Encoder input\n","    enc_inputs = Input(shape=(input_shape1, input_shape2))\n","\n","    # Encoder\n","    enc_outputs = transformer_encoder(enc_inputs, head_size=64, num_heads=4, ff_dim=256, dropout=0.2)\n","\n","    # Decoder input\n","    dec_inputs = Input(shape=(output_shape[1], input_shape2))\n","\n","    # Decoder\n","    dec_outputs = transformer_decoder(dec_inputs, enc_outputs, head_size=64, num_heads=4, ff_dim=256, dropout=0.2)\n","\n","    # Global pooling\n","    x = GlobalAveragePooling1D()(dec_outputs)\n","\n","    # Task-specific layers\n","    task_outputs = []\n","    task_layers = []\n","    for i in range(output_shape[1]):\n","        task_layer = Dense(256, activation='relu', name=f'task_layer_{i}')\n","        task_layers.append(task_layer)\n","        task_x = task_layer(x)\n","        task_x = Dropout(0.2)(task_x)\n","        output = Dense(1, name=f'output_{i}')(task_x)\n","        task_outputs.append(output)\n","\n","    model = Model([enc_inputs, dec_inputs], task_outputs)\n","\n","    return model\n","\n","# Training and Evaluation Loop\n","for iteration in range(5):\n","\n","  # Assuming input_groups and output_groups are lists of arrays for the 5 groups\n","    input_groups = [input_group_1, input_group_2, input_group_3, input_group_4, input_group_5]\n","    output_groups = [output_group_1, output_group_2, output_group_3, output_group_4, output_group_5]\n","    ssq_groups = [output_total_ssq_group_1, output_total_ssq_group_2, output_total_ssq_group_3, output_total_ssq_group_4, output_total_ssq_group_5]\n","\n","    # Initialize lists for storing training, validation, and test sets\n","    X_train, X_val, X_test = [], [], []\n","    y_train, y_val, y_test = [], [], []\n","    ssq_train, ssq_val, ssq_test = [], [], []\n","\n","    # Loop over each group\n","    for input_group, output_group, ssq_group in zip(input_groups, output_groups, ssq_groups):\n","        # Step 1: Split the group into a training/validation set (80%) and a test set (20%)\n","        X_temp, X_test_temp, y_temp, y_test_temp, ssq_temp, ssq_test_temp = train_test_split(\n","            input_group, output_group, ssq_group, test_size=0.2)\n","\n","        # Step 2: Split the training/validation set into a training set (60%) and a validation set (20%)\n","        X_train_temp, X_val_temp, y_train_temp, y_val_temp, ssq_train_temp, ssq_val_temp = train_test_split(\n","            X_temp, y_temp, ssq_temp, test_size=0.25)  # 0.25 * 0.8 = 0.2\n","\n","        # Step 3: Append the results to the corresponding lists\n","        X_train.append(X_train_temp)\n","        X_val.append(X_val_temp)\n","        X_test.append(X_test_temp)\n","\n","        y_train.append(y_train_temp)\n","        y_val.append(y_val_temp)\n","        y_test.append(y_test_temp)\n","\n","        ssq_train.append(ssq_train_temp)\n","        ssq_val.append(ssq_val_temp)\n","        ssq_test.append(ssq_test_temp)\n","\n","    # After the loop, concatenate the data for all groups if needed\n","    input_train = np.concatenate(X_train, axis=0)\n","    input_val = np.concatenate(X_val, axis=0)\n","    input_test = np.concatenate(X_test, axis=0)\n","\n","    output_train = np.concatenate(y_train, axis=0)\n","    output_val = np.concatenate(y_val, axis=0)\n","    output_test = np.concatenate(y_test, axis=0)\n","\n","    output_test_total_ssq = np.concatenate(ssq_test, axis=0)\n","    # input_train, input_test= scale_input_data(input_train[:, (60-sample_size):(180-sample_size), :], input_test[:, (60-sample_size):(180-sample_size), :])\n","    # output_train, min_val, max_val = scale_target_var(output_train)\n","\n","\n","    #  this section for scaling both train and validation set simultaniously\n","      # Step 1: Combine the training and validation sets\n","    combined_input = np.concatenate([input_train, input_val], axis=0)\n","    combined_output = np.concatenate([output_train, output_val], axis=0)\n","\n","    # Step 2: Scale the combined input data\n","    # Assuming scale_input_data scales the data based on the combined dataset\n","    combined_input, input_test = scale_input_data(\n","        combined_input[:, (60-sample_size):(180-sample_size), :],\n","        input_test[:, (60-sample_size):(180-sample_size), :]\n","    )\n","\n","    # Step 3: Scale the combined output data\n","    # Assuming scale_target_var scales the data and returns min_val, max_val\n","    combined_output, min_val, max_val = scale_target_var(combined_output)\n","\n","    # Step 4: Split the combined data back into training and validation sets\n","    # Use the original shapes of input_train and input_val to slice the combined arrays\n","    input_train = combined_input[:input_train.shape[0], :, :]\n","    input_val = combined_input[input_train.shape[0]:, :, :]\n","\n","    output_train = combined_output[:output_train.shape[0], :]\n","    output_val = combined_output[output_train.shape[0]:, :]\n","\n","\n","\n","    print(\"input_train :\", input_train.shape)\n","    print(\"output_train :\", output_train.shape)\n","    print(\"input_val :\", input_val.shape)\n","    print(\"output_val :\", output_val.shape)\n","    print(\"input_test :\", input_test.shape)\n","    print(\"output_test :\", output_test.shape)\n","\n","    # Reshape inputs\n","    train_input_reshaped = input_train.reshape((input_train.shape[0], input_train.shape[1], input_train.shape[2]))\n","    test_input_reshaped = input_test.reshape((input_test.shape[0], input_test.shape[1], input_test.shape[2]))\n","    val_input_reshaped = input_val.reshape((input_val.shape[0], input_val.shape[1], input_val.shape[2]))\n","\n","    # Create decoder inputs\n","    train_dec_input = np.zeros((train_input_reshaped.shape[0], output_train.shape[1], train_input_reshaped.shape[2]))\n","    val_dec_input = np.zeros((val_input_reshaped.shape[0], output_train.shape[1], val_input_reshaped.shape[2]))\n","    test_dec_input = np.zeros((test_input_reshaped.shape[0], output_test.shape[1], test_input_reshaped.shape[2]))\n","\n","    # Create the soft parameter sharing model\n","    model = get_soft_shared_model(input_train.shape[1], input_train.shape[2], output_train.shape)\n","\n","    # Compile the model with the custom loss function\n","    model.compile(optimizer=Adam(learning_rate=0.001),\n","                  loss=lambda y_true, y_pred: custom_loss(y_true, y_pred, model),\n","                  metrics=[['mse'] for _ in range(output_train.shape[1])])\n","\n","    best_val = 1000000\n","    patience = 0\n","    best_model = None\n","\n","    for k in range(200):\n","      # Train the model\n","      model.fit([train_input_reshaped, train_dec_input],\n","                [output_train[:, i] for i in range(output_train.shape[1])],\n","                epochs=1, batch_size=32, verbose=1)\n","\n","      # Predict validation data\n","      pred_val = np.array(model.predict([val_input_reshaped, val_dec_input]))\n","      pred_val = np.transpose(pred_val, (1, 0, 2)).squeeze()\n","      print(\"k:\", k, \"patience:\", patience)\n","\n","      # Evaluate the model\n","      losses = []\n","      for i in range(pred_val.shape[0]):\n","        total_ssq=0\n","        for j in [0,5,6,7,8,14,15]:\n","          total_ssq=np.sum(pred_val[i,j]*(max_val[j]-min_val[j]) + min_val[j])+total_ssq\n","\n","        for j in [0,1,2,3,4,8,10]:\n","          total_ssq=np.sum(pred_val[i,j]*(max_val[j]-min_val[j]) + min_val[j])+total_ssq\n","\n","        for j in [4,7,9,10,11,12,13]:\n","          total_ssq=np.sum(pred_val[i,j]*(max_val[j]-min_val[j]) + min_val[j])+total_ssq\n","        total_ssq=total_ssq*3.74\n","        output_val_ssq= output_val[i,0]\n","        #print(\"total_ssq\",total_ssq)\n","        #print(\"output_val_ssq\",output_val_ssq)\n","        loss = sklearn.metrics.mean_squared_error([total_ssq], [output_val_ssq], squared=False)\n","        losses.append(loss)\n","      tmp_val_loss = np.mean(losses)\n","      if tmp_val_loss <= best_val:\n","          best_val = tmp_val_loss\n","          patience = 0\n","          best_model = model\n","      else:\n","          patience +=1\n","          if patience > 10:\n","            break\n","\n","    # Predict test data\n","    pred_test = np.array(best_model.predict([test_input_reshaped, test_dec_input]))\n","    pred_test = np.transpose(pred_test, (1, 0, 2)).squeeze()\n","\n","    # Evaluate the model\n","    pred_total_ssq = []\n","    for i in range(pred_test.shape[0]):\n","      total_ssq=0\n","      for j in [0,5,6,7,8,14,15]:\n","        total_ssq=np.sum(pred_test[i,j]*(max_val[j]-min_val[j]) + min_val[j])+total_ssq\n","\n","      for j in [0,1,2,3,4,8,10]:\n","        total_ssq=np.sum(pred_test[i,j]*(max_val[j]-min_val[j]) + min_val[j])+total_ssq\n","\n","      for j in [4,7,9,10,11,12,13]:\n","        total_ssq=np.sum(pred_test[i,j]*(max_val[j]-min_val[j]) + min_val[j])+total_ssq\n","      total_ssq=total_ssq*3.74\n","\n","      pred_total_ssq.append(total_ssq)\n","    # Overall Test Loss\n","    loss = sklearn.metrics.mean_squared_error(pred_total_ssq, output_test_total_ssq, squared=False)\n","    print(\"Test Loss:\", loss)\n","    total_losses.append(loss)\n","\n","average_loss = sum(total_losses) / len(total_losses)\n","total_losses.append(average_loss)\n","print(\"average_loss:\", average_loss)\n"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOS6YGZ1CuuH7fRg8TcqI5q"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}