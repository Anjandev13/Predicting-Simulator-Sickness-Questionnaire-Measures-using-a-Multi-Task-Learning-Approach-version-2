{"cells":[{"cell_type":"code","execution_count":69,"metadata":{"id":"fT_A9oAGAepC","executionInfo":{"status":"ok","timestamp":1725436649350,"user_tz":300,"elapsed":167,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"outputs":[],"source":["from google.colab import drive\n","import pandas as pd\n","from sklearn.preprocessing import StandardScaler\n","from scipy import stats\n","import numpy as np\n","import logging\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense\n","from datetime import datetime\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import datetime\n","import matplotlib.dates as mdates\n","import os"]},{"cell_type":"code","execution_count":70,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TishlaEBAmlN","executionInfo":{"status":"ok","timestamp":1725436650344,"user_tz":300,"elapsed":826,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}},"outputId":"6f1268b8-9dee-442b-dd01-54ed5a2aea94"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Set the base path to the desired directory on Google Drive\n","base_path = '/content/drive/MyDrive/Study_1_Data/'"]},{"cell_type":"code","execution_count":71,"metadata":{"id":"XeSwJ8oKAt2r","executionInfo":{"status":"ok","timestamp":1725436650345,"user_tz":300,"elapsed":14,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"outputs":[],"source":["def read_csv(file_path):\n","    data = pd.read_csv(file_path)\n","    return data"]},{"cell_type":"code","execution_count":72,"metadata":{"id":"BFHyHoFvA5bX","executionInfo":{"status":"ok","timestamp":1725436650346,"user_tz":300,"elapsed":13,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"outputs":[],"source":["def process_data(data, columns_to_remove):\n","    processed_data = data.drop(columns=columns_to_remove).values\n","    return processed_data"]},{"cell_type":"code","execution_count":73,"metadata":{"id":"QeoIWcudA94b","executionInfo":{"status":"ok","timestamp":1725436650347,"user_tz":300,"elapsed":13,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"outputs":[],"source":["\n","def construct_3d_array(base_dir, participants, simulations, columns_to_remove_hr, columns_to_remove_gsr, columns_to_remove_head, columns_to_remove_eye):\n","    \"\"\"\n","    Construct 3D array from CSV files.\n","    \"\"\"\n","    num_rows = 180  # Define number of rows to keep (last 180 rows)\n","    arrays_3d = []\n","\n","    for participant in participants:\n","        participant_id = f\"{int(participant):02d}\"  # Format participant number to two digits\n","\n","        valid_simulations = []\n","\n","        for simulation in simulations:\n","            hr_file_path = os.path.join(base_dir, participant_id, simulation, f'HR{simulation.capitalize()}.csv')\n","            gsr_file_path = os.path.join(base_dir, participant_id, simulation, f'EDA{simulation.capitalize()}_downsampled.csv')\n","            head_file_path = os.path.join(base_dir, participant_id, simulation, 'head_tracking_downsampled.csv')\n","            eye_file_path = os.path.join(base_dir, participant_id, simulation, 'eye_tracking_downsampled.csv')\n","\n","            # Check if all files exist\n","            if all(os.path.exists(file) for file in [hr_file_path, gsr_file_path, head_file_path, eye_file_path]):\n","                valid_simulations.append(simulation)\n","\n","        num_valid_simulations = len(valid_simulations)\n","        if num_valid_simulations == 0:\n","            continue  # Skip this participant if no valid simulations are found\n","\n","        array_3d = np.zeros((num_valid_simulations, num_rows, 47)) # hr=1, gsr=1, head=15-3, eye=41-8 total columns after removing columns= 48\n","\n","        for s_idx, simulation in enumerate(valid_simulations):\n","            # Process hr data\n","            hr_file_path = os.path.join(base_dir, participant_id, simulation, f'HR{simulation.capitalize()}.csv')\n","            hr_data = read_csv(hr_file_path)\n","            processed_hr_data = process_data(hr_data, columns_to_remove_hr)\n","            processed_hr_data = processed_hr_data[-num_rows:]  # Keep only the last 180 rows\n","\n","            # Process gsr data\n","            gsr_file_path = os.path.join(base_dir, participant_id, simulation, f'EDA{simulation.capitalize()}_downsampled.csv')\n","            gsr_data = read_csv(gsr_file_path)\n","            processed_gsr_data = process_data(gsr_data, columns_to_remove_gsr)\n","            processed_gsr_data = processed_gsr_data[-num_rows:]  # Keep only the last 180 rows\n","\n","            # Process head data\n","            head_file_path = os.path.join(base_dir, participant_id, simulation, 'head_tracking_downsampled.csv')\n","            head_data = read_csv(head_file_path)\n","            processed_head_data = process_data(head_data, columns_to_remove_head)\n","            processed_head_data = processed_head_data[-num_rows:]  # Keep only the last 180 rows\n","\n","            # Process eye data\n","            eye_file_path = os.path.join(base_dir, participant_id, simulation, 'eye_tracking_downsampled.csv')\n","            eye_data = read_csv(eye_file_path)\n","            processed_eye_data = process_data(eye_data, columns_to_remove_eye)\n","            processed_eye_data = processed_eye_data[-num_rows:]  # Keep only the last 180 rows\n","\n","            # Combine processed data\n","            combined_data = np.concatenate((processed_hr_data, processed_gsr_data, processed_head_data, processed_eye_data), axis=1)\n","\n","\n","\n","            array_3d[s_idx, :, :] = combined_data\n","\n","            arrays_3d.append(array_3d)\n","    return arrays_3d\n"]},{"cell_type":"code","source":["sample_size=60\n","# simulations_train = ['noise','bumps']\n","# simulations_test=['flat']\n","# val_indices = [4, 10, 11, 26, 28, 31, 33, 37] # for flat\n","# train_indices = [0, 1, 2, 3, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 29, 30, 32, 34, 35, 36, 38, 39, 40, 41] # for flat\n","\n","\n","# simulations_test=['noise']\n","# simulations_train = ['flat','bumps']\n","# val_indices = [7, 15, 17, 19, 28, 31, 32, 42, 44, 48] # for noise\n","# train_indices = [0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 16, 18, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 33, 34, 35, 36, 37, 38, 39, 40, 41, 43, 45, 46, 47] # for noise\n","\n","simulations_test=['bumps']\n","simulations_train = ['flat','noise']\n","val_indices = [1, 12, 16, 18, 22, 26, 28, 37, 41] # for speedbumps\n","train_indices = [0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 17, 19, 20, 21, 23, 24, 25, 27, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 42, 43, 44] # for speedbumps"],"metadata":{"id":"0D8UyttEAUzU","executionInfo":{"status":"ok","timestamp":1725436650347,"user_tz":300,"elapsed":12,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"execution_count":74,"outputs":[]},{"cell_type":"code","execution_count":75,"metadata":{"id":"fqaeUGUDBCtT","executionInfo":{"status":"ok","timestamp":1725436650347,"user_tz":300,"elapsed":12,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"outputs":[],"source":["participants = [str(i) for i in range(1, 27)]  # Participants 101 to 123\n","columns_to_remove_hr = []\n","columns_to_remove_gsr = []\n","columns_to_remove_eye = ['#Frame','Time', 'Unnamed: 40','ConvergenceValid','Left_Eye_Closed','Right_Eye_Closed','LocalGazeValid','WorldGazeValid']\n","columns_to_remove_head = ['#Frame','Time', 'Unnamed: 14']"]},{"cell_type":"code","execution_count":76,"metadata":{"id":"DwS92BItBL4B","executionInfo":{"status":"ok","timestamp":1725436652702,"user_tz":300,"elapsed":2364,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"outputs":[],"source":["arrays_train = construct_3d_array(base_path, participants, simulations_train, columns_to_remove_hr, columns_to_remove_gsr, columns_to_remove_head, columns_to_remove_eye)\n","arrays_test = construct_3d_array(base_path, participants, simulations_test, columns_to_remove_hr, columns_to_remove_gsr, columns_to_remove_head, columns_to_remove_eye)"]},{"cell_type":"code","source":[],"metadata":{"id":"98DbLCnXafN5","executionInfo":{"status":"ok","timestamp":1725436652703,"user_tz":300,"elapsed":12,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"execution_count":76,"outputs":[]},{"cell_type":"code","execution_count":77,"metadata":{"id":"GgAM9zg_BRe8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725436652703,"user_tz":300,"elapsed":11,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}},"outputId":"1448de1e-4b58-467c-d155-7461a37eb8b5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of the final concatenated 3D array: (83, 180, 47)\n","Shape of the final concatenated 3D array: (23, 180, 47)\n"]}],"source":["# Concatenate arrays along the first axis\n","input_train = np.concatenate(arrays_train, axis=0)\n","input_test = np.concatenate(arrays_test, axis=0)\n","\n","# Display the shape of the final concatenated 3D array\n","print(f\"Shape of the final concatenated 3D array: {input_train.shape}\")\n","print(f\"Shape of the final concatenated 3D array: {input_test.shape}\")"]},{"cell_type":"code","execution_count":78,"metadata":{"id":"vPQyeAYKBYaO","executionInfo":{"status":"ok","timestamp":1725436652704,"user_tz":300,"elapsed":8,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"outputs":[],"source":["def calculate_total_ssq(csv_file):\n","    # Read the CSV file into a DataFrame\n","    df = pd.read_csv(csv_file)\n","    n_columns = [0, 5, 6, 7, 8, 14, 15]\n","    o_columns = [0, 1, 2, 3, 4, 8, 10]\n","    d_columns = [4, 7, 9, 10, 11, 12, 13]\n","\n","    # Calculate sum for each specified set of columns\n","    n_val = df.iloc[:, n_columns].sum(axis=1)\n","    o_val = df.iloc[:, o_columns].sum(axis=1)\n","    d_val = df.iloc[:, d_columns].sum(axis=1)\n","\n","    return n_val,o_val,d_val"]},{"cell_type":"code","execution_count":79,"metadata":{"id":"Xpn0lDt0BfvE","executionInfo":{"status":"ok","timestamp":1725436652704,"user_tz":300,"elapsed":8,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"outputs":[],"source":["def merge_ssq_column(conditions,participants):\n","  directories = []\n","  total_ssq_values = []\n","  for participant in participants:\n","      participant = f\"{int(participant):02d}\"\n","      for condition in conditions:\n","          directory = os.path.join(base_path, participant, condition)\n","          directories.append(directory)\n","\n","  # Loop through each directory\n","  for directory in directories:\n","      # Check if the directory exists\n","      if not os.path.exists(directory):\n","          continue\n","\n","      # Get all CSV files in the directory that are named 'ssq.csv'\n","      csv_files = [file for file in os.listdir(directory) if file == 'ssq.csv']\n","\n","      # Loop through each CSV file\n","      for csv_file in csv_files:\n","          csv_path = os.path.join(directory, csv_file)\n","          df = pd.read_csv(csv_path)\n","          n_val,o_val,d_val = calculate_total_ssq(csv_path)\n","          total_ssq_values.append([n_val, o_val, d_val])\n","          #ssq_values_participant = df.iloc[:, 0:17].values.flatten()   # Assuming SSQ values are in columns 1 to 16\n","          #total_ssq_values.append(ssq_values_participant)\n","  ssq_array = np.array(total_ssq_values)\n","  return ssq_array\n","\n","def merge_total_ssq(conditions,participants):\n","  directories = []\n","  total_ssq_values = []\n","  for participant in participants:\n","      participant = f\"{int(participant):02d}\"\n","      for condition in conditions:\n","          directory = os.path.join(base_path, participant, condition)\n","          directories.append(directory)\n","\n","  # Loop through each directory\n","  for directory in directories:\n","      # Check if the directory exists\n","      if not os.path.exists(directory):\n","          continue\n","\n","      # Get all CSV files in the directory that are named 'ssq.csv'\n","      csv_files = [file for file in os.listdir(directory) if file == 'ssq.csv']\n","\n","      # Loop through each CSV file\n","      for csv_file in csv_files:\n","          csv_path = os.path.join(directory, csv_file)\n","          n_val,o_val,d_val = calculate_total_ssq(csv_path)\n","          total_ssq = (n_val+o_val+d_val) * 3.74\n","          df = pd.read_csv(csv_path)\n","          df[\"total-ssq\"] = total_ssq\n","          #print(\"csv_path: \",csv_path,\"   \",total_ssq)\n","          total_ssq_values.append(total_ssq)\n","  # Create a DataFrame from the list of total SSQ values\n","  df_total_ssq = pd.DataFrame(total_ssq_values, columns=[\"total-ssq\"])\n","  # Convert the list of total SSQ values to a NumPy array\n","  total_ssq_array = np.array(total_ssq_values)\n","  return total_ssq_array\n","\n"]},{"cell_type":"code","execution_count":80,"metadata":{"id":"7k17K0HrCr6-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725436655110,"user_tz":300,"elapsed":2413,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}},"outputId":"cb48284a-adae-462c-fb8b-e47a98e5389e"},"outputs":[{"output_type":"stream","name":"stdout","text":["(45, 3) (23, 3) (45, 1) (23, 1)\n"]}],"source":["output_train=merge_ssq_column(simulations_train,participants)\n","output_train = np.squeeze(output_train)\n","output_test=merge_ssq_column(simulations_test,participants)\n","output_test = np.squeeze(output_test)\n","output_train_total_ssq=merge_total_ssq(simulations_train,participants)\n","output_test_total_ssq=merge_total_ssq(simulations_test,participants)\n","output_train_total_ssq=output_train_total_ssq.reshape(-1, 1)\n","output_test_total_ssq=output_test_total_ssq.reshape(-1, 1)\n","print(output_train.shape,output_test.shape,output_train_total_ssq.shape,output_test_total_ssq.shape)\n","# print(output_train)\n","# print(output_train_total_ssq)\n"]},{"cell_type":"code","source":["input_train.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RKwH-BAwaA3t","executionInfo":{"status":"ok","timestamp":1725436655110,"user_tz":300,"elapsed":15,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}},"outputId":"7a893398-b301-48e8-ff98-e1cf0544d2bc"},"execution_count":81,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(83, 180, 47)"]},"metadata":{},"execution_count":81}]},{"cell_type":"code","execution_count":82,"metadata":{"id":"26ADF-kiC1EZ","executionInfo":{"status":"ok","timestamp":1725436655111,"user_tz":300,"elapsed":13,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}}},"outputs":[],"source":["def scale_input_data(input_train, input_test):\n","    # Get the shape of the input data\n","    num_samples_train, time_steps_train, num_features = input_train.shape\n","    num_samples_test, time_steps_test, _ = input_test.shape\n","\n","    # Reshape the input data into 2D arrays\n","    flattened_train_data = input_train.reshape(-1, num_features)\n","    flattened_test_data = input_test.reshape(-1, num_features)\n","\n","    # Initialize a MinMaxScaler object\n","    scaler = MinMaxScaler()\n","\n","    # Fit the scaler on the training data and transform both train and test data\n","    scaled_train_data = scaler.fit_transform(flattened_train_data)\n","    scaled_test_data = scaler.transform(flattened_test_data)\n","\n","    # Reshape the scaled data back to its original shape\n","    scaled_train_data = scaled_train_data.reshape(num_samples_train, time_steps_train, num_features)\n","    scaled_test_data = scaled_test_data.reshape(num_samples_test, time_steps_test, num_features)\n","\n","    return scaled_train_data, scaled_test_data\n","\n","def scale_target_var(target_data):\n","    min_val, max_val = np.min(target_data, axis=0), np.max(target_data, axis=0)\n","    target_data = (target_data-min_val)/(max_val-min_val)\n","\n","    return target_data, min_val, max_val"]},{"cell_type":"code","execution_count":83,"metadata":{"id":"LyvV6GFDC66F","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725436655111,"user_tz":300,"elapsed":11,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}},"outputId":"f94e41d7-0265-450b-8321-19248cc19e4c"},"outputs":[{"output_type":"stream","name":"stdout","text":["val input : (9, 120, 47)\n","train input : (36, 120, 47)\n","val output : (9, 1)\n","train output : (36, 3)\n"]}],"source":["input_train, input_test= scale_input_data(input_train[:, (60-sample_size):(180-sample_size), :], input_test[:, (60-sample_size):(180-sample_size), :])\n","output_train, min_val, max_val = scale_target_var(output_train)\n","\n","input_val = input_train[val_indices]\n","input_train = input_train[train_indices]\n","output_val = output_train_total_ssq[val_indices]\n","output_train = output_train[train_indices]\n","\n","print(\"val input :\",input_val.shape)\n","print(\"train input :\",input_train.shape)\n","print(\"val output :\",output_val.shape)\n","print(\"train output :\",output_train.shape)"]},{"cell_type":"code","source":["input_train.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g7XAkiG-SZDI","executionInfo":{"status":"ok","timestamp":1725436655275,"user_tz":300,"elapsed":172,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}},"outputId":"d4836326-eb90-4d67-b47e-34d479d916d5"},"execution_count":84,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(36, 120, 47)"]},"metadata":{},"execution_count":84}]},{"cell_type":"code","execution_count":85,"metadata":{"id":"E6ssyYUeDJwI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725436693161,"user_tz":300,"elapsed":37892,"user":{"displayName":"Anjan Kumar Dev","userId":"10866355726897008846"}},"outputId":"bdd6be15-71bd-44ca-c869-9e0a3c2715b7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:664: UserWarning: Gradients do not exist for variables ['kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 397ms/step - loss: 0.0320 - sequential_60_mse: 0.0320 - val_loss: 5046.5454 - val_sequential_60_mse: 5046.5454\n","Epoch 2/20\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.0398 - sequential_60_mse: 0.0398 - val_loss: 5063.4526 - val_sequential_60_mse: 5063.4526\n","Epoch 3/20\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.0240 - sequential_60_mse: 0.0240 - val_loss: 5064.9287 - val_sequential_60_mse: 5064.9287\n","Epoch 4/20\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.0265 - sequential_60_mse: 0.0265 - val_loss: 5056.5425 - val_sequential_60_mse: 5056.5425\n","Epoch 5/20\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.0206 - sequential_60_mse: 0.0206 - val_loss: 5052.1680 - val_sequential_60_mse: 5052.1680\n","Epoch 6/20\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.0220 - sequential_60_mse: 0.0220 - val_loss: 5055.4683 - val_sequential_60_mse: 5055.4683\n","Epoch 6: early stopping\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - loss: 0.1015 - sequential_60_mse: 0.0248\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step\n","pred_shape_before (3, 23, 1)\n","pred_shape_after (23, 3)\n","Epoch 1/20\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:664: UserWarning: Gradients do not exist for variables ['kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 584ms/step - loss: 0.0342 - sequential_63_mse: 0.0342 - val_loss: 5048.7227 - val_sequential_63_mse: 5048.7227\n","Epoch 2/20\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.0253 - sequential_63_mse: 0.0253 - val_loss: 5069.0156 - val_sequential_63_mse: 5069.0156\n","Epoch 3/20\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.0470 - sequential_63_mse: 0.0470 - val_loss: 5065.3105 - val_sequential_63_mse: 5065.3105\n","Epoch 4/20\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 0.0498 - sequential_63_mse: 0.0498 - val_loss: 5054.2285 - val_sequential_63_mse: 5054.2285\n","Epoch 5/20\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.0210 - sequential_63_mse: 0.0210 - val_loss: 5052.2700 - val_sequential_63_mse: 5052.2700\n","Epoch 6/20\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.0220 - sequential_63_mse: 0.0220 - val_loss: 5057.1660 - val_sequential_63_mse: 5057.1660\n","Epoch 6: early stopping\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step - loss: 0.1491 - sequential_63_mse: 0.0315\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step\n","pred_shape_before (3, 23, 1)\n","pred_shape_after (23, 3)\n","Epoch 1/20\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:664: UserWarning: Gradients do not exist for variables ['kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 393ms/step - loss: 0.0493 - sequential_66_mse: 0.0493 - val_loss: 5041.8262 - val_sequential_66_mse: 5041.8262\n","Epoch 2/20\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.0547 - sequential_66_mse: 0.0547 - val_loss: 5062.6289 - val_sequential_66_mse: 5062.6289\n","Epoch 3/20\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 0.0228 - sequential_66_mse: 0.0228 - val_loss: 5071.1396 - val_sequential_66_mse: 5071.1396\n","Epoch 4/20\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.0476 - sequential_66_mse: 0.0476 - val_loss: 5063.9473 - val_sequential_66_mse: 5063.9473\n","Epoch 5/20\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.0313 - sequential_66_mse: 0.0313 - val_loss: 5052.6875 - val_sequential_66_mse: 5052.6875\n","Epoch 6/20\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 0.0259 - sequential_66_mse: 0.0259 - val_loss: 5048.0488 - val_sequential_66_mse: 5048.0488\n","Epoch 6: early stopping\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - loss: 0.2102 - sequential_66_mse: 0.0355\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step\n","pred_shape_before (3, 23, 1)\n","pred_shape_after (23, 3)\n","Epoch 1/20\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:664: UserWarning: Gradients do not exist for variables ['kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 618ms/step - loss: 0.0279 - sequential_69_mse: 0.0279 - val_loss: 5046.4624 - val_sequential_69_mse: 5046.4624\n","Epoch 2/20\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - loss: 0.0431 - sequential_69_mse: 0.0431 - val_loss: 5074.6299 - val_sequential_69_mse: 5074.6299\n","Epoch 3/20\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.0661 - sequential_69_mse: 0.0661 - val_loss: 5069.2793 - val_sequential_69_mse: 5069.2793\n","Epoch 4/20\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.0381 - sequential_69_mse: 0.0381 - val_loss: 5054.8374 - val_sequential_69_mse: 5054.8374\n","Epoch 5/20\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 0.0298 - sequential_69_mse: 0.0298 - val_loss: 5051.2710 - val_sequential_69_mse: 5051.2710\n","Epoch 6/20\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0346 - sequential_69_mse: 0.0346 - val_loss: 5055.5103 - val_sequential_69_mse: 5055.5103\n","Epoch 6: early stopping\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 0.0614 - sequential_69_mse: 0.0264\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step\n","pred_shape_before (3, 23, 1)\n","pred_shape_after (23, 3)\n","Epoch 1/20\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:664: UserWarning: Gradients do not exist for variables ['kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 381ms/step - loss: 0.0824 - sequential_72_mse: 0.0824 - val_loss: 5044.9873 - val_sequential_72_mse: 5044.9873\n","Epoch 2/20\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.0370 - sequential_72_mse: 0.0370 - val_loss: 5051.6729 - val_sequential_72_mse: 5051.6729\n","Epoch 3/20\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.0211 - sequential_72_mse: 0.0211 - val_loss: 5058.7676 - val_sequential_72_mse: 5058.7676\n","Epoch 4/20\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.0255 - sequential_72_mse: 0.0255 - val_loss: 5060.6772 - val_sequential_72_mse: 5060.6772\n","Epoch 5/20\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0272 - sequential_72_mse: 0.0272 - val_loss: 5056.9126 - val_sequential_72_mse: 5056.9126\n","Epoch 6/20\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.0163 - sequential_72_mse: 0.0163 - val_loss: 5050.7520 - val_sequential_72_mse: 5050.7520\n","Epoch 6: early stopping\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - loss: 0.0834 - sequential_72_mse: 0.0241\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step\n","pred_shape_before (3, 23, 1)\n","pred_shape_after (23, 3)\n","4.380659927962845 2.675433809684405 2.6257988893819797\n"]}],"source":["from keras.models import Sequential\n","from keras.layers import Input, LSTM, Dense, Dropout\n","from keras.models import Model\n","import numpy as np\n","from keras.callbacks import EarlyStopping\n","import sklearn\n","rmse_n=[]\n","rmse_o=[]\n","rmse_d=[]\n","np.set_printoptions(precision=2)\n","for iteration in range(5):\n","  def get_shared_lstm(input_shape1, input_shape2):\n","      # Define shared LSTM model\n","      input_layer = Input(shape=(input_shape1, input_shape2))\n","      x = LSTM(64, return_sequences=False)(input_layer)\n","      x = Dense(256, activation='relu')(x)\n","      x = Dropout(0.2)(x)\n","      shared_model = Model(inputs=input_layer, outputs=x)\n","      return shared_model\n","\n","  def get_output_model(shared_lstm_output, output_shape):\n","      # Define separate output model for each column\n","      output_models = []\n","      for i in range(output_shape[1]):\n","          output_model = Sequential()\n","          output_model.add(Dense(256, activation='relu'))\n","          output_model.add(Dropout(0.2))\n","          output_model.add(Dense(1))  # Output shape is (None, 1) for each column\n","          output_model_output = output_model(shared_lstm_output)\n","          output_models.append(output_model_output)\n","      return output_models\n","\n","  # Assuming train_input, train_output, test_input, test_output are numpy arrays\n","\n","  # Reshape train and test inputs to match LSTM input shape\n","  train_input_reshaped = input_train.reshape((input_train.shape[0], input_train.shape[1], input_train.shape[2]))\n","  test_input_reshaped = input_test.reshape((input_test.shape[0], input_test.shape[1], input_test.shape[2]))\n","  val_input_reshaped = input_val.reshape((input_val.shape[0], input_val.shape[1], input_val.shape[2]))\n","\n","  # Get shared LSTM model\n","  shared_lstm = get_shared_lstm(input_train.shape[1], input_train.shape[2])\n","\n","  # Create separate output models for each column\n","  output_models = get_output_model(shared_lstm.output, output_train.shape)\n","\n","  # Create combined model\n","  model = Model(inputs=shared_lstm.input, outputs=output_models)\n","\n","  # Compile and train the model\n","  model.compile(loss='mse', optimizer='adam', metrics=[['mse'] for _ in range(output_train.shape[1])])  # Using MSE as loss and metric\n","  early_stopping = EarlyStopping(monitor='val_loss', mode='min', patience=5, verbose=1)\n","  model.fit(train_input_reshaped, output_train, epochs=20, batch_size=32,validation_data=(val_input_reshaped, output_val), callbacks=[early_stopping])\n","\n","\n","  # Predict test data\n","  model.fit(train_input_reshaped, [output_train[:, i] for i in range(output_train.shape[1])], epochs=1, batch_size=32)\n","\n","\n","  # Predict test data\n","  pred_test = np.array(model.predict(test_input_reshaped))\n","  print(\"pred_shape_before\",pred_test.shape)\n","  pred_test = np.transpose(pred_test.squeeze(), (1, 0))\n","  print(\"pred_shape_after\",pred_test.shape)\n","\n","  pred_test_n = np.zeros((pred_test.shape[0], 1))\n","  pred_test_o = np.zeros((pred_test.shape[0], 1))\n","  pred_test_d = np.zeros((pred_test.shape[0], 1))\n","  pred_test_final = np.empty((output_test.shape[0], 0))\n","  for m in range(pred_test.shape[0]):\n","    pred_test_n[m,0] = pred_test[m,0]*(max_val[0]-min_val[0]) + min_val[0]\n","    pred_test_o[m,0] = pred_test[m,1]*(max_val[1]-min_val[1]) + min_val[1]\n","    pred_test_d[m,0] = pred_test[m,2]*(max_val[2]-min_val[2]) + min_val[2]\n","\n","  pred_test_final=np.hstack((pred_test_final, pred_test_n))\n","  pred_test_final=np.hstack((pred_test_final, pred_test_o))\n","  pred_test_final=np.hstack((pred_test_final, pred_test_d))\n","\n","  # Overall Test Loss\n","  loss_n = sklearn.metrics.mean_squared_error(pred_test_final[:,0], output_test[:,0], squared = False)\n","  rmse_n.append(loss_n)\n","  loss_o = sklearn.metrics.mean_squared_error(pred_test_final[:,1], output_test[:,1], squared = False)\n","  rmse_o.append(loss_o)\n","  loss_d= sklearn.metrics.mean_squared_error(pred_test_final[:,2], output_test[:,2], squared = False)\n","  rmse_d.append(loss_d)\n","rmse_combined = np.column_stack((rmse_n, rmse_o, rmse_d))\n","rmse_n_loss = sum(rmse_n) / len(rmse_n)\n","rmse_o_loss = sum(rmse_o) / len(rmse_o)\n","rmse_d_loss = sum(rmse_d) / len(rmse_d)\n","average_rmse = np.array([rmse_n_loss, rmse_o_loss, rmse_d_loss])\n","\n","print(rmse_n_loss,rmse_o_loss,rmse_d_loss)\n","\n"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOwKdcdpHZZdzCrqAFRjvFc"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}